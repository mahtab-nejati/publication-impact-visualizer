{
  "authorID": "WLN3QrAAAAAJ",
  "name": "Yann LeCun",
  "image_link": null,
  "interests": [
    "AI",
    "machine learning",
    "computer vision",
    "robotics",
    "image compression"
  ],
  "citations": 266388,
  "hindex": 138,
  "i10index": 332,
  "citation_histogram": [
    ["2005", "792"],
    ["2006", "995"],
    ["2007", "1161"],
    ["2008", "1167"],
    ["2009", "1358"],
    ["2010", "1619"],
    ["2011", "2037"],
    ["2012", "2362"],
    ["2013", "3227"],
    ["2014", "4685"],
    ["2015", "7780"],
    ["2016", "13691"],
    ["2017", "20940"],
    ["2018", "31031"],
    ["2019", "38612"],
    ["2020", "42857"],
    ["2021", "46555"],
    ["2022", "34420"]
  ],
  "coauthors": [
    ["kukA0LcAAAAJ", "Yoshua Bengio"],
    ["kbN88gsAAAAJ", "Leon Bottou"],
    ["Wck7gd0AAAAJ", "patrick g haffner"],
    ["NHIpV98AAAAJ", "Bernhard Boser"],
    ["0nPi5YYAAAAJ", "Pierre Sermanet"],
    ["6KgM0OkAAAAJ", "Richard E. Howard"],
    ["JicYPdAAAAAJ", "Geoffrey Hinton"],
    ["NbXF7T8AAAAJ", "Marc'Aurelio Ranzato"],
    ["u3u16tgAAAAJ", "Clement Farabet"],
    ["SSTIBK0AAAAJ", "Michael Mathieu"],
    ["sGFyDIUAAAAJ", "koray kavukcuoglu"],
    ["akrkYU0AAAAJ", "Patrice Simard"],
    ["L4bNmsMAAAAJ", "Joan Bruna"],
    ["ePAAAAAJ", "Sumit Chopra"],
    ["FxUgAAAAJ", "Arthur Szlam"],
    ["QAAAAJ", "Raia Hadsell"],
    ["GgQ9GEkAAAAJ", "Rob Fergus"],
    ["TGpo3KAAAAAJ", "Eduard Sackinger"],
    ["U_IVY50AAAAJ", "Corinna Cortes"],
    ["1T2Xh68AAAAJ", "Camille Couprie"],
    ["W2pvAfEAAAAJ", "Jane Bromley"],
    ["4AAAAJ", "David Eigen"],
    ["jmERpL4AAAAJ", "Sara A. Solla"],
    ["8ipao8MAAAAJ", "Jake Zhao (Junbo)"],
    ["GfcBlpUAAAAJ", "Y-Lan Boureau"],
    ["bX__wkYAAAAJ", "Mikael Henaff"],
    ["cAAAAJ", "Hans Peter Graf"],
    ["vtegaJgAAAAJ", "vapnik"],
    ["vC2vywcAAAAJ", "Jean Ponce"],
    ["2_cT0AAAAJ", "Laurent Najman"],
    ["n4QjVfoAAAAJ", "Xiang Zhang"],
    ["mlF7YAAAAJ", "Anna Choromanska"],
    ["U_Jw8DUAAAAJ", "Jonathan Tompson"],
    ["uKXVH54AAAAJ", "Piotr Mirowski"],
    ["36ofBJgAAAAJ", "Soumith Chintala"],
    ["krWNpKoAAAAJ", "Jure Zbontar"],
    ["CBzRa74AAAAJ", "Eric Cosatto"],
    ["1oAAAAJ", "Ross Goroshin"],
    ["SeGmqkIAAAAJ", "Eugenio Culurciello"],
    ["jplQac8AAAAJ", "Klaus-Robert M\u00fcller"],
    ["XCZpOcAAAAAJ", "Wojciech Zaremba"],
    ["EOnaBbUAAAAJ", "Kevin Jarrett"],
    ["spAJDzYAAAAJ", "Ayse Naz Erkan"],
    ["QUJ0kPMAAAAJ", "Beat Flepp"],
    ["elmWdycAAAAJ", "Arjun Jain"],
    ["iPZaBcAAAAJ", "Levent Sagun"],
    ["S6moyNoAAAAJ", "Fran\u00e7oise Fogelman-Souli\u00e9"],
    ["6PJWcFEAAAAJ", "Francis Bach"],
    ["8sGC5D4AAAAJ", "Gerard BEN AROUS"],
    ["Q9jd0mgAAAAJ", "ruben I. kuzniecky"],
    ["UAAAAJ", "Michael Bronstein"],
    ["YCPycGAAAAAJ", "Pablo Sprechmann"],
    ["VJ8Qd6sAAAAJ", "Serkan Piantino"],
    ["6yMIP8AAAAAJ", "M. Reha Civanlar"],
    ["WRTLuxcAAAAJ", "J\u00f6rn Ostermann"],
    ["WBCKQMsAAAAJ", "Pascal Vincent"],
    ["Ysjk8kkAAAAJ", "Holger Schwenk"],
    ["45KfCpgAAAAJ", "Alexis Conneau"],
    ["a2KklUoAAAAJ", "Matthew Zeiler"],
    ["1p9NOFEAAAAJ", "Pierre Vandergheynst"],
    ["lMkTx0EAAAAJ", "Jason Weston"],
    ["nZEtlZoAAAAJ", "Margarita Osadchy"],
    ["4AAAAJ", "Tom Schaul"],
    ["rFaxB20AAAAJ", "Patrick Gallinari"],
    ["8Ir1WvIAAAAJ", "Selcuk Talay"],
    ["PUeKU8kAAAAJ", "Graham Taylor"],
    ["llt18PUAAAAJ", "Christopher S. Poultney"],
    ["dh03btIAAAAJ", "Jason Tyler Rolfe"],
    ["PMHXcoAAAAAJ", "Juan P Bello"],
    ["rAAAAAJ", "Jakob Verbeek"],
    ["8qisprwAAAAJ", "Pauline Luc"],
    ["LIjnUGgAAAAJ", "Alexander M. Rush"],
    ["n_ts4eYAAAAJ", "Yoon Kim"],
    ["FMJePIUAAAAJ", "Tom Sercu"],
    ["aNI9zwAAAAJ", "Tapani Raiko"],
    ["84AAAAJ", "Harri Valpola"],
    ["70I8ZIMAAAAJ", "Behzad Shahraray"],
    ["LmKtwk8AAAAJ", "Nicolas Le Roux"],
    ["TGRPKRIAAAAJ", "Ken Perlin"],
    ["kzS_Xd4AAAAJ", "Eric J. Humphrey"],
    ["vIGcvLsAAAAJ", "Nicolas Vasilache"],
    ["_UIAAAAJ", "Jose M. Alvarez"],
    ["yqsvxQgAAAAJ", "Theo Gevers"],
    ["3LYW1zMAAAAJ", "Antonio Manuel Lopez"],
    ["yyIoQu4AAAAJ", "Diederik P. Kingma"],
    ["jWWx33IAAAAJ", "Antoine Bordes"],
    ["cLPaHcIAAAAJ", "Natalia Neverova"],
    ["lH1PdF8AAAAJ", "Stefano Soatto"],
    ["c_z5hWEAAAAJ", "Pratik Chaudhari"],
    ["DYUloYkAAAAJ", "Carlo Baldassi"],
    ["fNOReswAAAAJ", "Riccardo Zecchina"],
    ["YAHWbtkAAAAJ", "Jennifer Chayes"],
    ["1rDKD9kAAAAJ", "Andrew Caplin"],
    ["zl0AOEgAAAAJ", "John Leahy"],
    ["xlkTND4AAAAJ", "Jeff Johnson"],
    ["LgF3Ds0AAAAJ", "Fabio Piano"],
    ["j29kMCwAAAAJ", "Pietro Perona"],
    ["AxFrw0sAAAAJ", "Barak A. Pearlmutter"],
    ["UVuQeJIAAAAJ", "Gloria Coruzzi"],
    ["8vjzfWwAAAAJ", "Gab Krouk \u5149\u5e0c"],
    ["iJENOG8AAAAJ", "Brian Kingsbury"],
    ["YfompPIAAAAJ", "Christian Puhrsch"],
    ["OaE_xMgAAAAJ", "Phi-Hung PHAM"],
    ["EKPoKcQAAAAJ", "serrano-gotarredona"],
    ["yjeEP_EAAAAJ", "Bernabe Linares-Barranco"],
    ["Hft_8cgAAAAJ", "Lawrence K Saul"],
    ["InxhqXgAAAAJ", "Tommi Vatanen"]
  ],
  "publications": [
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:lo0OIn9KAZgC",
      "title": "Deep learning",
      "link": "https://www.nature.com/articles/nature14539",
      "year": 2015,
      "cited_by": 56757,
      "authors": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton"],
      "description": "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.",
      "citation_histogram": [
        [2015, 219],
        [2016, 1673],
        [2017, 4188],
        [2018, 7064],
        [2019, 9744],
        [2020, 11293],
        [2021, 12456],
        [2022, 9526]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:u5HHmVD_uO8C",
      "title": "Gradient-based learning applied to document recognition",
      "link": "https://ieeexplore.ieee.org/abstract/document/726791/",
      "year": 1998,
      "cited_by": 49398,
      "authors": [
        "Yann LeCun",
        "L\u00e9on Bottou",
        "Yoshua Bengio",
        "Patrick Haffner"
      ],
      "description": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows\u00a0\u2026",
      "citation_histogram": [
        [2005, 137],
        [2006, 174],
        [2007, 186],
        [2008, 207],
        [2009, 224],
        [2010, 301],
        [2011, 361],
        [2012, 373],
        [2013, 545],
        [2014, 819],
        [2015, 1605],
        [2016, 2622],
        [2017, 3951],
        [2018, 5926],
        [2019, 7549],
        [2020, 8373],
        [2021, 8964],
        [2022, 6445]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:u-x6o8ySG0sC",
      "title": "Backpropagation applied to handwritten zip code recognition",
      "link": "https://ieeexplore.ieee.org/abstract/document/6795724/",
      "year": 1989,
      "cited_by": 11801,
      "authors": [
        "Yann LeCun",
        "Bernhard Boser",
        "John S Denker",
        "Donnie Henderson",
        "Richard E Howard",
        "Wayne Hubbard",
        "Lawrence D Jackel"
      ],
      "description": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.",
      "citation_histogram": [
        [1991, 42],
        [1992, 55],
        [1993, 67],
        [1994, 78],
        [1995, 50],
        [1996, 71],
        [1997, 64],
        [1998, 75],
        [1999, 49],
        [2000, 46],
        [2001, 35],
        [2002, 32],
        [2003, 42],
        [2004, 43],
        [2005, 47],
        [2006, 53],
        [2007, 67],
        [2008, 44],
        [2009, 49],
        [2010, 53],
        [2011, 58],
        [2012, 64],
        [2013, 90],
        [2014, 150],
        [2015, 353],
        [2016, 590],
        [2017, 860],
        [2018, 1336],
        [2019, 1652],
        [2020, 1764],
        [2021, 2065],
        [2022, 1580]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:YsMSGLbcyi4C",
      "title": "Convolutional networks for images, speech, and time series",
      "link": "http://www.iro.umontreal.ca/~lisa/pointeurs/handbook-convo.pdf",
      "year": 1995,
      "cited_by": 6292,
      "authors": ["Yann LeCun", "Yoshua Bengio"],
      "description": "The ability of multilayer back-propagation networks to learn complex, high-dimensional, nonlinear mappings from large collections of examples makes them obvious candidates for image recognition or speech recognition tasks (see PATTERN RECOGNITION AND NEURAL NETWORKS). In the traditional model of pattern recognition, a hand-designed feature extractor gathers relevant information from the input and eliminates irrelevant variabilities. A trainable classi er then categorizes the resulting feature vectors (or strings of symbols) into classes. In this scheme, standard, fully-connected multilayer networks can be used as classi ers. A potentially more interesting scheme is to eliminate the feature extractor, feeding the network with\\raw\" inputs (eg normalized images), and to rely on backpropagation to turn the rst few layers into an appropriate feature extractor. While this can be done with an ordinary fully connected feed-forward network with some success for tasks such as character recognition, there are problems.Firstly, typical images, or spectral representations of spoken words, are large, often with several hundred variables. A fully-connected rst layer with, say a few 100 hidden units, would already contain several 10,000 weights. Over tting problems may occur if training data is scarce. In addition, the memory requirement for that many weights may rule out certain hardware implementations. But, the main de ciency of unstructured nets for image or speech aplications is that they have no built-in invariance with respect to translations, or",
      "citation_histogram": [
        [2006, 18],
        [2007, 23],
        [2008, 21],
        [2009, 23],
        [2010, 32],
        [2011, 33],
        [2012, 35],
        [2013, 59],
        [2014, 114],
        [2015, 206],
        [2016, 334],
        [2017, 512],
        [2018, 751],
        [2019, 938],
        [2020, 1035],
        [2021, 1209],
        [2022, 785]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:W7OEmFMy1HYC",
      "title": "The MNIST database of handwritten digits",
      "link": "https://ci.nii.ac.jp/naid/10027939599/",
      "year": 1998,
      "cited_by": 5739,
      "authors": ["Yann LeCun", "Corinna Cortes"],
      "description": "CiNii \u8ad6\u6587 - THE MNIST DATABASE of handwritten digits CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\n\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\n\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\n\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 [7/12\u66f4\u65b0\n]2022\u5e744\u67081\u65e5\u304b\u3089\u306eCiNii Articles\u306eCiNii Research\u3078\u306e\u7d71\u5408\u306b\u3064\u3044\u3066 THE MNIST \nDATABASE of handwritten digits LECUN Y. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 LECUN Y. \u53ce\u9332\u520a\u884c\u7269 http://yann.lecun.com/exdb/mnist/ \nhttp://yann.lecun.com/exdb/mnist/ \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u30e6\u30fc\u30b6\u30fc\u306e\u9818\u57df\u5206\u5272\n\u7d50\u679c\u306b\u57fa\u3065\u304f\u4e00\u822c\u7269\u4f53\u8a8d\u8b58 \u5b89\u500d \u6e80 , \u5409\u7530 \u60a0\u4e00 \u96fb\u5b50\u60c5\u5831\u901a\u4fe1\u5b66\u4f1a\u6280\u8853\u7814\u7a76\u5831\u544a. PRMU, \u30d1\u30bf\u30fc\u30f3\n\u8a8d\u8b58\u30fb\u30e1\u30c7\u30a3\u30a2\u7406\u89e3 110(187), 13-20, 2010-08-29 \u53c2\u8003\u6587\u732e24\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) \n10027939599 \u8cc7\u6599\u7a2e\u5225 \u305d\u306e\u4ed6 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 RefWorks\u306b\u66f8\u304d\u51fa\u3057 \u2026",
      "citation_histogram": [
        [2003, 15],
        [2004, 18],
        [2005, 22],
        [2006, 24],
        [2007, 27],
        [2008, 40],
        [2009, 46],
        [2010, 42],
        [2011, 61],
        [2012, 51],
        [2013, 71],
        [2014, 114],
        [2015, 166],
        [2016, 270],
        [2017, 411],
        [2018, 735],
        [2019, 853],
        [2020, 937],
        [2021, 1035],
        [2022, 745]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Zo6E4E6iJKMC",
      "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",
      "link": "https://arxiv.org/abs/1312.6229",
      "year": 2014,
      "cited_by": 5570,
      "authors": [
        "Pierre Sermanet",
        "David Eigen",
        "Xiang Zhang",
        "Michael Mathieu",
        "Rob Fergus",
        "Yann LeCun"
      ],
      "description": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.",
      "citation_histogram": [
        [2014, 150],
        [2015, 428],
        [2016, 667],
        [2017, 752],
        [2018, 861],
        [2019, 847],
        [2020, 727],
        [2021, 620],
        [2022, 365]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:TIZ-Mc8IlK0C",
      "title": "Handwritten digit recognition with a back-propagation network",
      "link": "https://proceedings.neurips.cc/paper/1989/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html",
      "year": 1990,
      "cited_by": 5095,
      "authors": [
        "Y LeCun",
        "B Boser",
        "JS Denker",
        "D Henderson",
        "RE Howard",
        "W Hubbard",
        "LD Jackel"
      ],
      "description": "We present an application of back-propagation networks to hand (cid: 173) written digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the US Postal Service.",
      "citation_histogram": [
        [1990, 18],
        [1991, 27],
        [1992, 36],
        [1993, 39],
        [1994, 47],
        [1995, 52],
        [1996, 25],
        [1997, 51],
        [1998, 27],
        [1999, 23],
        [2000, 23],
        [2001, 21],
        [2002, 28],
        [2003, 28],
        [2004, 24],
        [2005, 37],
        [2006, 33],
        [2007, 31],
        [2008, 31],
        [2009, 35],
        [2010, 23],
        [2011, 41],
        [2012, 36],
        [2013, 51],
        [2014, 80],
        [2015, 132],
        [2016, 281],
        [2017, 424],
        [2018, 654],
        [2019, 672],
        [2020, 763],
        [2021, 719],
        [2022, 532]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:PkcyUWeTMh0C",
      "title": "Efficient backprop",
      "link": "https://link.springer.com/chapter/10.1007/978-3-642-35289-8_3",
      "year": 2012,
      "cited_by": 4987,
      "authors": [
        "Yann A LeCun",
        "L\u00e9on Bottou",
        "Genevieve B Orr",
        "Klaus-Robert M\u00fcller"
      ],
      "description": " The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most \u201cclassical\u201d second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.",
      "citation_histogram": [
        [2011, 76],
        [2012, 94],
        [2013, 122],
        [2014, 143],
        [2015, 208],
        [2016, 387],
        [2017, 503],
        [2018, 639],
        [2019, 663],
        [2020, 633],
        [2021, 644],
        [2022, 403]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:2osOgNQ5qMEC",
      "title": "Efficient backprop",
      "link": null,
      "year": 1998,
      "cited_by": 4987,
      "authors": [
        "Yann A LeCun",
        "L\u00e9on Bottou",
        "Genevieve B Orr",
        "Klaus-Robert M\u00fcller"
      ],
      "description": null,
      "citation_histogram": [
        [2011, 76],
        [2012, 94],
        [2013, 122],
        [2014, 143],
        [2015, 208],
        [2016, 387],
        [2017, 503],
        [2018, 639],
        [2019, 663],
        [2020, 633],
        [2021, 644],
        [2022, 403]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:rywEMSoAiS0C",
      "title": "Optimal Brain Damage",
      "link": "https://proceedings.neurips.cc/paper/250-optimal-brain-damage",
      "year": 1990,
      "cited_by": 4830,
      "authors": ["Yann LeCun", "John S Denker", "Sara A Solla"],
      "description": "We have used information-theoretic ideas to derive a class of prac (cid: 173) tical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, sev (cid: 173) eral improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative informa (cid: 173) tion to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.",
      "citation_histogram": [
        [1990, 23],
        [1991, 43],
        [1992, 58],
        [1993, 85],
        [1994, 95],
        [1995, 104],
        [1996, 121],
        [1997, 100],
        [1998, 125],
        [1999, 94],
        [2000, 86],
        [2001, 93],
        [2002, 86],
        [2003, 70],
        [2004, 78],
        [2005, 65],
        [2006, 95],
        [2007, 114],
        [2008, 76],
        [2009, 83],
        [2010, 62],
        [2011, 65],
        [2012, 68],
        [2013, 71],
        [2014, 58],
        [2015, 77],
        [2016, 124],
        [2017, 173],
        [2018, 311],
        [2019, 414],
        [2020, 570],
        [2021, 611],
        [2022, 473]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:7EeV9ZTah7MC",
      "title": "Character-level convolutional networks for text classification",
      "link": "https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html",
      "year": 2015,
      "cited_by": 4691,
      "authors": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"],
      "description": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.",
      "citation_histogram": [
        [2016, 146],
        [2017, 329],
        [2018, 634],
        [2019, 857],
        [2020, 978],
        [2021, 939],
        [2022, 754]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:KlAtU1dfN6UC",
      "title": "Dimensionality reduction by learning an invariant mapping",
      "link": "https://ieeexplore.ieee.org/abstract/document/1640964/",
      "year": 2006,
      "cited_by": 4006,
      "authors": ["Raia Hadsell", "Sumit Chopra", "Yann LeCun"],
      "description": "Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that 'similar\" points in input space are mapped to nearby points on the manifold. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distancemeasure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular LLE.",
      "citation_histogram": [
        [2007, 10],
        [2008, 12],
        [2009, 18],
        [2010, 18],
        [2011, 16],
        [2012, 21],
        [2013, 36],
        [2014, 30],
        [2015, 68],
        [2016, 135],
        [2017, 192],
        [2018, 329],
        [2019, 481],
        [2020, 723],
        [2021, 1024],
        [2022, 858]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:jjenCjXDw2QC",
      "title": "Spectral Networks and Locally Connected Networks on Graphs",
      "link": "https://arxiv.org/abs/1312.6203",
      "year": 2014,
      "cited_by": 3986,
      "authors": [
        "Joan Bruna",
        "Wojciech Zaremba",
        "Arthur Szlam",
        "Yann LeCun"
      ],
      "description": "Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.",
      "citation_histogram": [
        [2015, 15],
        [2016, 31],
        [2017, 105],
        [2018, 289],
        [2019, 601],
        [2020, 880],
        [2021, 1127],
        [2022, 908]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:LkGwnXOMwfcC",
      "title": "Learning a similarity metric discriminatively, with application to face verification",
      "link": "https://ieeexplore.ieee.org/abstract/document/1467314/",
      "year": 2005,
      "cited_by": 3980,
      "authors": ["Sumit Chopra", "Raia Hadsell", "Yann LeCun"],
      "description": "We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L/sub 1/ norm in the target space approximates the \"semantic\" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of\u00a0\u2026",
      "citation_histogram": [
        [2006, 21],
        [2007, 25],
        [2008, 28],
        [2009, 29],
        [2010, 33],
        [2011, 25],
        [2012, 27],
        [2013, 33],
        [2014, 52],
        [2015, 100],
        [2016, 209],
        [2017, 352],
        [2018, 512],
        [2019, 582],
        [2020, 655],
        [2021, 691],
        [2022, 553]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:kNdYIx-mwKoC",
      "title": "Signature verification using a\" siamese\" time delay neural network",
      "link": "https://proceedings.neurips.cc/paper/1993/hash/288cc0ff022877bd3df94bc9360b9c5d-Abstract.html",
      "year": 1993,
      "cited_by": 3568,
      "authors": [
        "Jane Bromley",
        "Isabelle Guyon",
        "Yann LeCun",
        "Eduard S\u00e4ckinger",
        "Roopak Shah"
      ],
      "description": "This paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a\" Siamese\" neural network. This network consists of two identical sub-networks joined at their out (cid: 173) puts. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance be (cid: 173) tween the two feature vectors. Verification consists of comparing an extracted feature vector~ ith a stored feature vector for the signer. Signatures closer to this stored representation than a chosen thresh (cid: 173) old are accepted, all other signatures are rejected as forgeries.",
      "citation_histogram": [
        [2008, 12],
        [2009, 8],
        [2010, 7],
        [2011, 15],
        [2012, 9],
        [2013, 10],
        [2014, 14],
        [2015, 56],
        [2016, 145],
        [2017, 258],
        [2018, 385],
        [2019, 585],
        [2020, 690],
        [2021, 749],
        [2022, 536]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:EPG8bYD4jVwC",
      "title": "Learning Hierarchical Features for Scene Labeling",
      "link": "https://ieeexplore.ieee.org/abstract/document/6338939/",
      "year": 2013,
      "cited_by": 3124,
      "authors": [
        "Cl\u00e9ment Farabet",
        "Camille Couprie",
        "Laurent Najman",
        "Yann LeCun"
      ],
      "description": "Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona\u00a0\u2026",
      "citation_histogram": [
        [2013, 34],
        [2014, 122],
        [2015, 267],
        [2016, 421],
        [2017, 482],
        [2018, 478],
        [2019, 400],
        [2020, 372],
        [2021, 279],
        [2022, 207]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:6fs0NoO7GbkC",
      "title": "MNIST handwritten digit database",
      "link": "https://scholar.google.com/scholar?cluster=15669739124100992429&hl=en&oi=scholarr",
      "year": 2010,
      "cited_by": 3090,
      "authors": ["Yann LeCun", "Corinna Cortes", "Chris Burges"],
      "description": null,
      "citation_histogram": [
        [2013, 14],
        [2014, 14],
        [2015, 27],
        [2016, 50],
        [2017, 115],
        [2018, 270],
        [2019, 456],
        [2020, 666],
        [2021, 892],
        [2022, 535]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:5nxA0vEk-isC",
      "title": "What is the best multi-stage architecture for object recognition?",
      "link": "https://ieeexplore.ieee.org/abstract/document/5459469/",
      "year": 2009,
      "cited_by": 2749,
      "authors": [
        "Kevin Jarrett",
        "Koray Kavukcuoglu",
        "Marc\u2019Aurelio Ranzato",
        "Yann LeCun"
      ],
      "description": "In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition\u00a0\u2026",
      "citation_histogram": [
        [2010, 36],
        [2011, 75],
        [2012, 75],
        [2013, 121],
        [2014, 167],
        [2015, 206],
        [2016, 288],
        [2017, 305],
        [2018, 343],
        [2019, 335],
        [2020, 305],
        [2021, 293],
        [2022, 148]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:kJDgFkosVoMC",
      "title": "Regularization of neural networks using dropconnect",
      "link": "http://proceedings.mlr.press/v28/wan13.html?ref=https://githubhelp.com",
      "year": 2013,
      "cited_by": 2743,
      "authors": [
        "Li Wan",
        "Matthew Zeiler",
        "Sixin Zhang",
        "Yann LeCun",
        "Rob Fergus"
      ],
      "description": "We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.",
      "citation_histogram": [
        [2013, 15],
        [2014, 82],
        [2015, 154],
        [2016, 268],
        [2017, 336],
        [2018, 408],
        [2019, 454],
        [2020, 419],
        [2021, 345],
        [2022, 228]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:KG521SxztIwC",
      "title": "Geometric deep learning: going beyond euclidean data",
      "link": "https://ieeexplore.ieee.org/abstract/document/7974879/",
      "year": 2017,
      "cited_by": 2591,
      "authors": [
        "Michael M Bronstein",
        "Joan Bruna",
        "Yann LeCun",
        "Arthur Szlam",
        "Pierre Vandergheynst"
      ],
      "description": "Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains, such as graphs and manifolds. The purpose of this article is to overview different examples of geometric deep-learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.",
      "citation_histogram": [
        [2017, 43],
        [2018, 236],
        [2019, 435],
        [2020, 578],
        [2021, 692],
        [2022, 575]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:_Qo2XoVZTnwC",
      "title": "Convolutional networks and applications in vision",
      "link": "https://ieeexplore.ieee.org/abstract/document/5537907/",
      "year": 2010,
      "cited_by": 2435,
      "authors": ["Yann LeCun", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"],
      "description": "Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or \"features\")? which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologically-inspired trainable architecture that can learn invariant features. Each stage in a ConvNets is composed of a filter bank, some nonlinearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that\u00a0\u2026",
      "citation_histogram": [
        [2011, 18],
        [2012, 35],
        [2013, 43],
        [2014, 78],
        [2015, 122],
        [2016, 208],
        [2017, 299],
        [2018, 323],
        [2019, 343],
        [2020, 355],
        [2021, 331],
        [2022, 245]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:FcH-RsB9iB0C",
      "title": "A Closer Look at Spatiotemporal Convolutions for Action Recognition",
      "link": "http://openaccess.thecvf.com/content_cvpr_2018/html/Tran_A_Closer_Look_CVPR_2018_paper.html",
      "year": 2017,
      "cited_by": 1978,
      "authors": [
        "Du Tran",
        "Heng Wang",
        "Lorenzo Torresani",
        "Jamie Ray",
        "Yann LeCun",
        "Manohar Paluri"
      ],
      "description": "In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly gains in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block``R (2+ 1) D''which produces CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101, and HMDB51.",
      "citation_histogram": [
        [2018, 40],
        [2019, 219],
        [2020, 414],
        [2021, 707],
        [2022, 586]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:q7hqJx8pYzEC",
      "title": "Deep multi-scale video prediction beyond mean square error",
      "link": "https://arxiv.org/abs/1511.05440",
      "year": 2015,
      "cited_by": 1837,
      "authors": ["Michael Mathieu", "Camille Couprie", "Yann LeCun"],
      "description": "Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset",
      "citation_histogram": [
        [2016, 41],
        [2017, 174],
        [2018, 328],
        [2019, 378],
        [2020, 365],
        [2021, 308],
        [2022, 220]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:eQOLeE2rZwMC",
      "title": "Efficient learning of sparse representations with an energy-based model",
      "link": "https://proceedings.neurips.cc/paper/2006/hash/87f4d79e36d68c3031ccf6c55e9bbd39-Abstract.html",
      "year": 2006,
      "cited_by": 1696,
      "authors": [
        "Marc\u2019Aurelio Ranzato",
        "Christopher Poultney",
        "Sumit Chopra",
        "Yann LeCun"
      ],
      "description": "We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion:(1) compute the minimum-energy code vector,(2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces\" stroke detectors\" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.",
      "citation_histogram": [
        [2007, 18],
        [2008, 24],
        [2009, 36],
        [2010, 48],
        [2011, 61],
        [2012, 55],
        [2013, 96],
        [2014, 114],
        [2015, 130],
        [2016, 149],
        [2017, 180],
        [2018, 198],
        [2019, 186],
        [2020, 153],
        [2021, 138],
        [2022, 84]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Tyk-4Ss8FVUC",
      "title": "Learning methods for generic object recognition with invariance to pose and lighting",
      "link": "https://ieeexplore.ieee.org/abstract/document/1315150/",
      "year": 2004,
      "cited_by": 1671,
      "authors": ["Yann LeCun", "Fu Jie Huang", "Leon Bottou"],
      "description": "We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform\u00a0\u2026",
      "citation_histogram": [
        [2005, 19],
        [2006, 29],
        [2007, 26],
        [2008, 35],
        [2009, 42],
        [2010, 45],
        [2011, 60],
        [2012, 53],
        [2013, 83],
        [2014, 87],
        [2015, 101],
        [2016, 134],
        [2017, 137],
        [2018, 180],
        [2019, 173],
        [2020, 157],
        [2021, 165],
        [2022, 115]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:vq7B84E5p90C",
      "title": "Joint training of a convolutional network and a graphical model for human pose estimation",
      "link": "https://proceedings.neurips.cc/paper/2014/hash/e744f91c29ec99f0e662c9177946c627-Abstract.html",
      "year": 2014,
      "cited_by": 1628,
      "authors": [
        "Jonathan J Tompson",
        "Arjun Jain",
        "Yann LeCun",
        "Christoph Bregler"
      ],
      "description": "This paper proposes a new hybrid architecture that consists of a deep Convolutional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.",
      "citation_histogram": [
        [2014, 7],
        [2015, 99],
        [2016, 153],
        [2017, 202],
        [2018, 249],
        [2019, 254],
        [2020, 251],
        [2021, 225],
        [2022, 144]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:OcT3jDimY5MC",
      "title": "Exploiting linear structure within convolutional networks for efficient evaluation",
      "link": "https://proceedings.neurips.cc/paper/2014/hash/2afe4567e1bf64d32a5527244d104cea-Abstract.html",
      "year": 2014,
      "cited_by": 1621,
      "authors": [
        "Emily L Denton",
        "Wojciech Zaremba",
        "Joan Bruna",
        "Yann LeCun",
        "Rob Fergus"
      ],
      "description": "We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2\u00d7, while keeping the accuracy within 1% of the original model.",
      "citation_histogram": [
        [2014, 10],
        [2015, 41],
        [2016, 87],
        [2017, 151],
        [2018, 249],
        [2019, 290],
        [2020, 309],
        [2021, 291],
        [2022, 173]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:WF5omc3nYNoC",
      "title": "Scaling learning algorithms towards AI",
      "link": "https://pdfs.semanticscholar.org/f01e/080777b59d6978e412ded8995edabbaa62f0.pdf",
      "year": 2007,
      "cited_by": 1577,
      "authors": ["Yoshua Bengio", "Yann LeCun"],
      "description": "Local Derivative: For a Gaussian Kernel classifier, the normal of the tangent of the decision surface at x is constrained to approximately lie in the span of the vectors (x\u2212 xi), where|| x\u2212 xi|| is small compared to \u03c3 and xi are in the training set.",
      "citation_histogram": [
        [2007, 11],
        [2008, 21],
        [2009, 27],
        [2010, 69],
        [2011, 41],
        [2012, 52],
        [2013, 79],
        [2014, 83],
        [2015, 121],
        [2016, 148],
        [2017, 155],
        [2018, 175],
        [2019, 166],
        [2020, 143],
        [2021, 168],
        [2022, 103]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:blknAaTinKkC",
      "title": "A theoretical analysis of feature pooling in visual recognition",
      "link": "https://www.di.ens.fr/willow/pdfs/icml2010b.pdf",
      "year": 2010,
      "cited_by": 1537,
      "authors": ["Y-Lan Boureau", "Jean Ponce", "Yann LeCun"],
      "description": "Many modern visual recognition algorithms incorporate a step of spatial \u2018pooling\u2019, where the outputs of several nearby feature detectors are combined into a local or global \u2018bag of features\u2019, in a way that preserves task-related information while removing irrelevant details. Pooling is used to achieve invariance to image transformations, more compact representations, and better robustness to noise and clutter. Several papers have shown that the details of the pooling operation can greatly influence the performance, but studies have so far been purely empirical. In this paper, we show that the reasons underlying the performance of various pooling methods are obscured by several confounding factors, such as the link between the sample cardinality in a spatial pool and the resolution at which low-level features have been extracted. We provide a detailed theoretical analysis of max pooling and average pooling, and give extensive empirical comparisons for object recognition tasks.",
      "citation_histogram": [
        [2010, 4],
        [2011, 13],
        [2012, 31],
        [2013, 45],
        [2014, 76],
        [2015, 97],
        [2016, 127],
        [2017, 155],
        [2018, 236],
        [2019, 212],
        [2020, 233],
        [2021, 191],
        [2022, 97]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:V3AGJWp-ZtQC",
      "title": "Comparison of Learning Algorithms for Handwritten Digit Recognition",
      "link": "https://www.academia.edu/download/47948171/KNN_recognition.pdf",
      "year": 1995,
      "cited_by": 1526,
      "authors": [
        "Y LeCun",
        "L Jackel",
        "L Bottou",
        "A Brunot",
        "C Cortes",
        "J Denker",
        "H Drucker",
        "I Guyon",
        "U Muiller",
        "E Sackinger",
        "P Simard",
        "V Vapnik"
      ],
      "description": "This paper compares the performance of several classifier algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also rejection, training time, recognition time, and memory requirements.",
      "citation_histogram": [
        [1997, 6],
        [1998, 21],
        [1999, 14],
        [2000, 22],
        [2001, 21],
        [2002, 34],
        [2003, 30],
        [2004, 32],
        [2005, 48],
        [2006, 53],
        [2007, 38],
        [2008, 28],
        [2009, 40],
        [2010, 24],
        [2011, 15],
        [2012, 39],
        [2013, 32],
        [2014, 47],
        [2015, 64],
        [2016, 107],
        [2017, 117],
        [2018, 143],
        [2019, 166],
        [2020, 128],
        [2021, 138],
        [2022, 96]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:R3JqVFXIqpYC",
      "title": "Deep convolutional networks on graph-structured data",
      "link": "https://arxiv.org/abs/1506.05163",
      "year": 2015,
      "cited_by": 1493,
      "authors": ["Mikael Henaff", "Joan Bruna", "Yann LeCun"],
      "description": "Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities. In this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.",
      "citation_histogram": [
        [2016, 23],
        [2017, 75],
        [2018, 177],
        [2019, 287],
        [2020, 348],
        [2021, 350],
        [2022, 219]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:RGFaLdJalmkC",
      "title": "Learning fast approximations of sparse coding",
      "link": "https://dl.acm.org/doi/abs/10.5555/3104322.3104374",
      "year": 2010,
      "cited_by": 1450,
      "authors": ["Karol Gregor", "Yann Lecun"],
      "description": "In Sparse Coding (SC), input vectors are reconstructed using a sparse linear combination of basis vectors. SC has become a popular method for extracting features from data. For a given input, SC minimizes a quadratic reconstruction error with an L 1 penalty term on the code. The process is often too slow for applications such as real-time pattern recognition. We proposed two versions of a very fast algorithm that produces approximate estimates of the sparse code that can be used to compute good visual features, or to initialize exact iterative algorithms. The main idea is to train a non-linear, feed-forward predictor with a specific architecture and a fixed depth to produce the best possible approximation of the sparse code. A version of the method, which can be seen as a trainable version of Li and Osher's coordinate descent method, is shown to produce approximate solutions with 10 times less computation than Li\u00a0\u2026",
      "citation_histogram": [
        [2011, 11],
        [2012, 19],
        [2013, 28],
        [2014, 29],
        [2015, 41],
        [2016, 80],
        [2017, 105],
        [2018, 144],
        [2019, 198],
        [2020, 237],
        [2021, 294],
        [2022, 252]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:ufrVoPGSRksC",
      "title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition",
      "link": "https://ieeexplore.ieee.org/abstract/document/4270182/",
      "year": 2007,
      "cited_by": 1443,
      "authors": [
        "Marc'Aurelio Ranzato",
        "Fu Jie Huang",
        "Y-L Boureau",
        "Yann LeCun"
      ],
      "description": "We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each filter output within adjacent windows, and a point-wise sigmoid non-linearity. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level. Training a supervised classifier on these features yields 0.64% error on MNIST, and 54% average recognition rate on Caltech 101 with 30 training samples per category. While the resulting architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled\u00a0\u2026",
      "citation_histogram": [
        [2007, 4],
        [2008, 30],
        [2009, 40],
        [2010, 54],
        [2011, 47],
        [2012, 67],
        [2013, 77],
        [2014, 104],
        [2015, 107],
        [2016, 130],
        [2017, 130],
        [2018, 139],
        [2019, 139],
        [2020, 166],
        [2021, 107],
        [2022, 76]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:qjMakFHDy7sC",
      "title": "Generalization and network design strategies",
      "link": "https://www.academia.edu/download/30766382/lecun.pdf",
      "year": 1989,
      "cited_by": 1378,
      "authors": ["Yann LeCun"],
      "description": "An interestmg property of connectiomst systems is their ability to learn from examples. Although most recent work in the field concentrates on reducing learning times, the most important feature of a learning machine is its generalization performance. It is usually accepted that good generalization performance on real-world problems cannot be achieved unless some a pnon knowledge about the task is butlt Into the system. Back-propagation networks provide a way of specifymg such knowledge by imposing constraints both on the architecture of the network and on its weights. In general, such constramts can be considered as particular transformations of the parameter space Building a constramed network for image recogmtton appears to be a feasible task. We descnbe a small handwritten digit recogmtion problem and show that, even though the problem is linearly separable, single layer networks exhibit poor generalizatton performance. Multtlayer constrained networks perform very well on this task when orgamzed in a hierarchical structure with shift invariant feature detectors. These results confirm the idea that minimizing the number of free parameters in the network enhances generalization.",
      "citation_histogram": [
        [1989, 9],
        [1990, 18],
        [1991, 19],
        [1992, 23],
        [1993, 22],
        [1994, 26],
        [1995, 35],
        [1996, 24],
        [1997, 22],
        [1998, 22],
        [1999, 9],
        [2000, 2],
        [2001, 6],
        [2002, 16],
        [2003, 10],
        [2004, 12],
        [2005, 6],
        [2006, 8],
        [2007, 12],
        [2008, 9],
        [2009, 10],
        [2010, 7],
        [2011, 14],
        [2012, 11],
        [2013, 16],
        [2014, 21],
        [2015, 20],
        [2016, 34],
        [2017, 89],
        [2018, 156],
        [2019, 169],
        [2020, 167],
        [2021, 209],
        [2022, 130]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:ULOm3_A8WrAC",
      "title": "Learning mid-level features for recognition",
      "link": "https://ieeexplore.ieee.org/abstract/document/5539963/",
      "year": 2010,
      "cited_by": 1372,
      "authors": ["Y Boureau", "Francis Bach", "Yann LeCun", "Jean Ponce"],
      "description": "Many successful models for scene or object recognition transform low-level descriptors (such as Gabor filter responses, or SIFT descriptors) into richer representations of intermediate complexity. This process can often be broken down into two steps: (1) a coding step, which performs a pointwise transformation of the descriptors into a representation better adapted to the task, and (2) a pooling step, which summarizes the coded features over larger neighborhoods. Several combinations of coding and pooling schemes have been proposed in the literature. The goal of this paper is threefold. We seek to establish the relative importance of each step of mid-level feature extraction through a comprehensive cross evaluation of several types of coding modules (hard and soft vector quantization, sparse coding) and pooling schemes (by taking the average, or the maximum), which obtains state-of-the-art performance or\u00a0\u2026",
      "citation_histogram": [
        [2009, 6],
        [2010, 19],
        [2011, 66],
        [2012, 126],
        [2013, 139],
        [2014, 198],
        [2015, 147],
        [2016, 142],
        [2017, 136],
        [2018, 95],
        [2019, 84],
        [2020, 78],
        [2021, 68],
        [2022, 40]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:H7WDvlwkmv8C",
      "title": "Efficient object localization using convolutional networks",
      "link": "http://openaccess.thecvf.com/content_cvpr_2015/html/Tompson_Efficient_Object_Localization_2015_CVPR_paper.html",
      "year": 2014,
      "cited_by": 1369,
      "authors": [
        "Jonathan Tompson",
        "Ross Goroshin",
        "Arjun Jain",
        "Yann LeCun",
        "Christopher Bregler"
      ],
      "description": "Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficientposition refinement'model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC dataset and outperforms all existing approaches on the MPII-human-pose dataset.",
      "citation_histogram": [
        [2015, 15],
        [2016, 71],
        [2017, 112],
        [2018, 154],
        [2019, 229],
        [2020, 278],
        [2021, 285],
        [2022, 200]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:sWz8iI0ruhYC",
      "title": "Very deep convolutional networks for natural language processing",
      "link": "https://www.researchgate.net/profile/Loic-Barrault/publication/303822179_Very_Deep_Convolutional_Networks_for_Natural_Language_Processing/links/5772506308aeeec38953dd26/Very-Deep-Convolutional-Networks-for-Natural-Language-Processing.pdf",
      "year": 2016,
      "cited_by": 1361,
      "authors": [
        "Alexis Conneau",
        "Holger Schwenk",
        "Lo\u0131c Barrault",
        "Yann Lecun"
      ],
      "description": "The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which are very successful in computer vision. We present a new architecture for text processing which operates directly on the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report significant improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to NLP.",
      "citation_histogram": [
        [2016, 14],
        [2017, 89],
        [2018, 225],
        [2019, 322],
        [2020, 296],
        [2021, 241],
        [2022, 157]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:aAWV-AKBBEQC",
      "title": "The Loss Surface of Multilayer Networks",
      "link": "https://proceedings.mlr.press/v38/choromanska15.html",
      "year": 2015,
      "cited_by": 1310,
      "authors": [
        "Anna Choromanska",
        "Mikael Henaff",
        "Michael Mathieu",
        "G\u00e9rard Ben Arous",
        "Yann LeCun"
      ],
      "description": "We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large-and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.",
      "citation_histogram": [
        [2015, 30],
        [2016, 99],
        [2017, 153],
        [2018, 223],
        [2019, 263],
        [2020, 196],
        [2021, 219],
        [2022, 112]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:9KdEqzwCTsEC",
      "title": "Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches",
      "link": "https://www.jmlr.org/papers/volume17/15-535/15-535.pdf",
      "year": 2016,
      "cited_by": 1273,
      "authors": ["Jure \u017dbontar", "Yann LeCun"],
      "description": "We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. Training is carried out in a supervised manner by constructing a binary classification data set with examples of similar and dissimilar pairs of patches. We examine two network architectures for this task: one tuned for speed, the other for accuracy. The output of the convolutional neural network is used to initialize the stereo matching cost. A series of post-processing steps follow: cross-based cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median filter, and a bilateral filter. We evaluate our method on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it outperforms other approaches on all three data sets.",
      "citation_histogram": [
        [2015, 7],
        [2016, 59],
        [2017, 158],
        [2018, 205],
        [2019, 250],
        [2020, 200],
        [2021, 228],
        [2022, 147]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:GdooyxxgJVwC",
      "title": "Energy-based generative adversarial network",
      "link": "https://arxiv.org/abs/1609.03126",
      "year": 2016,
      "cited_by": 1261,
      "authors": ["Junbo Zhao", "Michael Mathieu", "Yann LeCun"],
      "description": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.",
      "citation_histogram": [
        [2016, 22],
        [2017, 144],
        [2018, 265],
        [2019, 248],
        [2020, 226],
        [2021, 226],
        [2022, 114]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:r0BpntZqJG4C",
      "title": "Object recognition with gradient-based learning",
      "link": "https://link.springer.com/chapter/10.1007/3-540-46805-6_19",
      "year": 1999,
      "cited_by": 1231,
      "authors": [
        "Yann LeCun",
        "Patrick Haffner",
        "L\u00e9on Bottou",
        "Yoshua Bengio"
      ],
      "description": " Finding an appropriate set of features is an essential problem in the design of shape recognition systems. This paper attempts to show that for recognizing simple objects with high shape variability such as handwritten characters, it is possible, and even advantageous, to feed the system directly with minimally processed images and to rely on learning to extract the right set of features. Convolutional Neural Networks are shown to be particularly well suited to this task. We also show that these networks can be used to recognize multiple objects without requiring explicit segmentation of the objects from their surrounding. The second part of the paper presents the Graph Transformer Network model which extends the applicability of gradient-based learning to systems that use graphs to represents features, objects, and their combinations.",
      "citation_histogram": [
        [2013, 4],
        [2014, 7],
        [2015, 7],
        [2016, 13],
        [2017, 20],
        [2018, 110],
        [2019, 210],
        [2020, 276],
        [2021, 334],
        [2022, 217]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:4_XrlajHzhgC",
      "title": "Mnist handwritten digit database. ATT Labs",
      "link": "https://scholar.google.com/scholar?cluster=10102052865200651144&hl=en&oi=scholarr",
      "year": 2010,
      "cited_by": 1107,
      "authors": ["Yann LeCun", "Corinna Cortes", "CJ Burges"],
      "description": null,
      "citation_histogram": [
        [2015, 5],
        [2016, 17],
        [2017, 27],
        [2018, 108],
        [2019, 150],
        [2020, 254],
        [2021, 358],
        [2022, 181]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Se3iqnhoufwC",
      "title": "Sparse feature learning for deep belief networks",
      "link": "https://proceedings.neurips.cc/paper/3363-sparse-feature-learning-for-deep-belief-networks",
      "year": 2008,
      "cited_by": 1033,
      "authors": ["Marc'Aurelio Ranzato", "Y-lan Boureau", "Yann LeCun"],
      "description": "Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (eg low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efficient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machines trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input variables can be captured.",
      "citation_histogram": [
        [2007, 3],
        [2008, 9],
        [2009, 16],
        [2010, 40],
        [2011, 42],
        [2012, 54],
        [2013, 47],
        [2014, 77],
        [2015, 94],
        [2016, 109],
        [2017, 96],
        [2018, 110],
        [2019, 118],
        [2020, 90],
        [2021, 52],
        [2022, 62]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:8k81kl-MbHgC",
      "title": "A tutorial on energy-based learning",
      "link": "http://yann.lecun.com/exdb/publis/orig/lecun-06.pdf",
      "year": 2006,
      "cited_by": 1027,
      "authors": [
        "Yann LeCun",
        "Sumit Chopra",
        "Raia Hadsell",
        "M Ranzato",
        "F Huang"
      ],
      "description": "Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each configuration of the variables. Inference consists in clamping the value of observed variables and finding configurations of the remaining variables that minimize the energy. Learning consists in finding an energy function in which observed configurations of the variables are given lower energies than unobserved ones. The EBM approach provides a common theoretical framework for many learning models, including traditional discriminative and generative approaches, as well as graph-transformer networks, conditional random fields, maximum margin Markov networks, and several manifold learning methods. Probabilistic models must be properly normalized, which sometimes requires evaluating intractable integrals over the space of all possible variable configurations. Since EBMs have no requirement for proper normalization, this problem is naturally circumvented. EBMs can be viewed as a form of non-probabilistic factor graphs, and they provide considerably more flexibility in the design of architectures and training criteria than probabilistic approaches.",
      "citation_histogram": [
        [2007, 13],
        [2008, 11],
        [2009, 23],
        [2010, 19],
        [2011, 23],
        [2012, 27],
        [2013, 31],
        [2014, 40],
        [2015, 49],
        [2016, 67],
        [2017, 58],
        [2018, 76],
        [2019, 87],
        [2020, 136],
        [2021, 197],
        [2022, 163]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:eO3_k5sD8BwC",
      "title": "Pedestrian detection with unsupervised multi-stage feature learning",
      "link": "http://openaccess.thecvf.com/content_cvpr_2013/html/Sermanet_Pedestrian_Detection_with_2013_CVPR_paper.html",
      "year": 2013,
      "cited_by": 1025,
      "authors": [
        "Pierre Sermanet",
        "Koray Kavukcuoglu",
        "Soumith Chintala",
        "Yann LeCun"
      ],
      "description": "Pedestrian detection is a problem of considerable practical interest. Adding to the list of successful applications of deep learning methods to vision, we report state-of-theart and competitive results on all major pedestrian datasets with a convolutional network model. The model uses a few new twists, such as multi-stage features, connections that skip layers to integrate global shape information with local distinctive motif information, and an unsupervised method based on convolutional sparse coding to pre-train the filters at each stage.",
      "citation_histogram": [
        [2013, 11],
        [2014, 60],
        [2015, 129],
        [2016, 163],
        [2017, 164],
        [2018, 148],
        [2019, 131],
        [2020, 92],
        [2021, 74],
        [2022, 37]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:9yKSN-GCB0IC",
      "title": "Comparison of classifier methods: a case study in handwritten digit recognition",
      "link": "https://ieeexplore.ieee.org/abstract/document/576879/",
      "year": 1994,
      "cited_by": 978,
      "authors": [
        "L\u00e9on Bottou",
        "Corinna Cortes",
        "John S Denker",
        "Harris Drucker",
        "Isabelle Guyon",
        "Larry D Jackel",
        "Yann LeCun",
        "Urs A Muller",
        "Edward Sackinger",
        "Patrice Simard",
        "Vladimir Vapnik"
      ],
      "description": "This paper compares the performance of several classifier algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also training time, recognition time, and memory requirements. When available, we report measurements of the fraction of patterns that must be rejected so that the remaining patterns have misclassification rates less than a given threshold.",
      "citation_histogram": [
        [1995, 10],
        [1996, 9],
        [1997, 19],
        [1998, 10],
        [1999, 9],
        [2000, 3],
        [2001, 10],
        [2002, 6],
        [2003, 11],
        [2004, 21],
        [2005, 28],
        [2006, 52],
        [2007, 47],
        [2008, 58],
        [2009, 58],
        [2010, 50],
        [2011, 48],
        [2012, 43],
        [2013, 39],
        [2014, 51],
        [2015, 58],
        [2016, 53],
        [2017, 54],
        [2018, 56],
        [2019, 46],
        [2020, 37],
        [2021, 43],
        [2022, 33]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:5awf1xo2G04C",
      "title": "Traffic sign recognition with multi-scale convolutional networks",
      "link": "https://ieeexplore.ieee.org/abstract/document/6033589/",
      "year": 2011,
      "cited_by": 897,
      "authors": ["Pierre Sermanet", "Yann LeCun"],
      "description": "We apply Convolutional Networks (ConvNets) to the task of traffic sign classification as part of the GTSRB competition. ConvNets are biologically-inspired multi-stage architectures that automatically learn hierarchies of invariant features. While many popular vision approaches use hand-crafted features such as HOG or SIFT, ConvNets learn features at every level from data that are tuned to the task at hand. The traditional ConvNet architecture was modified by feeding 1 st  stage features in addition to 2 nd  stage features to the classifier. The system yielded the 2nd-best accuracy of 98.97% during phase I of the competition (the best entry obtained 98.98%), above the human performance of 98.81%, using 32\u00d732 color input images. Experiments conducted after phase 1 produced a new record of 99.17% by increasing the network capacity, and by using greyscale images instead of color. Interestingly, random features\u00a0\u2026",
      "citation_histogram": [
        [2012, 19],
        [2013, 38],
        [2014, 34],
        [2015, 66],
        [2016, 105],
        [2017, 123],
        [2018, 128],
        [2019, 126],
        [2020, 103],
        [2021, 76],
        [2022, 56]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:ZuybSZzF8UAC",
      "title": "A theoretical framework for back-propagation",
      "link": "https://www.researchgate.net/profile/Yann-Lecun/publication/2360531_A_Theoretical_Framework_for_Back-Propagation/links/0deec519dfa297eac1000000/A-Theoretical-Framework-for-Back-Propagation.pdf",
      "year": 1988,
      "cited_by": 829,
      "authors": ["Yann LeCun"],
      "description": "Among all the supervised learning algo-rithms, back propagation (BP) is probably the most wi (l) dely used. Although nu-merous experimental works have demonstrated its capabilities, a deeper theoretical understanding of the algorithm is definitely needed. We present a mathematical framework for studying back-propagation based on the Lagrangian formalism. In this framework, inspired by optimal control theory, back-propagation is formulated as an optimization problem with nonlinear constraints. The Lagrange function is the sum of an output objective function and a constraint term which describes the network dynamics. This approach suggests many natural extensions to the basic algorithm. It also provides an extremely simple for-mulation (and derivation) of continuous re-current network equations as described by Pineda [Pineda, 1987]. Other easily described variations involve either additional terms in the\u00a0\u2026",
      "citation_histogram": [
        [1989, 5],
        [1990, 8],
        [1991, 12],
        [1992, 10],
        [1993, 17],
        [1994, 15],
        [1995, 11],
        [1996, 8],
        [1997, 7],
        [1998, 2],
        [1999, 2],
        [2000, 4],
        [2001, 3],
        [2002, 5],
        [2003, 5],
        [2004, 6],
        [2005, 8],
        [2006, 2],
        [2007, 4],
        [2008, 6],
        [2009, 4],
        [2010, 8],
        [2011, 12],
        [2012, 10],
        [2013, 10],
        [2014, 10],
        [2015, 30],
        [2016, 34],
        [2017, 65],
        [2018, 133],
        [2019, 132],
        [2020, 131],
        [2021, 101]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:JV2RwH3_ST0C",
      "title": "Convolutional learning of spatio-temporal features",
      "link": "https://link.springer.com/chapter/10.1007/978-3-642-15567-3_11",
      "year": 2010,
      "cited_by": 808,
      "authors": [
        "Graham Taylor",
        "Rob Fergus",
        "Yann LeCun",
        "Christoph Bregler"
      ],
      "description": " We address the problem of learning good features for understanding video data. We introduce a model that learns latent representations of image sequences from pairs of successive images. The convolutional architecture of our model allows it to scale to realistic image sizes whilst using a compact parametrization. In experiments on the NORB dataset, we show our model extracts latent \u201cflow fields\u201d which correspond to the transformation between the pair of input frames. We also use our model to extract low-level motion features in a multi-stage architecture for action recognition, demonstrating competitive performance on both the KTH and Hollywood2 datasets.",
      "citation_histogram": [
        [2010, 2],
        [2011, 14],
        [2012, 34],
        [2013, 42],
        [2014, 59],
        [2015, 70],
        [2016, 81],
        [2017, 100],
        [2018, 90],
        [2019, 87],
        [2020, 89],
        [2021, 76],
        [2022, 50]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:HWVPSj4JXeEC",
      "title": "Real-time continuous pose recovery of human hands using convolutional networks",
      "link": "https://dl.acm.org/doi/abs/10.1145/2629500",
      "year": 2014,
      "cited_by": 805,
      "authors": [
        "Jonathan Tompson",
        "Murphy Stein",
        "Yann Lecun",
        "Ken Perlin"
      ],
      "description": "We present a novel method for real-time continuous pose recovery of markerless complex articulable objects from a single depth image. Our method consists of the following stages: a randomized decision forest classifier for image segmentation, a robust method for labeled dataset generation, a convolutional network for dense feature extraction, and finally an inverse kinematics stage for stable real-time pose recovery. As one possible application of this pipeline, we show state-of-the-art results for real-time puppeteering of a skinned hand-model.",
      "citation_histogram": [
        [2014, 5],
        [2015, 46],
        [2016, 77],
        [2017, 102],
        [2018, 130],
        [2019, 122],
        [2020, 140],
        [2021, 95],
        [2022, 64]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:cFHS6HbyZ2cC",
      "title": "LeNet-5, convolutional neural networks",
      "link": "http://yann.lecun.com/exdb/lenet/",
      "year": 2015,
      "cited_by": 798,
      "authors": ["Yann LeCun"],
      "description": "Convolutional Neural Networks are are a special kind of multi-layer neural networks. Like almost every other neural networks they are trained with a version of the back-propagation algorithm. Where they differ is in the architecture.",
      "citation_histogram": [
        [2015, 6],
        [2016, 17],
        [2017, 56],
        [2018, 113],
        [2019, 139],
        [2020, 157],
        [2021, 174],
        [2022, 126]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:8baWPQ8fTxwC",
      "title": "Computing the stereo matching cost with a convolutional neural network",
      "link": "http://openaccess.thecvf.com/content_cvpr_2015/html/Zbontar_Computing_the_Stereo_2015_CVPR_paper.html",
      "year": 2014,
      "cited_by": 772,
      "authors": ["Jure \u017dbontar", "Yann LeCun"],
      "description": "We present a method for extracting depth information from a rectified image pair. We train a convolutional neural network to predict how well two image patches match and use it to compute the stereo matching cost. The cost is refined by cross-based cost aggregation and semiglobal matching, followed by a left-right consistency check to eliminate errors in the occluded regions. Our stereo method achieves an error rate of 2.61% on the KITTI stereo dataset and is currently (August 2014) the top performing method on this dataset.",
      "citation_histogram": [
        [2015, 37],
        [2016, 68],
        [2017, 115],
        [2018, 116],
        [2019, 114],
        [2020, 115],
        [2021, 117],
        [2022, 73]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Jxy3h8XkNu0C",
      "title": "MNIST handwritten digit database. 2010",
      "link": "https://scholar.google.com/scholar?cluster=12636701001560519297&hl=en&oi=scholarr",
      "year": 2010,
      "cited_by": 752,
      "authors": ["Yann LeCun", "Corinna Cortes", "Chris Burges"],
      "description": null,
      "citation_histogram": [
        [2015, 5],
        [2016, 8],
        [2017, 24],
        [2018, 71],
        [2019, 112],
        [2020, 157],
        [2021, 215],
        [2022, 152]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:d1gkVwhDpl0C",
      "title": "Efficient pattern recognition using a new transformation distance",
      "link": "https://proceedings.neurips.cc/paper/1992/hash/26408ffa703a72e8ac0117e74ad46f33-Abstract.html",
      "year": 1992,
      "cited_by": 730,
      "authors": ["Patrice Simard", "Yann LeCun", "John S Denker"],
      "description": "Memory-based classification algorithms such as radial basis func (cid: 173) tions or K-nearest neighbors typically rely on simple distances (Eu (cid: 173) clidean, dot product...), which are not particularly meaningful on pattern vectors. More complex, better suited distance measures are often expensive and rather ad-hoc (elastic matching, deformable templates). We propose a new distance measure which (a) can be made locally invariant to any set of transformations of the input and (b) can be computed efficiently. We tested the method on large handwritten character databases provided by the Post Office and the NIST. Using invariances with respect to translation, rota (cid: 173) tion, scaling, shearing and line thickness, the method consistently outperformed all other systems tested on the same databases.",
      "citation_histogram": [
        [1993, 7],
        [1994, 18],
        [1995, 10],
        [1996, 26],
        [1997, 26],
        [1998, 32],
        [1999, 25],
        [2000, 33],
        [2001, 27],
        [2002, 28],
        [2003, 21],
        [2004, 31],
        [2005, 37],
        [2006, 34],
        [2007, 39],
        [2008, 26],
        [2009, 25],
        [2010, 23],
        [2011, 23],
        [2012, 23],
        [2013, 21],
        [2014, 21],
        [2015, 25],
        [2016, 33],
        [2017, 19],
        [2018, 20],
        [2019, 18],
        [2020, 13],
        [2021, 17],
        [2022, 12]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:mB3voiENLucC",
      "title": "Learning convolutional feature hierarchies for visual recognition",
      "link": "https://proceedings.neurips.cc/paper/2010/hash/a01610228fe998f515a72dd730294d87-Abstract.html",
      "year": 2010,
      "cited_by": 701,
      "authors": [
        "Koray Kavukcuoglu",
        "Pierre Sermanet",
        "Y-Lan Boureau",
        "Karol Gregor",
        "Micha\u00ebl Mathieu",
        "Yann LeCun"
      ],
      "description": "We propose an unsupervised method for learning multi-stage hierarchies of sparse convolutional features. While sparse coding has become an increasingly popular method for learning visual features, it is most often trained at the patch level. Applying the resulting filters convolutionally results in highly redundant codes because overlapping patches are encoded in isolation. By training convolutionally over large image windows, our method reduces the redudancy between feature vectors at neighboring locations and improves the efficiency of the overall representation. In addition to a linear decoder that reconstructs the image from sparse features, our method trains an efficient feed-forward encoder that predicts quasi-sparse features from the input. While patch-based training rarely produces anything but oriented edge detectors, we show that convolutional training produces highly diverse filters, including center-surround filters, corner detectors, cross detectors, and oriented grating detectors. We show that using these filters in multi-stage convolutional network architecture improves performance on a number of visual recognition and detection tasks.",
      "citation_histogram": [
        [2010, 6],
        [2011, 20],
        [2012, 27],
        [2013, 49],
        [2014, 75],
        [2015, 95],
        [2016, 86],
        [2017, 88],
        [2018, 91],
        [2019, 49],
        [2020, 48],
        [2021, 34],
        [2022, 21]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:_FxGoFyzp5QC",
      "title": "Learning algorithms for classification: A comparison on handwritten digit recognition",
      "link": "https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf",
      "year": 1995,
      "cited_by": 689,
      "authors": [
        "Yann LeCun",
        "Lawrence D Jackel",
        "L\u00e9on Bottou",
        "Corinna Cortes",
        "John S Denker",
        "Harris Drucker",
        "Isabelle Guyon",
        "Urs A Muller",
        "Eduard Sackinger",
        "Patrice Simard",
        "Vladimir Vapnik"
      ],
      "description": "This paper compares the performance of several classifier algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also training time, recognition time, and memory requirements. When available, we report measurements of the fraction of patterns that must be rejected so that the remaining patterns have misclassification rates less than a given threshold.",
      "citation_histogram": [
        [1997, 3],
        [1998, 7],
        [1999, 5],
        [2000, 8],
        [2001, 6],
        [2002, 8],
        [2003, 11],
        [2004, 8],
        [2005, 14],
        [2006, 30],
        [2007, 20],
        [2008, 8],
        [2009, 16],
        [2010, 8],
        [2011, 6],
        [2012, 15],
        [2013, 7],
        [2014, 11],
        [2015, 20],
        [2016, 46],
        [2017, 69],
        [2018, 75],
        [2019, 85],
        [2020, 62],
        [2021, 74],
        [2022, 57]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:srBcWmd28BQC",
      "title": "Text understanding from scratch",
      "link": "https://arxiv.org/abs/1502.01710",
      "year": 2015,
      "cited_by": 682,
      "authors": ["Xiang Zhang", "Yann LeCun"],
      "description": "This article demontrates that we can apply deep learning to text understanding from character-level inputs all the way up to abstract text concepts, using temporal convolutional networks (ConvNets). We apply ConvNets to various large-scale datasets, including ontology classification, sentiment analysis, and text categorization. We show that temporal ConvNets can achieve astonishing performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language. Evidence shows that our models can work for both English and Chinese.",
      "citation_histogram": [
        [2015, 33],
        [2016, 75],
        [2017, 98],
        [2018, 133],
        [2019, 121],
        [2020, 107],
        [2021, 75],
        [2022, 33]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:RPps9qLA3-kC",
      "title": "Fast Training of Convolutional Networks through FFTs",
      "link": "https://arxiv.org/abs/1312.5851",
      "year": 2014,
      "cited_by": 655,
      "authors": ["Michael Mathieu", "Mikael Henaff", "Yann LeCun"],
      "description": "Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges.",
      "citation_histogram": [
        [2014, 14],
        [2015, 32],
        [2016, 56],
        [2017, 73],
        [2018, 100],
        [2019, 117],
        [2020, 88],
        [2021, 99],
        [2022, 69]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:QyXJ3EUuO1IC",
      "title": "Convolutional neural networks applied to house numbers digit classification",
      "link": "https://ieeexplore.ieee.org/abstract/document/6460867/",
      "year": 2012,
      "cited_by": 631,
      "authors": ["Pierre Sermanet", "Soumith Chintala", "Yann LeCun"],
      "description": "We classify digits of real-world house numbers using convolutional neural networks (ConvNets). Con-vNets are hierarchical feature learning neural networks whose structure is biologically inspired. Unlike many popular vision approaches that are hand-designed, ConvNets can automatically learn a unique set of features optimized for a given task. We augmented the traditional ConvNet architecture by learning multi-stage features and by using Lp pooling and establish a new state-of-the-art of 95.10% accuracy on the SVHN dataset (48% error improvement). Furthermore, we analyze the benefits of different pooling methods and multi-stage features in ConvNets. The source code and a tutorial are available at eblearn.sf.net.",
      "citation_histogram": [
        [2013, 24],
        [2014, 27],
        [2015, 54],
        [2016, 75],
        [2017, 75],
        [2018, 95],
        [2019, 88],
        [2020, 74],
        [2021, 68],
        [2022, 37]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:iKmOvsfbmGkC",
      "title": "Barlow twins: Self-supervised learning via redundancy reduction",
      "link": "http://proceedings.mlr.press/v139/zbontar21a.html",
      "year": 2021,
      "cited_by": 620,
      "authors": [
        "Jure Zbontar",
        "Li Jing",
        "Ishan Misra",
        "Yann LeCun",
        "St\u00e9phane Deny"
      ],
      "description": "Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow\u2019s redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.",
      "citation_histogram": [
        [2021, 192],
        [2022, 424]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:3fE2CSJIrl8C",
      "title": "Off-road obstacle avoidance through end-to-end learning",
      "link": null,
      "year": 2006,
      "cited_by": 615,
      "authors": [
        "Yann LeCun",
        "Urs Muller",
        "Jan Ben",
        "Eric Cosatto",
        "Beat Flepp"
      ],
      "description": null,
      "citation_histogram": [
        [2006, 7],
        [2007, 17],
        [2008, 9],
        [2009, 15],
        [2010, 20],
        [2011, 5],
        [2012, 3],
        [2013, 7],
        [2014, 4],
        [2015, 12],
        [2016, 22],
        [2017, 62],
        [2018, 80],
        [2019, 89],
        [2020, 104],
        [2021, 88],
        [2022, 64]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:N5tVd3kTz84C",
      "title": "Handwritten digit recognition: Applications of neural network chips and automatic learning",
      "link": "https://ieeexplore.ieee.org/abstract/document/41400/",
      "year": 1989,
      "cited_by": 613,
      "authors": [
        "Yann LeCun",
        "LD Jackel",
        "B Boser",
        "JS Denker",
        "HP Graf",
        "I Guyon",
        "D Henderson",
        "RE Howard",
        "W Hubbard"
      ],
      "description": "Two novel methods for achieving handwritten digit recognition are described. The first method is based on a neural network chip that performs line thinning and feature extraction using local template matching. The second method is implemented on a digital signal processor and makes extensive use of constrained automatic learning. Experimental results obtained using isolated handwritten digits taken from postal zip codes, a rather difficult data set, are reported and discussed.< >",
      "citation_histogram": [
        [1990, 9],
        [1991, 13],
        [1992, 11],
        [1993, 13],
        [1994, 6],
        [1995, 11],
        [1996, 9],
        [1997, 3],
        [1998, 10],
        [1999, 4],
        [2000, 8],
        [2001, 3],
        [2002, 10],
        [2003, 8],
        [2004, 3],
        [2005, 2],
        [2006, 7],
        [2007, 8],
        [2008, 9],
        [2009, 3],
        [2010, 8],
        [2011, 10],
        [2012, 9],
        [2013, 11],
        [2014, 7],
        [2015, 7],
        [2016, 18],
        [2017, 22],
        [2018, 50],
        [2019, 72],
        [2020, 76],
        [2021, 98],
        [2022, 68]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:WqliGbK-hY8C",
      "title": "Une proc\u00e9dure d'apprentissage pour r\u00e9seau \u00e0 seuil asym\u00e9trique (A learning scheme for asymmetric threshold networks)",
      "link": "https://ci.nii.ac.jp/naid/10008951599/",
      "year": 1985,
      "cited_by": 609,
      "authors": ["Yann LeCun"],
      "description": "CiNii \u8ad6\u6587 - Une procedure d'apprentissage ponr reseau a seuil asymetrique CiNii \u56fd\u7acb\u60c5\u5831\u5b66\n\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\n\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005\nID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 \n[3/11\u66f4\u65b0]2022\u5e744\u67081\u65e5\u304b\u3089\u306eCiNii Articles\u306eCiNii Research\u3078\u306e\u7d71\u5408\u306b\u3064\u3044\u3066 Une procedure \nd'apprentissage ponr reseau a seuil asymetrique LECUN Y. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 LECUN Y. \n\u53ce\u9332\u520a\u884c\u7269 Proceedings of Cognitiva 85 Proceedings of Cognitiva 85, 599-604, 1985 \u88ab\u5f15\u7528\n\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Discovering Neural Nets with Low Kolmogorov Complexity and \nHigh Generalization Capability SCHMIDHUBER Jurgen Neural networks : the official journal \nof the International Neural Network Society 10(5), 857-873, 1997-07-01 \u53c2\u8003\u6587\u732e86\u4ef6 \u2026",
      "citation_histogram": [
        [1986, 5],
        [1987, 2],
        [1988, 9],
        [1989, 8],
        [1990, 15],
        [1991, 23],
        [1992, 15],
        [1993, 29],
        [1994, 18],
        [1995, 14],
        [1996, 15],
        [1997, 15],
        [1998, 14],
        [1999, 15],
        [2000, 15],
        [2001, 10],
        [2002, 16],
        [2003, 11],
        [2004, 8],
        [2005, 16],
        [2006, 16],
        [2007, 16],
        [2008, 14],
        [2009, 6],
        [2010, 13],
        [2011, 14],
        [2012, 13],
        [2013, 20],
        [2014, 13],
        [2015, 20],
        [2016, 22],
        [2017, 37],
        [2018, 39],
        [2019, 23],
        [2020, 20],
        [2021, 25],
        [2022, 12]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:JTutsjMeBaAC",
      "title": "Deep learning with elastic averaging SGD",
      "link": "https://proceedings.neurips.cc/paper/2015/hash/d18f655c3fce66ca401d5f38b48c89af-Abstract.html",
      "year": 2015,
      "cited_by": 602,
      "authors": ["Sixin Zhang", "Anna E Choromanska", "Yann LeCun"],
      "description": "We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, ie the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very\u00a0\u2026",
      "citation_histogram": [
        [2015, 9],
        [2016, 37],
        [2017, 66],
        [2018, 89],
        [2019, 104],
        [2020, 118],
        [2021, 103],
        [2022, 70]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:YsrPvlHIBpEC",
      "title": "Measuring the VC-dimension of a learning machine",
      "link": "https://ieeexplore.ieee.org/abstract/document/6795928/",
      "year": 1994,
      "cited_by": 599,
      "authors": ["Vladimir Vapnik", "Esther Levin", "Yann Le Cun"],
      "description": "A method for measuring the capacity of learning machines is described. The method is based on fitting a theoretically derived function to empirical measurements of the maximal difference between the error rates on two separate data sets of varying sizes. Experimental measurements of the capacity of various types of linear classifiers are presented.",
      "citation_histogram": [
        [1994, 3],
        [1995, 10],
        [1996, 5],
        [1997, 4],
        [1998, 6],
        [1999, 14],
        [2000, 8],
        [2001, 9],
        [2002, 16],
        [2003, 11],
        [2004, 20],
        [2005, 12],
        [2006, 19],
        [2007, 28],
        [2008, 22],
        [2009, 26],
        [2010, 27],
        [2011, 23],
        [2012, 28],
        [2013, 22],
        [2014, 37],
        [2015, 28],
        [2016, 38],
        [2017, 20],
        [2018, 23],
        [2019, 33],
        [2020, 34],
        [2021, 32],
        [2022, 28]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:E9jS3u5z5twC",
      "title": "Entropy-sgd: Biasing gradient descent into wide valleys",
      "link": "https://iopscience.iop.org/article/10.1088/1742-5468/ab39d9/meta",
      "year": 2016,
      "cited_by": 556,
      "authors": [
        "Pratik Chaudhari",
        "Anna Choromanska",
        "Stefano Soatto",
        "Yann LeCun",
        "Carlo Baldassi",
        "Christian Borgs",
        "Jennifer Chayes",
        "Levent Sagun",
        "Riccardo Zecchina"
      ],
      "description": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under\u00a0\u2026",
      "citation_histogram": [
        [2018, 93],
        [2019, 92],
        [2020, 124],
        [2021, 122],
        [2022, 91]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:uWiczbcajpAC",
      "title": "Large Scale Online Learning.",
      "link": "https://proceedings.neurips.cc/paper/2365-large-scale-online-learning",
      "year": 2003,
      "cited_by": 531,
      "authors": ["Leon Bottou", "Yann LeCun"],
      "description": "We consider situations where training data is abundant and computing resources are comparatively scarce. We argue that suitably designed on-line learning algorithms asymptotically outperform any batch learning algorithm. Both theoretical and experimental evidences are presented.",
      "citation_histogram": [
        [2004, 2],
        [2005, 2],
        [2006, 8],
        [2007, 10],
        [2008, 8],
        [2009, 17],
        [2010, 13],
        [2011, 15],
        [2012, 34],
        [2013, 40],
        [2014, 32],
        [2015, 35],
        [2016, 28],
        [2017, 40],
        [2018, 44],
        [2019, 43],
        [2020, 68],
        [2021, 53],
        [2022, 34]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:ye4kPcJQO24C",
      "title": "Improving the Convergence of Back-Propagation Learning with",
      "link": "https://www.researchgate.net/profile/Suzanna-Becker/publication/216792889_Improving_the_Convergence_of_Back-Propagation_Learning_with_Second-Order_Methods/links/00b49526e57b930da1000000/Improving-the-Convergence-of-Back-Propagation-Learning-with-Second-Order-Methods.pdf",
      "year": 1988,
      "cited_by": 531,
      "authors": ["Sue Becker", "Yann Le Cun"],
      "description": "Back-propagation has proven to be a robust algorithm for difficult connectionist learning problems. However, as with many gradient based optimization methods, it converges slowly. We describe an extension of the back-propagation algorithm which uses a simple approximation to the second derivative terms. This method is shown to reduce the required number of iterations to learn a random classification problem, w\u0131th only a small increase in the complex-ity of each iteration.The back-propagation learning algorithm for multilayer connectionist networks performs a gradient descent search in weight space for a minimum of some cost function C (which is frequently the mean squared error between actual and desired outputs). A general drawback of gradient-based numerical op-timizat\u0131on methods is their slow convergence. In connectionist learning problems in particular, one typically starts a long way from the\u00a0\u2026",
      "citation_histogram": [
        [1988, 2],
        [1989, 7],
        [1990, 14],
        [1991, 17],
        [1992, 23],
        [1993, 27],
        [1994, 23],
        [1995, 24],
        [1996, 23],
        [1997, 16],
        [1998, 22],
        [1999, 17],
        [2000, 11],
        [2001, 9],
        [2002, 18],
        [2003, 12],
        [2004, 5],
        [2005, 7],
        [2006, 5],
        [2007, 10],
        [2008, 3],
        [2009, 14],
        [2010, 9],
        [2011, 4],
        [2012, 13],
        [2013, 6],
        [2014, 17],
        [2015, 22],
        [2016, 10],
        [2017, 15],
        [2018, 28],
        [2019, 23],
        [2020, 23],
        [2021, 24],
        [2022, 18]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:_Xy5tTOxz_oC",
      "title": "Transformation invariance in pattern recognition\u2014tangent distance and tangent propagation",
      "link": "https://link.springer.com/chapter/10.1007/3-540-49430-8_13",
      "year": 1998,
      "cited_by": 521,
      "authors": [
        "Patrice Y Simard",
        "Yann A LeCun",
        "John S Denker",
        "Bernard Victorri"
      ],
      "description": " In pattern recognition, statistical modeling, or regression, the amount of data is a critical factor a.ecting the performance. If the amount of data and computational resources are unlimited, even trivial algorithms will converge to the optimal solution. However, in the practical case, given limited data and other resources, satisfactory performance requires sophisticated methods to regularize the problem by introducing a priori knowledge. Invariance of the output with respect to certain transformations of the input is a typical example of such a priori knowledge. In this chapter, we introduce the concept of tangent vectors, which compactly represent the essence of these transformation invariances, and two classes of algorithms, \u201ctangent distance\u201d and \u201ctangent propagation\u201d, which make use of these invariances to improve performance.",
      "citation_histogram": [
        [2000, 6],
        [2001, 9],
        [2002, 21],
        [2003, 14],
        [2004, 13],
        [2005, 20],
        [2006, 26],
        [2007, 30],
        [2008, 20],
        [2009, 28],
        [2010, 20],
        [2011, 14],
        [2012, 31],
        [2013, 24],
        [2014, 12],
        [2015, 15],
        [2016, 17],
        [2017, 23],
        [2018, 26],
        [2019, 34],
        [2020, 46],
        [2021, 37],
        [2022, 33]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:W5xh706n7nkC",
      "title": "Synergistic face detection and pose estimation with energy-based model",
      "link": "https://www.jmlr.org/papers/volume8/osadchy07a/osadchy07a.pdf",
      "year": 2005,
      "cited_by": 514,
      "authors": ["Margarita Osadchy", "Yann LeCun", "Matthew L Miller"],
      "description": "We describe a novel method for simultaneously detecting faces and estimating their pose in real time. The method employs a convolutional network to map images of faces to points on a lowdimensional manifold parametrized by pose, and images of non-faces to points far away from that manifold. Given an image, detecting a face and estimating its pose is viewed as minimizing an energy function with respect to the face/non-face binary variable and the continuous pose parameters. The system is trained to minimize a loss function that drives correct combinations of labels and pose to be associated with lower energy values than incorrect ones. The system is designed to handle very large range of poses without retraining. The performance of the system was tested on three standard data sets\u2014for frontal views, rotated faces, and profiles\u2014is comparable to previous systems that are designed to handle a single one of these data sets. We show that a system trained simuiltaneously for detection and pose estimation is more accurate on both tasks than similar systems trained for each task separately. 1",
      "citation_histogram": [
        [2005, 6],
        [2006, 19],
        [2007, 16],
        [2008, 20],
        [2009, 20],
        [2010, 25],
        [2011, 29],
        [2012, 25],
        [2013, 27],
        [2014, 43],
        [2015, 52],
        [2016, 38],
        [2017, 49],
        [2018, 43],
        [2019, 33],
        [2020, 25],
        [2021, 28],
        [2022, 7]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:XiSMed-E-HIC",
      "title": "NeuFlow: A Runtime Reconfigurable Dataflow Processor for Vision",
      "link": "https://ieeexplore.ieee.org/abstract/document/5981829/",
      "year": 2011,
      "cited_by": 490,
      "authors": [
        "Cl\u00e9ment Farabet",
        "Berin Martini",
        "Benoit Corda",
        "Polina Akselrod",
        "Eugenio Culurciello",
        "Yann LeCun"
      ],
      "description": "In this paper we present a scalable dataflow hardware architecture optimized for the computation of general-purpose vision algorithms - neuFlow - and a dataflow compiler - luaFlow - that transforms high-level flow-graph representations of these algorithms into machine code for neuFlow. This system was designed with the goal of providing real-time detection, categorization and localization of objects in complex scenes, while consuming 10 Watts when implemented on a Xilinx Virtex 6 FPGA platform, or about ten times less than a laptop computer, and producing speedups of up to 100 times in real-world applications. We present an application of the system on street scene analysis, segmenting 20 categories on 500 \u00d7 375 frames at 12 frames per second on our custom hardware neuFlow.",
      "citation_histogram": [
        [2011, 2],
        [2012, 3],
        [2013, 13],
        [2014, 27],
        [2015, 41],
        [2016, 63],
        [2017, 47],
        [2018, 82],
        [2019, 65],
        [2020, 64],
        [2021, 48],
        [2022, 21]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:HJSXoJQnj-YC",
      "title": "Indoor Semantic Segmentation using depth information",
      "link": "https://arxiv.org/abs/1301.3572",
      "year": 2013,
      "cited_by": 486,
      "authors": [
        "Camille Couprie",
        "Cl\u00e9ment Farabet",
        "Laurent Najman",
        "Yann LeCun"
      ],
      "description": "This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. We obtain state-of-the-art on the NYU-v2 depth dataset with an accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos sequences that could be processed in real-time using appropriate hardware such as an FPGA.",
      "citation_histogram": [
        [2013, 4],
        [2014, 23],
        [2015, 47],
        [2016, 58],
        [2017, 72],
        [2018, 61],
        [2019, 54],
        [2020, 55],
        [2021, 54],
        [2022, 40]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:_Ycx_gLZKoMC",
      "title": "Neural networks: Tricks of the trade",
      "link": "https://scholar.google.com/scholar?cluster=2374284198495528803&hl=en&oi=scholarr",
      "year": 1998,
      "cited_by": 484,
      "authors": [
        "Yann LeCun",
        "Leon Bottou",
        "Genevieve B Orr",
        "Klaus-Robert M\u00fcller"
      ],
      "description": null,
      "citation_histogram": [
        [2006, 3],
        [2007, 2],
        [2008, 4],
        [2009, 4],
        [2010, 2],
        [2011, 1],
        [2012, 4],
        [2013, 2],
        [2014, 6],
        [2015, 15],
        [2016, 45],
        [2017, 50],
        [2018, 51],
        [2019, 72],
        [2020, 66],
        [2021, 84],
        [2022, 67]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:NDuN12AVoxsC",
      "title": "No more pesky learning rates",
      "link": "http://proceedings.mlr.press/v28/schaul13.html?ref=https://githubhelp.com",
      "year": 2012,
      "cited_by": 481,
      "authors": ["Tom Schaul", "Sixin Zhang", "Yann LeCun"],
      "description": "The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of the best settings obtained through systematic search, and effectively removes the need for learning rate tuning.",
      "citation_histogram": [
        [2012, 6],
        [2013, 17],
        [2014, 36],
        [2015, 36],
        [2016, 55],
        [2017, 59],
        [2018, 57],
        [2019, 64],
        [2020, 55],
        [2021, 62],
        [2022, 27]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:IWHjjKOFINEC",
      "title": "Cnp: An fpga-based processor for convolutional networks",
      "link": "https://ieeexplore.ieee.org/abstract/document/5272559/",
      "year": 2009,
      "cited_by": 468,
      "authors": [
        "Cl\u00e9ment Farabet",
        "Cyril Poulet",
        "Jefferson Y Han",
        "Yann LeCun"
      ],
      "description": "Convolutional networks (ConvNets) are biologically inspired hierarchical architectures that can be trained to perform a variety of detection, recognition and segmentation tasks. ConvNets have a feed-forward architecture consisting of multiple linear convolution filters interspersed with pointwise non-linear squashing functions. This paper presents an efficient implementation of ConvNets on a low-end DSP-oriented field programmable gate array (FPGA). The implementation exploits the inherent parallelism of ConvNets and takes full advantage of multiple hardware multiply accumulate units on the FPGA. The entire system uses a single FPGA with an external memory module, and no extra parts. A network compiler software was implemented, which takes a description of a trained ConvNet and compiles it into a sequence of instructions for the ConvNet Processor (CNP). A ConvNet face detection system was\u00a0\u2026",
      "citation_histogram": [
        [2009, 2],
        [2010, 11],
        [2011, 15],
        [2012, 10],
        [2013, 3],
        [2014, 7],
        [2015, 24],
        [2016, 51],
        [2017, 71],
        [2018, 80],
        [2019, 67],
        [2020, 50],
        [2021, 43],
        [2022, 23]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:0xcHesCNKywC",
      "title": "Disentangling factors of variation in deep representation using adversarial training",
      "link": "https://proceedings.neurips.cc/paper/2016/hash/ef0917ea498b1665ad6c701057155abe-Abstract.html",
      "year": 2016,
      "cited_by": 467,
      "authors": [
        "Michael F Mathieu",
        "Junbo Jake Zhao",
        "Junbo Zhao",
        "Aditya Ramesh",
        "Pablo Sprechmann",
        "Yann LeCun"
      ],
      "description": "We propose a deep generative model for learning to distill the hidden factors of variation within a set of labeled observations into two complementary codes. One code describes the factors of variation relevant to solving a specified task. The other code describes the remaining factors of variation that are irrelevant to solving this task. The only available source of supervision during the training process comes from our ability to distinguish among different observations belonging to the same category. Concrete examples include multiple images of the same object from different viewpoints, or multiple speech samples from the same speaker. In both of these instances, the factors of variation irrelevant to classification are implicitly expressed by intra-class variabilities, such as the relative position of an object in an image, or the linguistic content of an utterance. Most existing approaches for solving this problem rely heavily on having access to pairs of observations only sharing a single factor of variation, eg different objects observed in the exact same conditions. This assumption is often not encountered in realistic settings where data acquisition is not controlled and labels for the uninformative components are not available. In this work, we propose to overcome this limitation by augmenting deep convolutional autoencoders with a form of adversarial training. Both factors of variation are implicitly captured in the organization of the learned embedding space, and can be used for solving single-image analogies. Experimental results on synthetic and real datasets show that the proposed method is capable of disentangling the influences of style and content\u00a0\u2026",
      "citation_histogram": [
        [2017, 31],
        [2018, 70],
        [2019, 121],
        [2020, 112],
        [2021, 86],
        [2022, 43]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:zYLM7Y9cAGgC",
      "title": "Boosting and other ensemble methods",
      "link": "https://ieeexplore.ieee.org/abstract/document/6795389/",
      "year": 1994,
      "cited_by": 463,
      "authors": [
        "Harris Drucker",
        "Corinna Cortes",
        "Lawrence D Jackel",
        "Yann LeCun",
        "Vladimir Vapnik"
      ],
      "description": "We compare the performance of three types of neural network-based ensemble techniques to that of a single neural network. The ensemble algorithms are two versions of boosting and committees of neural networks trained independently. For each of the four algorithms, we experimentally determine the test and training error curves in an optical character recognition (OCR) problem as both a function of training set size and computational cost using three architectures. We show that a single machine is best for small training set size while for large training set size some version of boosting is best. However, for a given computational cost, boosting is always best. Furthermore, we show a surprising result for the original boosting algorithm: namely, that as the training set size increases, the training error decreases until it asymptotes to the test error rate. This has potential implications in the search for better training\u00a0\u2026",
      "citation_histogram": [
        [1995, 2],
        [1996, 16],
        [1997, 22],
        [1998, 16],
        [1999, 13],
        [2000, 9],
        [2001, 15],
        [2002, 13],
        [2003, 28],
        [2004, 19],
        [2005, 17],
        [2006, 17],
        [2007, 13],
        [2008, 16],
        [2009, 12],
        [2010, 12],
        [2011, 20],
        [2012, 12],
        [2013, 17],
        [2014, 15],
        [2015, 15],
        [2016, 22],
        [2017, 19],
        [2018, 20],
        [2019, 17],
        [2020, 16],
        [2021, 25],
        [2022, 21]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:qxL8FJ1GzNcC",
      "title": "Large-scale learning with svm and convolutional nets for generic object categorization",
      "link": null,
      "year": 2006,
      "cited_by": 454,
      "authors": ["Fu Jie Huang", "Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [2006, 4],
        [2007, 16],
        [2008, 5],
        [2009, 14],
        [2010, 7],
        [2011, 13],
        [2012, 8],
        [2013, 17],
        [2014, 22],
        [2015, 19],
        [2016, 44],
        [2017, 51],
        [2018, 66],
        [2019, 61],
        [2020, 50],
        [2021, 30],
        [2022, 21]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:L8Ckcad2t8MC",
      "title": "Classification of patterns of EEG synchronization for seizure prediction",
      "link": "https://www.sciencedirect.com/science/article/pii/S1388245709005264",
      "year": 2009,
      "cited_by": 448,
      "authors": [
        "Piotr Mirowski",
        "Deepak Madhavan",
        "Yann LeCun",
        "Ruben Kuzniecky"
      ],
      "description": "ObjectiveResearch in seizure prediction from intracranial EEG has highlighted the usefulness of bivariate measures of brainwave synchronization. Spatio-temporal bivariate features are very high-dimensional and cannot be analyzed with conventional statistical methods. Hence, we propose state-of-the-art machine learning methods that handle high-dimensional inputs.MethodsWe computed bivariate features of EEG synchronization (cross-correlation, nonlinear interdependence, dynamical entrainment or wavelet synchrony) on the 21-patient Freiburg dataset. Features from all channel pairs and frequencies were aggregated over consecutive time points, to form patterns. Patient-specific machine learning-based classifiers (support vector machines, logistic regression or convolutional neural networks) were trained to discriminate interictal from preictal patterns of features. In this explorative study, we evaluated out-of\u00a0\u2026",
      "citation_histogram": [
        [2010, 9],
        [2011, 21],
        [2012, 15],
        [2013, 21],
        [2014, 20],
        [2015, 44],
        [2016, 41],
        [2017, 51],
        [2018, 58],
        [2019, 44],
        [2020, 58],
        [2021, 38],
        [2022, 23]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:JQOojiI6XY0C",
      "title": "Learning invariant features through topographic filter maps",
      "link": "https://ieeexplore.ieee.org/abstract/document/5206545/",
      "year": 2009,
      "cited_by": 431,
      "authors": [
        "Koray Kavukcuoglu",
        "Marc\u2019Aurelio Ranzato",
        "Rob Fergus",
        "Yann LeCun"
      ],
      "description": "Several recently-proposed architectures for high-performance object recognition are composed of two main stages: a feature extraction stage that extracts locally-invariant feature vectors from regularly spaced image patches, and a somewhat generic supervised classifier. The first stage is often composed of three main modules: (1) a bank of filters (often oriented edge detectors); (2) a non-linear transform, such as a point-wise squashing functions, quantization, or normalization; (3) a spatial pooling operation which combines the outputs of similar filters over neighboring regions. We propose a method that automatically learns such feature extractors in an unsupervised fashion by simultaneously learning the filters and the pooling units that combine multiple filter outputs together. The method automatically generates topographic maps of similar filters that extract features of orientations, scales, and positions. These\u00a0\u2026",
      "citation_histogram": [
        [2009, 7],
        [2010, 35],
        [2011, 39],
        [2012, 38],
        [2013, 39],
        [2014, 53],
        [2015, 52],
        [2016, 43],
        [2017, 28],
        [2018, 15],
        [2019, 26],
        [2020, 18],
        [2021, 21],
        [2022, 7]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:PR6Y55bgFSsC",
      "title": "Original approach for the localisation of objects in images",
      "link": "https://digital-library.theiet.org/content/journals/10.1049/ip-vis_19941301",
      "year": 1994,
      "cited_by": 426,
      "authors": ["R\u00e9gis Vaillant", "Christophe Monrocq", "Yann LeCun"],
      "description": "An original approach is presented for the localisation of objects in an image which approach is neuronal and has two steps. In the first step, a rough localisation is performed by presenting each pixel with its neighbourhood to a neural net which is able to indicate whether this pixel and its neighbourhood are the image of the search object. This first filter does not discriminate for position. From its result, areas which might contain an image of the object can be selected. In the second step, these areas are presented to another neural net which can determine the exact position of the object in each area. This algorithm is applied to the problem of localising faces in images.",
      "citation_histogram": [
        [1995, 3],
        [1996, 7],
        [1997, 7],
        [1998, 7],
        [1999, 5],
        [2000, 8],
        [2001, 9],
        [2002, 7],
        [2003, 6],
        [2004, 8],
        [2005, 6],
        [2006, 1],
        [2007, 6],
        [2008, 7],
        [2009, 5],
        [2010, 8],
        [2011, 6],
        [2012, 7],
        [2013, 9],
        [2014, 15],
        [2015, 24],
        [2016, 26],
        [2017, 36],
        [2018, 38],
        [2019, 49],
        [2020, 43],
        [2021, 45],
        [2022, 16]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Wp0gIr-vW9MC",
      "title": "Learning long\u2010range vision for autonomous off\u2010road driving",
      "link": "https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20276",
      "year": 2009,
      "cited_by": 424,
      "authors": [
        "Raia Hadsell",
        "Pierre Sermanet",
        "Jan Ben",
        "Ayse Erkan",
        "Marco Scoffier",
        "Koray Kavukcuoglu",
        "Urs Muller",
        "Yann LeCun"
      ],
      "description": " Most vision\u2010based approaches to mobile robotics suffer from the limitations imposed by stereo obstacle detection, which is short range and prone to failure. We present a self\u2010supervised learning process for long\u2010range vision that is able to accurately classify complex terrain at distances up to the horizon, thus allowing superior strategic planning. The success of the learning process is due to the self\u2010supervised training data that are generated on every frame: robust, visually consistent labels from a stereo module; normalized wide\u2010context input windows; and a discriminative and concise feature representation. A deep hierarchical network is trained to extract informative and meaningful features from an input image, and the features are used to train a real\u2010time classifier to predict traversability. The trained classifier sees obstacles and paths from 5 to more than 100 m, far beyond the maximum stereo range of 12 m\u00a0\u2026",
      "citation_histogram": [
        [2009, 5],
        [2010, 26],
        [2011, 15],
        [2012, 18],
        [2013, 24],
        [2014, 13],
        [2015, 23],
        [2016, 35],
        [2017, 45],
        [2018, 59],
        [2019, 59],
        [2020, 47],
        [2021, 36],
        [2022, 13]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:3YTBxAczQXIC",
      "title": "The role of over-parametrization in generalization of neural networks",
      "link": null,
      "year": 2019,
      "cited_by": 415,
      "authors": [
        "Behnam Neyshabur",
        "Zhiyuan Li",
        "Srinadh Bhojanapalli",
        "Yann LeCun",
        "Nathan Srebro"
      ],
      "description": null,
      "citation_histogram": [
        [2018, 11],
        [2019, 79],
        [2020, 127],
        [2021, 115],
        [2022, 81]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:b15ltGHXyxQC",
      "title": "Mod\u00e8les connexionnistes de l\u2019apprentissage",
      "link": "https://www.persee.fr/doc/intel_0769-4113_1987_num_2_1_1804",
      "year": 1987,
      "cited_by": 406,
      "authors": ["Yann LeCun"],
      "description": "L'apprentissage est \u00e0 la fois l\u2019un des probl\u00e8mes les plus anciens et les moins bien r\u00e9solus de l'Intelligence Artificielle. Ceci pourrait expliquer le nombre et la diversit\u00e9 des m\u00e9thodes mises en oeuvre pour le r\u00e9soudre. De nombreuses m\u00e9thodes actuelles font r\u00e9f\u00e9rence \u00e0 des travaux datant du d\u00e9but des ann\u00e9es soixante [5]. Une nouvelle classe de m\u00e9thodes, dites connexion/) istefi apparue r\u00e9cemment, n'\u00e9chappe pas \u00e0 la r\u00e8gle, et est souvent pr\u00e9sent\u00e9e comme l'h\u00e9riti\u00e8re d\u2019id\u00e9es ayant germ\u00e9 \u00e0 la fin des ann\u00e9es cinquante. Elle s' inspire en partie des mod\u00e8les d\u2019apprentissage statistique dont le repr\u00e9sentant le plus simple est le classifieur lin\u00e9aire. De nombreux travaux on montr\u00e9 que les capacit\u00e9s d'un classifieur lin\u00e9aire seul sont malheureusement tr\u00e8s limit\u00e9es [6.7],Cette limitation est \u00e0 la base de l\u2019\u00e9chec du perceptron propos\u00e9 par Rosenblatt [30], \u00e9tudi\u00e9 par Nilsson [27], Minsky & Papert [25]: Le perceptron est une\u00a0\u2026",
      "citation_histogram": [
        [1986, 1],
        [1987, 1],
        [1988, 7],
        [1989, 12],
        [1990, 16],
        [1991, 12],
        [1992, 7],
        [1993, 8],
        [1994, 9],
        [1995, 9],
        [1996, 8],
        [1997, 14],
        [1998, 10],
        [1999, 13],
        [2000, 5],
        [2001, 6],
        [2002, 6],
        [2003, 1],
        [2004, 7],
        [2005, 5],
        [2006, 3],
        [2007, 8],
        [2008, 6],
        [2009, 6],
        [2010, 4],
        [2011, 4],
        [2012, 7],
        [2013, 5],
        [2014, 7],
        [2015, 9],
        [2016, 7],
        [2017, 24],
        [2018, 21],
        [2019, 28],
        [2020, 34],
        [2021, 41],
        [2022, 26]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:BUYA1_V_uYcC",
      "title": "Synergistic face detection and pose estimation with energy-based models",
      "link": "https://www.jmlr.org/papers/volume8/osadchy07a/osadchy07a.pdf",
      "year": 2007,
      "cited_by": 393,
      "authors": ["Margarita Osadchy", "Yann LeCun", "Matthew L Miller"],
      "description": "We describe a novel method for simultaneously detecting faces and estimating their pose in real time. The method employs a convolutional network to map images of faces to points on a lowdimensional manifold parametrized by pose, and images of non-faces to points far away from that manifold. Given an image, detecting a face and estimating its pose is viewed as minimizing an energy function with respect to the face/non-face binary variable and the continuous pose parameters. The system is trained to minimize a loss function that drives correct combinations of labels and pose to be associated with lower energy values than incorrect ones. The system is designed to handle very large range of poses without retraining. The performance of the system was tested on three standard data sets\u2014for frontal views, rotated faces, and profiles\u2014is comparable to previous systems that are designed to handle a single one of these data sets. We show that a system trained simuiltaneously for detection and pose estimation is more accurate on both tasks than similar systems trained for each task separately. 1",
      "citation_histogram": [
        [2006, 1],
        [2007, 3],
        [2008, 14],
        [2009, 14],
        [2010, 19],
        [2011, 23],
        [2012, 19],
        [2013, 21],
        [2014, 37],
        [2015, 41],
        [2016, 36],
        [2017, 43],
        [2018, 40],
        [2019, 31],
        [2020, 19],
        [2021, 22],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Y0pCki6q_DkC",
      "title": "High quality document image compression with DjVu",
      "link": "https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-7/issue-3/0000/High-quality-document-image-compression-with-DjVu/10.1117/1.482609.short",
      "year": 1998,
      "cited_by": 373,
      "authors": [
        "Leon Bottou",
        "Patrick Haffner",
        "Paul G Howard",
        "Patrice Simard",
        "Yoshua Bengio",
        "Yann Le Cun"
      ],
      "description": "We present a new image compression technique called \u2018\u2018DjVu\u2019\u2019that is specifically geared towards the compression of highresolution, high-quality images of scanned documents in color. This enables fast transmission of document images over low-speed connections, while faithfully reproducing the visual aspect of the document, including color, fonts, pictures, and paper texture. The DjVu compressor separates the text and drawings, which need a high spatial resolution, from the pictures and backgrounds, which are smoother and can be coded at a lower spatial resolution. Then, several novel techniques are used to maximize the compression ratio: the bi-level foreground image is encoded with AT&T\u2019s proposal to the new JBIG2 fax standard, and a new wavelet-based compression method is used for the backgrounds and pictures. Both techniques use a new adaptive binary arithmetic coder called the ZP-coder. A\u00a0\u2026",
      "citation_histogram": [
        [1998, 2],
        [1999, 10],
        [2000, 14],
        [2001, 15],
        [2002, 18],
        [2003, 18],
        [2004, 17],
        [2005, 15],
        [2006, 23],
        [2007, 18],
        [2008, 17],
        [2009, 18],
        [2010, 12],
        [2011, 23],
        [2012, 16],
        [2013, 23],
        [2014, 27],
        [2015, 12],
        [2016, 16],
        [2017, 15],
        [2018, 17],
        [2019, 6],
        [2020, 6],
        [2021, 5],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:8AbLer7MMksC",
      "title": "Ask the locals: multi-way local pooling for image recognition",
      "link": "https://ieeexplore.ieee.org/abstract/document/6126555/",
      "year": 2011,
      "cited_by": 359,
      "authors": [
        "Y-Lan Boureau",
        "Nicolas Le Roux",
        "Francis Bach",
        "Jean Ponce",
        "Yann LeCun"
      ],
      "description": "Invariant representations in object recognition systems are generally obtained by pooling feature vectors over spatially local neighborhoods. But pooling is not local in the feature vector space, so that widely dissimilar features may be pooled together if they are in nearby locations. Recent approaches rely on sophisticated encoding methods and more specialized codebooks (or dictionaries), e.g., learned on subsets of descriptors which are close in feature space, to circumvent this problem. In this work, we argue that a common trait found in much recent work in image recognition or retrieval is that it leverages locality in feature space on top of purely spatial locality. We propose to apply this idea in its simplest form to an object recognition system based on the spatial pyramid framework, to increase the performance of small dictionaries with very little added engineering. State-of-the-art results on several object\u00a0\u2026",
      "citation_histogram": [
        [2011, 3],
        [2012, 31],
        [2013, 47],
        [2014, 49],
        [2015, 40],
        [2016, 39],
        [2017, 35],
        [2018, 31],
        [2019, 18],
        [2020, 19],
        [2021, 25],
        [2022, 10]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:zGdJYJv2LkUC",
      "title": "Learning process in an asymmetric threshold network",
      "link": "https://link.springer.com/chapter/10.1007/978-3-642-82657-3_24",
      "year": 1986,
      "cited_by": 356,
      "authors": ["Yann Le Cun"],
      "description": " Threshold functions and related operators are widely used as basic elements of adaptive and associative networks [Nakano 72, Amari 72, Hopfield 82]. There exist numerous learning rules for finding a set of weights to achieve a particular correspondence between input-output pairs. But early works in the field have shown that the number of threshold functions (or linearly separable functions) in N binary variables is small compared to the number of all possible boolean mappings in N variables, especially if N is large. This problem is one of the main limitations of most neural networks models where the state is fully specified by the environment during learning: they can only learn linearly separable functions of their inputs. Moreover, a learning procedure which requires the outside world to specify the state of every neuron during the learning session can hardly be considered as a general learning rule\u00a0\u2026",
      "citation_histogram": [
        [1986, 3],
        [1987, 1],
        [1988, 4],
        [1989, 7],
        [1990, 7],
        [1991, 15],
        [1992, 5],
        [1993, 11],
        [1994, 12],
        [1995, 9],
        [1996, 9],
        [1997, 4],
        [1998, 10],
        [1999, 12],
        [2000, 6],
        [2001, 10],
        [2002, 6],
        [2003, 6],
        [2004, 6],
        [2005, 3],
        [2006, 12],
        [2007, 7],
        [2008, 5],
        [2009, 3],
        [2010, 7],
        [2011, 4],
        [2012, 7],
        [2013, 17],
        [2014, 10],
        [2015, 11],
        [2016, 11],
        [2017, 10],
        [2018, 26],
        [2019, 21],
        [2020, 22],
        [2021, 20],
        [2022, 13]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:k_IJM867U9cC",
      "title": "Hardware accelerated convolutional neural networks for synthetic vision systems",
      "link": "https://ieeexplore.ieee.org/abstract/document/5537908/",
      "year": 2010,
      "cited_by": 355,
      "authors": [
        "Cl\u00e9ment Farabet",
        "Berin Martini",
        "Polina Akselrod",
        "Sel\u00e7uk Talay",
        "Yann LeCun",
        "Eugenio Culurciello"
      ],
      "description": "In this paper we present a scalable hardware architecture to implement large-scale convolutional neural networks and state-of-the-art multi-layered artificial vision systems. This system is fully digital and is a modular vision engine with the goal of performing real-time detection, recognition and segmentation of mega-pixel images. We present a performance comparison between a software, FPGA and ASIC implementation that shows a speed up in custom hardware implementations.",
      "citation_histogram": [
        [2010, 3],
        [2011, 10],
        [2012, 8],
        [2013, 11],
        [2014, 14],
        [2015, 30],
        [2016, 55],
        [2017, 40],
        [2018, 48],
        [2019, 42],
        [2020, 43],
        [2021, 29],
        [2022, 17]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:1tvASLRm6poC",
      "title": "Fast Convolutional Nets With fbfft: A GPU Performance Evaluation",
      "link": "https://arxiv.org/abs/1412.7580",
      "year": 2014,
      "cited_by": 353,
      "authors": [
        "Nicolas Vasilache",
        "Jeff Johnson",
        "Michael Mathieu",
        "Soumith Chintala",
        "Serkan Piantino",
        "Yann LeCun"
      ],
      "description": "We examine the performance profile of Convolutional Neural Network training on the current generation of NVIDIA Graphics Processing Units. We introduce two new Fast Fourier Transform convolution implementations: one based on NVIDIA's cuFFT library, and another based on a Facebook authored FFT implementation, fbfft, that provides significant speedups over cuFFT (over 1.5x) for whole CNNs. Both of these convolution implementations are available in open source, and are faster than NVIDIA's cuDNN implementation for many common convolutional layers (up to 23.5x for some synthetic kernel configurations). We discuss different performance regimes of convolutions, comparing areas where straightforward time domain convolutions outperform Fourier frequency domain convolutions. Details on algorithmic applications of NVIDIA GPU hardware specifics in the implementation of fbfft are also provided.",
      "citation_histogram": [
        [2014, 1],
        [2015, 23],
        [2016, 46],
        [2017, 43],
        [2018, 51],
        [2019, 61],
        [2020, 44],
        [2021, 42],
        [2022, 32]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:7PzlFSSx8tAC",
      "title": "Toward automatic phenotyping of developing embryos from videos",
      "link": "https://ieeexplore.ieee.org/abstract/document/1495508/",
      "year": 2005,
      "cited_by": 345,
      "authors": [
        "Feng Ning",
        "Damien Delhomme",
        "Yann LeCun",
        "Fabio Piano",
        "L\u00e9on Bottou",
        "Paolo Emilio Barbano"
      ],
      "description": "We describe a trainable system for analyzing videos of developing C. elegans embryos. The system automatically detects, segments, and locates cells and nuclei in microscopic images. The system was designed as the central component of a fully automated phenotyping system. The system contains three modules 1) a convolutional network trained to classify each pixel into five categories: cell wall, cytoplasm, nucleus membrane, nucleus, outside medium; 2) an energy-based model, which cleans up the output of the convolutional network by learning local consistency constraints that must be satisfied by label images; 3) a set of elastic models of the embryo at various stages of development that are matched to the label images.",
      "citation_histogram": [
        [2006, 4],
        [2007, 6],
        [2008, 2],
        [2009, 4],
        [2010, 9],
        [2011, 5],
        [2012, 6],
        [2013, 9],
        [2014, 10],
        [2015, 21],
        [2016, 32],
        [2017, 48],
        [2018, 51],
        [2019, 48],
        [2020, 36],
        [2021, 30],
        [2022, 20]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:kuK5TVdYjLIC",
      "title": "Tangent prop-a formalism for specifying selected invariances in an adaptive network",
      "link": "https://proceedings.neurips.cc/paper/1991/hash/65658fde58ab3c2b6e5132a39fae7cb9-Abstract.html",
      "year": 1992,
      "cited_by": 343,
      "authors": ["Patrice Simard", "Bernard Victorri", "Y LeCun", "J Denker"],
      "description": "In many machine learning applications, one has access, not only to training data, but also to some high-level a priori knowledge about the desired be (cid: 173) havior of the system. For example, it is known in advance that the output of a character recognizer should be invariant with respect to small spa (cid: 173) tial distortions of the input images (translations, rotations, scale changes, etcetera). We have implemented a scheme that allows a network to learn the deriva (cid: 173) tive of its outputs with respect to distortion operators of our choosing. This not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations we wish the network to perform.",
      "citation_histogram": [
        [1992, 6],
        [1993, 11],
        [1994, 13],
        [1995, 8],
        [1996, 16],
        [1997, 19],
        [1998, 11],
        [1999, 10],
        [2000, 9],
        [2001, 5],
        [2002, 10],
        [2003, 7],
        [2004, 4],
        [2005, 6],
        [2006, 4],
        [2007, 9],
        [2008, 5],
        [2009, 3],
        [2010, 4],
        [2011, 9],
        [2012, 7],
        [2013, 5],
        [2014, 14],
        [2015, 12],
        [2016, 14],
        [2017, 20],
        [2018, 24],
        [2019, 16],
        [2020, 21],
        [2021, 18],
        [2022, 16]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:nYc7v9NCwtsC",
      "title": "Super-Resolution with Deep Convolutional Sufficient Statistics",
      "link": "https://arxiv.org/abs/1511.05666",
      "year": 2015,
      "cited_by": 317,
      "authors": ["Joan Bruna", "Pablo Sprechmann", "Yann LeCun"],
      "description": "Inverse problems in image and audio, and super-resolution in particular, can be seen as high-dimensional structured prediction problems, where the goal is to characterize the conditional distribution of a high-resolution output given its low-resolution corrupted observation. When the scaling ratio is small, point estimates achieve impressive performance, but soon they suffer from the regression-to-the-mean problem, result of their inability to capture the multi-modality of this conditional distribution. Modeling high-dimensional image and audio distributions is a hard task, requiring both the ability to model complex geometrical structures and textured regions. In this paper, we propose to use as conditional model a Gibbs distribution, where its sufficient statistics are given by deep convolutional neural networks. The features computed by the network are stable to local deformation, and have reduced variance when the input is a stationary texture. These properties imply that the resulting sufficient statistics minimize the uncertainty of the target signals given the degraded observations, while being highly informative. The filters of the CNN are initialized by multiscale complex wavelets, and then we propose an algorithm to fine-tune them by estimating the gradient of the conditional log-likelihood, which bears some similarities with Generative Adversarial Networks. We evaluate experimentally the proposed approach in the image super-resolution task, but the approach is general and could be used in other challenging ill-posed problems such as audio bandwidth extension.",
      "citation_histogram": [
        [2016, 10],
        [2017, 31],
        [2018, 40],
        [2019, 85],
        [2020, 75],
        [2021, 43],
        [2022, 25]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:mSu_8wZzne8C",
      "title": "Stacked what-where auto-encoders",
      "link": "https://arxiv.org/abs/1506.02351",
      "year": 2015,
      "cited_by": 317,
      "authors": [
        "Junbo Zhao",
        "Michael Mathieu",
        "Ross Goroshin",
        "Yann Lecun"
      ],
      "description": "We present a novel architecture, the \"stacked what-where auto-encoders\" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. The objective function includes reconstruction terms that induce the hidden states in the Deconvnet to be similar to those of the Convnet. Each pooling layer produces two sets of variables: the \"what\" which are fed to the next layer, and its complementary variable \"where\" that are fed to the corresponding layer in the generative decoder.",
      "citation_histogram": [
        [2015, 5],
        [2016, 43],
        [2017, 50],
        [2018, 40],
        [2019, 48],
        [2020, 61],
        [2021, 39],
        [2022, 20]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:QIV2ME_5wuYC",
      "title": "Fast inference in sparse coding algorithms with applications to object recognition",
      "link": "https://arxiv.org/abs/1010.3467",
      "year": 2010,
      "cited_by": 286,
      "authors": ["Koray Kavukcuoglu", "Marc'Aurelio Ranzato", "Yann LeCun"],
      "description": "Adaptive sparse coding methods learn a possibly overcomplete set of basis functions, such that natural image patches can be reconstructed by linearly combining a small subset of these bases. The applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation. In this work we propose a simple and efficient algorithm to learn basis functions. After training, this model also provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks.",
      "citation_histogram": [
        [2009, 10],
        [2010, 10],
        [2011, 17],
        [2012, 20],
        [2013, 26],
        [2014, 35],
        [2015, 31],
        [2016, 23],
        [2017, 33],
        [2018, 25],
        [2019, 19],
        [2020, 16],
        [2021, 9],
        [2022, 8]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:sszUF3NjhM4C",
      "title": "Road scene segmentation from a single image",
      "link": "https://link.springer.com/chapter/10.1007/978-3-642-33786-4_28",
      "year": 2012,
      "cited_by": 275,
      "authors": [
        "Jose M Alvarez",
        "Theo Gevers",
        "Yann LeCun",
        "Antonio M Lopez"
      ],
      "description": " Road scene segmentation is important in computer vision for different applications such as autonomous driving and pedestrian detection. Recovering the 3D structure of road scenes provides relevant contextual information to improve their understanding. In this paper, we use a convolutional neural network based algorithm to learn features from noisy labels to recover the 3D scene layout of a road image. The novelty of the algorithm relies on generating training labels by applying an algorithm trained on a general image dataset to classify on\u2013board images. Further, we propose a novel texture descriptor based on a learned color plane fusion to obtain maximal uniformity in road areas. Finally, acquired (off\u2013line) and current (on\u2013line) information are combined to detect road areas in single images. From quantitative and qualitative experiments, conducted on publicly available datasets, it is\u00a0\u2026",
      "citation_histogram": [
        [2013, 13],
        [2014, 19],
        [2015, 20],
        [2016, 35],
        [2017, 39],
        [2018, 48],
        [2019, 39],
        [2020, 23],
        [2021, 20],
        [2022, 14]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:dQ2og3OwTAUC",
      "title": "Improving generalization performance using double backpropagation",
      "link": "http://yann.lecun.com/exdb/publis/pdf/drucker-lecun-92.pdf",
      "year": 1992,
      "cited_by": 271,
      "authors": ["Harris Drucker", "Yann LeCun"],
      "description": "In order to generalize from a training set to a test set, it is desirable that small changes in the input space of a pattern do not change the output components. This can be done by including variations of the input space as part of the training set\u2014but this is computationally very expensive. Another method is to force this behavior as part of the training algorithm. This is done in double backpropagation by forming an energy function that is the sum of the normal energy term found in backpropagation and an additional term that is a function of the Jacobian. Significant improvement is shown with different architectures and different test sets, especially with architectures that had previously been shown to have very good performance when trained using backpropagation. It also will be shown that double backpropagation, as compared to backpropagation, creates weights that are smaller thereby causing the output of the\u00a0\u2026",
      "citation_histogram": [
        [1993, 1],
        [1994, 3],
        [1995, 2],
        [1996, 5],
        [1997, 8],
        [1998, 10],
        [1999, 6],
        [2000, 7],
        [2001, 4],
        [2002, 6],
        [2003, 5],
        [2004, 5],
        [2005, 8],
        [2006, 3],
        [2007, 5],
        [2008, 1],
        [2009, 3],
        [2010, 10],
        [2011, 8],
        [2012, 4],
        [2013, 4],
        [2014, 5],
        [2015, 3],
        [2016, 5],
        [2017, 8],
        [2018, 26],
        [2019, 25],
        [2020, 24],
        [2021, 37],
        [2022, 29]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:aqlVkmm33-oC",
      "title": "Transforming neural-net output levels to probability distributions",
      "link": "https://proceedings.neurips.cc/paper/1990/hash/7eacb532570ff6858afd2723755ff790-Abstract.html",
      "year": 1991,
      "cited_by": 267,
      "authors": ["John Denker", "Yann LeCun"],
      "description": "(1) The outputs of a typical multi-output classification network do not satisfy the axioms of probability; probabilities should be positive and sum to one. This problem can be solved by treating the trained network as a preprocessor that produces a feature vector that can be further processed, for instance by classical statistical estimation techniques.(2) We present a method for computing the first two moments ofthe probability distribution indicating the range of outputs that are consistent with the input and the training data. It is particularly useful to combine these two ideas: we implement the ideas of section 1 using Parzen windows, where the shape and relative size of each window is computed using the ideas of section 2. This allows us to make contact between important theoretical ideas (eg the ensemble formalism) and practical techniques (eg back-prop). Our results also shed new light on and generalize the well-known\" soft max\" scheme.",
      "citation_histogram": [
        [1990, 1],
        [1991, 1],
        [1992, 5],
        [1993, 2],
        [1994, 7],
        [1995, 3],
        [1996, 6],
        [1997, 4],
        [1998, 3],
        [1999, 4],
        [2000, 3],
        [2001, 2],
        [2002, 1],
        [2003, 1],
        [2004, 2],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 6],
        [2010, 2],
        [2011, 9],
        [2012, 28],
        [2013, 32],
        [2014, 45],
        [2015, 63],
        [2016, 32]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:1yQoGdGgb4wC",
      "title": "Handwritten zip code recognition with multilayer networks",
      "link": "https://ieeexplore.ieee.org/abstract/document/119325/",
      "year": 1990,
      "cited_by": 267,
      "authors": [
        "Y LeCun",
        "O Matan",
        "B Boser",
        "JS Denker",
        "D Henderson",
        "RE Howard",
        "W Hubbard",
        "LD Jacket",
        "HS Baird"
      ],
      "description": "An application of back-propagation networks to handwritten zip code recognition is presented. Minimal preprocessing of the data is required, but the architecture of the network is highly constrained and specifically designed for the task. The input of the network consists of size-normalized images of isolated digits. The performance on zip code digits provided by the US Postal Service is 92% recognition, 1% substitution, and 7% rejects. Structured neural networks can be viewed as statistical methods with structure which bridge the gap between purely statistical and purely structural methods.< >",
      "citation_histogram": [
        [1990, 3],
        [1991, 6],
        [1992, 6],
        [1993, 14],
        [1994, 11],
        [1995, 17],
        [1996, 10],
        [1997, 6],
        [1998, 6],
        [1999, 4],
        [2000, 5],
        [2001, 1],
        [2002, 2],
        [2003, 4],
        [2004, 4],
        [2005, 1],
        [2006, 4],
        [2007, 4],
        [2008, 4],
        [2009, 5],
        [2010, 6],
        [2011, 4],
        [2012, 4],
        [2013, 8],
        [2014, 6],
        [2015, 7],
        [2016, 3],
        [2017, 7],
        [2018, 17],
        [2019, 19],
        [2020, 19],
        [2021, 27],
        [2022, 19]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&pagesize=100&citation_for_view=WLN3QrAAAAAJ:h4edN3iSU_gC",
      "title": "Adversarially Regularized Autoencoders for Generating Discrete Structures",
      "link": null,
      "year": 2018,
      "cited_by": 261,
      "authors": [
        "Junbo Zhao",
        "Yoon Kim",
        "Kelly Zhang",
        "Alexander M Rush",
        "Yann LeCun"
      ],
      "description": null,
      "citation_histogram": [
        [2017, 3],
        [2018, 27],
        [2019, 50],
        [2020, 63],
        [2021, 73],
        [2022, 44]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:pIp0rujYkN4C",
      "title": "3rd International Conference on Learning Representations",
      "link": "https://scholar.google.com/scholar?cluster=17683938468684367543&hl=en&oi=scholarr",
      "year": 2015,
      "cited_by": 261,
      "authors": ["Diederik P Kingma", "Jimmy Ba", "Y Bengio", "Y LeCun"],
      "description": null,
      "citation_histogram": [
        [2017, 1],
        [2018, 6],
        [2019, 23],
        [2020, 60],
        [2021, 106],
        [2022, 64]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:lSLTfruPkqcC",
      "title": "Predictive network modeling of the high-resolution dynamic plant transcriptome in response to nitrate",
      "link": "https://link.springer.com/article/10.1186/gb-2010-11-12-r123",
      "year": 2010,
      "cited_by": 254,
      "authors": [
        "Gabriel Krouk",
        "Piotr Mirowski",
        "Yann LeCun",
        "Dennis E Shasha",
        "Gloria M Coruzzi"
      ],
      "description": "Nitrate, acting as both a nitrogen source and a signaling molecule, controls many aspects of plant development. However, gene networks involved in plant adaptation to fluctuating nitrate environments have not yet been identified. Here we use time-series transcriptome data to decipher gene relationships and consequently to build core regulatory networks involved in Arabidopsis root adaptation to nitrate provision. The experimental approach has been to monitor genome-wide responses to nitrate at 3, 6, 9, 12, 15 and 20 minutes using Affymetrix ATH1 gene chips. This high-resolution time course analysis demonstrated that the previously known primary nitrate response is actually preceded by a very fast gene expression modulation, involving genes and functions needed to prepare plants to use or reduce nitrate. A state-space model inferred from this microarray time-series data successfully predicts gene behavior\u00a0\u2026",
      "citation_histogram": [
        [2011, 8],
        [2012, 10],
        [2013, 19],
        [2014, 27],
        [2015, 24],
        [2016, 24],
        [2017, 20],
        [2018, 26],
        [2019, 19],
        [2020, 32],
        [2021, 26],
        [2022, 19]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:LXmCCkuhhTsC",
      "title": "The need for open source software in machine learning",
      "link": "https://www.jmlr.org/papers/volume8/sonnenburg07a/sonnenburg07a.pdf",
      "year": 2007,
      "cited_by": 252,
      "authors": [
        "Soren Sonnenburg",
        "Mikio L Braun",
        "Cheng Soon Ong",
        "Samy Bengio",
        "Leon Bottou",
        "Geoffrey Holmes",
        "Yann LeCunn",
        "Klaus-Robert Muller",
        "Fernando Pereira",
        "Carl Edward Rasmussen",
        "Gunnar Ratsch",
        "Bernhard Scholkopf",
        "Alexander Smola",
        "Pascal Vincent",
        "Jason Weston",
        "Robert C Williamson"
      ],
      "description": "Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time, the field of machine learning has developed a large body of powerful learning algorithms for diverse applications. However, the true potential of these methods is not used, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. We argue that this situation can be significantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally, we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods. We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientific community.",
      "citation_histogram": [
        [2007, 1],
        [2008, 10],
        [2009, 23],
        [2010, 11],
        [2011, 20],
        [2012, 11],
        [2013, 19],
        [2014, 18],
        [2015, 14],
        [2016, 22],
        [2017, 9],
        [2018, 14],
        [2019, 17],
        [2020, 17],
        [2021, 26],
        [2022, 15]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:g57m_4BFVZsC",
      "title": "Predicting Deeper into the Future of Semantic Segmentation",
      "link": "http://openaccess.thecvf.com/content_iccv_2017/html/Luc_Predicting_Deeper_Into_ICCV_2017_paper.html",
      "year": 2017,
      "cited_by": 248,
      "authors": [
        "Pauline Luc",
        "Natalia Neverova",
        "Camille Couprie",
        "Jakob Verbeek",
        "Yann LeCun"
      ],
      "description": "The ability to predict and therefore to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, eg. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we introduce the novel task of predicting semantic segmentations of future frames. Given a sequence of video frames, our goal is to predict segmentation maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregressive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Prediction results up to half a second in the future are visually convincing and are much more accurate than those of a baseline based on warping semantic segmentations using optical flow.",
      "citation_histogram": [
        [2017, 15],
        [2018, 41],
        [2019, 44],
        [2020, 55],
        [2021, 59],
        [2022, 33]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:6_hjMsCP8ZoC",
      "title": "Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers",
      "link": "https://arxiv.org/abs/1202.2160",
      "year": 2012,
      "cited_by": 244,
      "authors": [
        "Cl\u00e9ment Farabet",
        "Camille Couprie",
        "Laurent Najman",
        "Yann LeCun"
      ],
      "description": "Scene parsing, or semantic segmentation, consists in labeling each pixel in an image with the category of the object it belongs to. It is a challenging task that involves the simultaneous detection, segmentation and recognition of all the objects in the image. The scene parsing method proposed here starts by computing a tree of segments from a graph of pixel dissimilarities. Simultaneously, a set of dense feature vectors is computed which encodes regions of multiple sizes centered on each pixel. The feature extractor is a multiscale convolutional network trained from raw pixels. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average \"purity\" of the class distributions, hence maximizing the overall likelihood that each segment will contain a single object. The convolutional network feature extractor is trained end-to-end from raw pixels, alleviating the need for engineered features. After training, the system is parameter free. The system yields record accuracies on the Stanford Background Dataset (8 classes), the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) while being an order of magnitude faster than competing approaches, producing a 320 \\times 240 image labeling in less than 1 second.",
      "citation_histogram": [
        [2011, 1],
        [2012, 9],
        [2013, 19],
        [2014, 33],
        [2015, 26],
        [2016, 53],
        [2017, 35],
        [2018, 17],
        [2019, 18],
        [2020, 8],
        [2021, 10],
        [2022, 6]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:hUq98zRk74IC",
      "title": "Very deep multilingual convolutional neural networks for LVCSR",
      "link": "https://ieeexplore.ieee.org/abstract/document/7472620/",
      "year": 2016,
      "cited_by": 243,
      "authors": [
        "Tom Sercu",
        "Christian Puhrsch",
        "Brian Kingsbury",
        "Yann LeCun"
      ],
      "description": "Convolutional neural networks (CNNs) are a standard component of many current state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) systems. However, CNNs in LVCSR have not kept pace with recent advances in other domains where deeper neural networks provide superior performance. In this paper we propose a number of architectural advances in CNNs for LVCSR. First, we introduce a very deep convolutional network architecture with up to 14 weight layers. There are multiple convolutional layers before each pooling layer, with small 3\u00d73 kernels, inspired by the VGG Imagenet 2014 architecture. Then, we introduce multilingual CNNs with multiple untied layers. Finally, we introduce multi-scale input features aimed at exploiting more context at negligible computational cost. We evaluate the improvements first on a Babel task for low resource speech recognition, obtaining an absolute\u00a0\u2026",
      "citation_histogram": [
        [2015, 5],
        [2016, 28],
        [2017, 49],
        [2018, 55],
        [2019, 40],
        [2020, 27],
        [2021, 21],
        [2022, 16]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:VL0QpB8kHFEC",
      "title": "Design of a neural network character recognizer for a touch terminal",
      "link": "https://www.sciencedirect.com/science/article/pii/003132039190081F",
      "year": 1991,
      "cited_by": 239,
      "authors": [
        "Isabelle Guyon",
        "Paul Albrecht",
        "Yann LeCun",
        "J Denker",
        "Wayne Hubbard"
      ],
      "description": "We describe a system which can recognize digits and uppercase letters handprinted on a touch terminal. A character is input as a sequence of [x(t), y(t)] coordinates, subjected to very simple preprocessing, and then classified by a trainable neural network. The classifier is analogous to \u201ctime delay neural networks\u201d previously applied to speech recognition. The network was trained on a set of 12,000 digits and uppercase letters, from approximately 250 different writers, and tested on 2500 such characters from other writers. Classification accuracy exceeded 96% on the test examples.",
      "citation_histogram": [
        [1990, 1],
        [1991, 12],
        [1992, 6],
        [1993, 20],
        [1994, 14],
        [1995, 14],
        [1996, 16],
        [1997, 5],
        [1998, 9],
        [1999, 6],
        [2000, 9],
        [2001, 8],
        [2002, 5],
        [2003, 2],
        [2004, 4],
        [2005, 8],
        [2006, 7],
        [2007, 9],
        [2008, 1],
        [2009, 6],
        [2010, 5],
        [2011, 5],
        [2012, 6],
        [2013, 3],
        [2014, 2],
        [2015, 7],
        [2016, 11],
        [2017, 7],
        [2018, 11],
        [2019, 9],
        [2020, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:CqsiOOvXZmUC",
      "title": "Tracking the World State with Recurrent Entity Networks",
      "link": "https://arxiv.org/abs/1612.03969",
      "year": 2016,
      "cited_by": 237,
      "authors": [
        "Mikael Henaff",
        "Jason Weston",
        "Arthur Szlam",
        "Antoine Bordes",
        "Yann LeCun"
      ],
      "description": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass.",
      "citation_histogram": [
        [2016, 1],
        [2017, 27],
        [2018, 40],
        [2019, 56],
        [2020, 49],
        [2021, 40],
        [2022, 24]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:roLk4NBRz8UC",
      "title": "LeRec: A NN/HMM hybrid for on-line handwriting recognition",
      "link": "https://ieeexplore.ieee.org/abstract/document/6796565/",
      "year": 1995,
      "cited_by": 232,
      "authors": ["Yoshua Bengio", "Yann LeCun", "Craig Nohl", "Chris Burges"],
      "description": "We introduce a new approach for on-line recognition of handwritten words written in unconstrained mixed style. The preprocessor performs a word-level normalization by fitting a model of the word structure using the EM algorithm. Words are then coded into low resolution \"annotated images\" where each pixel contains information about trajectory direction and curvature. The recognizer is a convolution network that can be spatially replicated. From the network output, a hidden Markov model produces word scores. The entire system is globally trained to minimize word-level errors.",
      "citation_histogram": [
        [1995, 1],
        [1996, 4],
        [1997, 9],
        [1998, 14],
        [1999, 14],
        [2000, 5],
        [2001, 13],
        [2002, 9],
        [2003, 8],
        [2004, 2],
        [2005, 7],
        [2006, 4],
        [2007, 4],
        [2008, 6],
        [2009, 7],
        [2010, 4],
        [2011, 1],
        [2012, 10],
        [2013, 12],
        [2014, 11],
        [2015, 17],
        [2016, 6],
        [2017, 12],
        [2018, 4],
        [2019, 13],
        [2020, 10],
        [2021, 5],
        [2022, 11]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:hkOj_22Ku90C",
      "title": "An analog neural network processor with programmable topology",
      "link": "https://ieeexplore.ieee.org/abstract/document/104196/",
      "year": 1991,
      "cited_by": 228,
      "authors": [
        "Bernhard E Boser",
        "Eduard Sackinger",
        "Jane Bromley",
        "Yann LeCun",
        "Lawrence D Jackel"
      ],
      "description": "The architecture, implementation, and applications of a special-purpose neural network processor are described. The chip performs over 2000 multiplications and additions simultaneously. Its data path is particularly suitable for the convolutional topologies that are typical in classification networks, but can also be configured for fully connected or feedback topologies. Resources can be multiplexed to permit implementation of networks with several hundreds of thousands of connections on a single chip. Computations are performed with 6 b accuracy for the weights and 3 b for the neuron states. Analog processing is used internally for reduced power dissipation and higher density, but all input/output is digital to simplify system integration. The practicality of the chip is demonstrated with an implementation of a neural network for optical character recognition. This network contains over 130000 connections and was\u00a0\u2026",
      "citation_histogram": [
        [1991, 7],
        [1992, 18],
        [1993, 15],
        [1994, 14],
        [1995, 15],
        [1996, 9],
        [1997, 5],
        [1998, 6],
        [1999, 9],
        [2000, 4],
        [2001, 3],
        [2002, 2],
        [2003, 4],
        [2004, 2],
        [2005, 3],
        [2006, 3],
        [2007, 2],
        [2008, 3],
        [2009, 2],
        [2010, 5],
        [2011, 6],
        [2012, 9],
        [2013, 10],
        [2014, 3],
        [2015, 7],
        [2016, 11],
        [2017, 13],
        [2018, 23],
        [2019, 5],
        [2020, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:MLfJN-KU85MC",
      "title": "Multi-digit recognition using a space displacement neural network",
      "link": "https://proceedings.neurips.cc/paper/1991/hash/6e2713a6efee97bacb63e52c54f0ada0-Abstract.html",
      "year": 1993,
      "cited_by": 227,
      "authors": [
        "Ofer Matan",
        "Christopher JC Burges",
        "Yann A LeCun",
        "John S Denker"
      ],
      "description": "We present a feed-forward network architecture for recognizing an uncon (cid: 173) strained handwritten multi-digit string. This is an extension of previous work on recognizing isolated digits. In this architecture a single digit rec (cid: 173) ognizer is replicated over the input. The output layer of the network is coupled to a Viterbi alignment module that chooses the best interpretation of the input. Training errors are propagated through the Viterbi module. The novelty in this procedure is that segmentation is done on the feature maps developed in the Space Displacement Neural Network (SDNN) rather than the input (pixel) space.",
      "citation_histogram": [
        [1992, 3],
        [1993, 10],
        [1994, 10],
        [1995, 9],
        [1996, 4],
        [1997, 8],
        [1998, 8],
        [1999, 7],
        [2000, 5],
        [2001, 1],
        [2002, 3],
        [2003, 3],
        [2004, 3],
        [2005, 5],
        [2006, 8],
        [2007, 5],
        [2008, 3],
        [2009, 2],
        [2010, 2],
        [2011, 5],
        [2012, 1],
        [2013, 5],
        [2014, 7],
        [2015, 25],
        [2016, 17],
        [2017, 26],
        [2018, 13],
        [2019, 9],
        [2020, 13],
        [2021, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:DBa1UEJaJKAC",
      "title": "Deep learning made easier by linear transformations in perceptrons",
      "link": "http://proceedings.mlr.press/v22/raiko12.html",
      "year": 2012,
      "cited_by": 222,
      "authors": ["Tapani Raiko", "Harri Valpola", "Yann LeCun"],
      "description": "We transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero activation and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. This transformation aims at separating the problems of learning the linear and nonlinear parts of the whole input-output mapping, which has many benefits. We study the theoretical properties of the transformation by noting that they make the Fisher information matrix closer to a diagonal matrix, and thus standard gradient closer to the natural gradient. We experimentally confirm the usefulness of the transformations by noting that they make basic stochastic gradient learning competitive with state-of-the-art learning algorithms in speed, and that they seem also to help find solutions that generalize better. The experiments include both classification of small images and learning a low-dimensional representation for images by using a deep unsupervised auto-encoder network. The transformations were beneficial in all cases, with and without regularization and with networks from two to five hidden layers.",
      "citation_histogram": [
        [2012, 5],
        [2013, 18],
        [2014, 11],
        [2015, 22],
        [2016, 30],
        [2017, 22],
        [2018, 34],
        [2019, 18],
        [2020, 28],
        [2021, 16],
        [2022, 14]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:3bvyWxjaHKcC",
      "title": "Transformation invariance in pattern recognition\u2013tangent distance and tangent propagation",
      "link": "https://link.springer.com/chapter/10.1007/978-3-642-35289-8_17",
      "year": 2012,
      "cited_by": 217,
      "authors": [
        "Patrice Y Simard",
        "Yann A LeCun",
        "John S Denker",
        "Bernard Victorri"
      ],
      "description": " In pattern recognition, statistical modeling, or regression, the amount of data is a critical factor affecting the performance. If the amount of data and computational resources are unlimited, even trivial algorithms will converge to the optimal solution. However, in the practical case, given limited data and other resources, satisfactory performance requires sophisticated methods to regularize the problem by introducing a priori knowledge. Invariance of the output with respect to certain transformations of the input is a typical example of such a priori knowledge. In this chapter, we introduce the concept of tangent vectors, which compactly represent the essence of these transformation invariances, and two classes of algorithms, \u201ctangent distance\u201d and \u201ctangent propagation\u201d, which make use of these invariances to improve performance.",
      "citation_histogram": [
        [2001, 1],
        [2002, 1],
        [2003, 2],
        [2004, 8],
        [2005, 16],
        [2006, 8],
        [2007, 12],
        [2008, 20],
        [2009, 8],
        [2010, 6],
        [2011, 10],
        [2012, 13],
        [2013, 15],
        [2014, 16],
        [2015, 11],
        [2016, 4],
        [2017, 8],
        [2018, 9],
        [2019, 7],
        [2020, 16],
        [2021, 12],
        [2022, 13]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:UeHWp8X0CEIC",
      "title": "Transformation invariance in pattern recognition\u2013tangent distance and tangent propagation",
      "link": "https://link.springer.com/chapter/10.1007/978-3-642-35289-8_17",
      "year": 2012,
      "cited_by": 217,
      "authors": [
        "Patrice Y Simard",
        "Yann A LeCun",
        "John S Denker",
        "Bernard Victorri"
      ],
      "description": " In pattern recognition, statistical modeling, or regression, the amount of data is a critical factor affecting the performance. If the amount of data and computational resources are unlimited, even trivial algorithms will converge to the optimal solution. However, in the practical case, given limited data and other resources, satisfactory performance requires sophisticated methods to regularize the problem by introducing a priori knowledge. Invariance of the output with respect to certain transformations of the input is a typical example of such a priori knowledge. In this chapter, we introduce the concept of tangent vectors, which compactly represent the essence of these transformation invariances, and two classes of algorithms, \u201ctangent distance\u201d and \u201ctangent propagation\u201d, which make use of these invariances to improve performance.",
      "citation_histogram": [
        [2001, 1],
        [2002, 1],
        [2003, 2],
        [2004, 8],
        [2005, 16],
        [2006, 8],
        [2007, 12],
        [2008, 20],
        [2009, 8],
        [2010, 6],
        [2011, 10],
        [2012, 13],
        [2013, 15],
        [2014, 16],
        [2015, 11],
        [2016, 4],
        [2017, 8],
        [2018, 9],
        [2019, 7],
        [2020, 16],
        [2021, 12],
        [2022, 13]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:isC4tDSrTZIC",
      "title": "Comparing SVM and convolutional networks for epileptic seizure prediction from intracranial EEG",
      "link": "https://ieeexplore.ieee.org/abstract/document/4685487/",
      "year": 2008,
      "cited_by": 212,
      "authors": [
        "Piotr W Mirowski",
        "Yann LeCun",
        "Deepak Madhavan",
        "Ruben Kuzniecky"
      ],
      "description": "Recent research suggests that electrophysiological changes develop minutes to hours before the actual clinical onset in focal epileptic seizures. Seizure prediction is a major field of neurological research, enabled by statistical analysis methods applied to features derived from intracranial Electroencephalographic (EEG) recordings of brain activity. However, no reliable seizure prediction method is ready for clinical applications. In this study, we use modern machine learning techniques to predict seizures from a number of features proposed in the literature. We concentrate on aggregated features that encode the relationship between pairs of EEG channels, such as cross-correlation, nonlinear interdependence, difference of Lyapunov exponents and wavelet analysis-based synchrony such as phase locking. We compare L1-regularized logistic regression, convolutional networks, and support vector machines\u00a0\u2026",
      "citation_histogram": [
        [2009, 2],
        [2010, 4],
        [2011, 10],
        [2012, 6],
        [2013, 5],
        [2014, 12],
        [2015, 14],
        [2016, 13],
        [2017, 30],
        [2018, 35],
        [2019, 24],
        [2020, 30],
        [2021, 16],
        [2022, 10]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:kiFE3DPpsncC",
      "title": "OverFeat: Integrated Recognition",
      "link": "https://scholar.google.com/scholar?cluster=7172423676221597396&hl=en&oi=scholarr",
      "year": 2013,
      "cited_by": 208,
      "authors": [
        "Pierre Sermanet",
        "David Eigen",
        "Xiang Zhang",
        "Michael Mathieu",
        "Rob Fergus",
        "Yann LeCun"
      ],
      "description": null,
      "citation_histogram": [
        [2014, 5],
        [2015, 5],
        [2016, 11],
        [2017, 32],
        [2018, 34],
        [2019, 39],
        [2020, 21],
        [2021, 35],
        [2022, 25]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:UHK10RUVsp4C",
      "title": "Boxlets: a fast convolution algorithm for signal processing and neural networks",
      "link": "https://proceedings.neurips.cc/paper/1998/hash/1bc0249a6412ef49b07fe6f62e6dc8de-Abstract.html",
      "year": 1999,
      "cited_by": 201,
      "authors": [
        "Patrice Y Simard",
        "Leon Bottou",
        "Patrick Haffner",
        "Yann LeCun"
      ],
      "description": "Signal processing and pattern recognition algorithms make exten (cid: 173) sive use of convolution. In many cases, computational accuracy is not as important as computational speed. In feature extraction, for instance, the features of interest in a signal are usually quite distorted. This form of noise justifies some level of quantization in order to achieve faster feature extraction. Our approach consists of approximating regions of the signal with low degree polynomi (cid: 173) als, and then differentiating the resulting signals in order to obtain impulse functions (or derivatives of impulse functions). With this representation, convolution becomes extremely simple and can be implemented quite effectively. The true convolution can be recov (cid: 173) ered by integrating the result of the convolution. This method yields substantial speed up in feature extraction and is applicable to convolutional neural networks.",
      "citation_histogram": [
        [2002, 4],
        [2003, 4],
        [2004, 7],
        [2005, 3],
        [2006, 8],
        [2007, 5],
        [2008, 18],
        [2009, 7],
        [2010, 12],
        [2011, 12],
        [2012, 12],
        [2013, 24],
        [2014, 19],
        [2015, 10],
        [2016, 19],
        [2017, 5],
        [2018, 7],
        [2019, 13],
        [2020, 4],
        [2021, 3],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:mKu_rENv82IC",
      "title": "Moving beyond feature design: Deep architectures and automatic feature learning in music informatics.",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.294.2304&rep=rep1&type=pdf",
      "year": 2012,
      "cited_by": 198,
      "authors": ["Eric J Humphrey", "Juan Pablo Bello", "Yann LeCun"],
      "description": "The short history of content-based music informatics research is dominated by hand-crafted feature design, and our community has grown admittedly complacent with a few de facto standards. Despite commendable progress in many areas, it is increasingly apparent that our efforts are yielding diminishing returns. This deceleration is largely due to the tandem of heuristic feature design and shallow processing architectures. We systematically discard hopefully irrelevant information while simultaneously calling upon creativity, intuition, or sheer luck to craft useful representations, gradually evolving complex, carefully tuned systems to address specific tasks. While other disciplines have seen the benefits of deep learning, it has only recently started to be explored in our field. By reviewing deep architectures and feature learning, we hope to raise awareness in our community about alternative approaches to solving MIR challenges, new and old alike.",
      "citation_histogram": [
        [2012, 1],
        [2013, 9],
        [2014, 19],
        [2015, 25],
        [2016, 24],
        [2017, 25],
        [2018, 28],
        [2019, 22],
        [2020, 18],
        [2021, 17],
        [2022, 8]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:27qsyVibG6YC",
      "title": "A learning scheme for asymmetric threshold networks",
      "link": "https://scholar.google.com/scholar?cluster=2615849709031859995&hl=en&oi=scholarr",
      "year": 1985,
      "cited_by": 198,
      "authors": ["Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [1986, 2],
        [1987, 4],
        [1988, 5],
        [1989, 11],
        [1990, 20],
        [1991, 14],
        [1992, 14],
        [1993, 15],
        [1994, 7],
        [1995, 5],
        [1996, 3],
        [1997, 1],
        [1998, 4],
        [1999, 1],
        [2000, 2],
        [2001, 6],
        [2002, 5],
        [2003, 1],
        [2004, 4],
        [2005, 5],
        [2006, 6],
        [2007, 4],
        [2008, 1],
        [2009, 2],
        [2010, 3],
        [2011, 3],
        [2012, 7],
        [2013, 4],
        [2014, 3],
        [2015, 4],
        [2016, 2],
        [2017, 5],
        [2018, 5],
        [2019, 9],
        [2020, 5],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:yM8WYnMLviIC",
      "title": "Deep learning for AI",
      "link": "https://dl.acm.org/doi/abs/10.1145/3448250",
      "year": 2021,
      "cited_by": 196,
      "authors": ["Yoshua Bengio", "Yann Lecun", "Geoffrey Hinton"],
      "description": "How can neural networks learn the rich internal representations required for difficult tasks such as recognizing objects or understanding language? ",
      "citation_histogram": [
        [2020, 2],
        [2021, 55],
        [2022, 135]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:AOeXN74AWYwC",
      "title": "Modeep: A deep learning framework using motion features for human pose estimation",
      "link": "https://link.springer.com/chapter/10.1007/978-3-319-16808-1_21",
      "year": 2014,
      "cited_by": 195,
      "authors": [
        "Arjun Jain",
        "Jonathan Tompson",
        "Yann LeCun",
        "Christoph Bregler"
      ],
      "description": " In this work, we propose a novel and efficient method for articulated human pose estimation in videos using a convolutional network architecture, which incorporates both color and motion features. We propose a new human body pose dataset, FLIC-motion (This dataset can be downloaded from                      http://cs.nyu.edu/~ajain/accv2014/                                        .), that extends the FLIC dataset\u00a0[1] with additional motion features. We apply our architecture to this dataset and report significantly better performance than current state-of-the-art pose detection systems.",
      "citation_histogram": [
        [2013, 1],
        [2014, 16],
        [2015, 30],
        [2016, 27],
        [2017, 39],
        [2018, 27],
        [2019, 20],
        [2020, 17],
        [2021, 15]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:WAzi4Gm8nLoC",
      "title": "Learning invariant feature hierarchies",
      "link": "https://link.springer.com/chapter/10.1007/978-3-642-33863-2_51",
      "year": 2012,
      "cited_by": 195,
      "authors": ["Yann LeCun"],
      "description": " Fast visual recognition in the mammalian cortex seems to be a hierarchical process by which the representation of the visual world is transformed in multiple stages from low-level retinotopic features to high-level, global and invariant features, and to object categories. Every single step in this hierarchy seems to be subject to learning. How does the visual cortex learn such hierarchical representations by just looking at the world? How could computers learn such representations from data? Computer vision models that are weakly inspired by the visual cortex will be described. A number of unsupervised learning algorithms to train these models will be presented, which are based on the sparse auto-encoder concept. The effectiveness of these algorithms for learning invariant feature hierarchies will be demonstrated with a number of practical tasks such as scene parsing, pedestrian detection, and object\u00a0\u2026",
      "citation_histogram": [
        [2012, 2],
        [2013, 12],
        [2014, 15],
        [2015, 18],
        [2016, 25],
        [2017, 22],
        [2018, 23],
        [2019, 19],
        [2020, 21],
        [2021, 23],
        [2022, 10]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:0EnyYjriUFMC",
      "title": "Application of the ANNA neural network chip to high-speed character recognition",
      "link": "http://oro.open.ac.uk/35661/",
      "year": 1992,
      "cited_by": 194,
      "authors": [
        "Eduard S\u00e4ckinger",
        "Bernhard E Boser",
        "Jane M Bromley",
        "Yann LeCun",
        "Larry D Jackel"
      ],
      "description": "A neural network with 136000 connections for recognition of handwritten digits has been implemented using a mixed analog/digital neural network chip. The neural network chip is capable of processing 1000 characters/s. The recognition system has essentially the same rate (5%) as a simulation of the network with 32-b floating-point precision",
      "citation_histogram": [
        [1991, 2],
        [1992, 4],
        [1993, 14],
        [1994, 8],
        [1995, 12],
        [1996, 18],
        [1997, 10],
        [1998, 5],
        [1999, 8],
        [2000, 3],
        [2001, 4],
        [2002, 5],
        [2003, 7],
        [2004, 1],
        [2005, 5],
        [2006, 5],
        [2007, 6],
        [2008, 3],
        [2009, 4],
        [2010, 7],
        [2011, 2],
        [2012, 2],
        [2013, 2],
        [2014, 9],
        [2015, 1],
        [2016, 5],
        [2017, 5],
        [2018, 7],
        [2019, 6],
        [2020, 7],
        [2021, 8],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:FiDNX6EVdGUC",
      "title": "Eigenvalues of covariance matrices: Application to neural-network learning",
      "link": "https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.66.2396",
      "year": 1991,
      "cited_by": 187,
      "authors": ["Yann Le Cun", "Ido Kanter", "Sara A Solla"],
      "description": "The learing time of a simple neural-network model is obtained through an analytic computation of the eigenvalue spectrum for the Hessian matrix, which describes the second-order properties of the objective function in the space of coupling coefficients. The results are generic for symmetric matrices obtained by summing outer products of random vectors. The form of the eigenvalue distribution suggests new techniques for accelerating the learning process, and provides a theoretical justification for the choice of centered versus biased state variables.",
      "citation_histogram": [
        [1990, 1],
        [1991, 2],
        [1992, 6],
        [1993, 2],
        [1994, 3],
        [1995, 3],
        [1996, 3],
        [1997, 10],
        [1998, 1],
        [1999, 4],
        [2000, 4],
        [2001, 4],
        [2002, 1],
        [2003, 13],
        [2004, 9],
        [2005, 1],
        [2006, 7],
        [2007, 4],
        [2008, 1],
        [2009, 4],
        [2010, 3],
        [2011, 4],
        [2012, 3],
        [2013, 3],
        [2014, 6],
        [2015, 4],
        [2016, 7],
        [2017, 10],
        [2018, 12],
        [2019, 19],
        [2020, 23],
        [2021, 9]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Fu2w8maKXqMC",
      "title": "On\u2010line learning for very large data sets",
      "link": "https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.538",
      "year": 2005,
      "cited_by": 186,
      "authors": ["L\u00e9on Bottou", "Yann LeCun"],
      "description": " The design of very large learning systems presents many unsolved challenges. Consider, for instance, a system that \u2018watches\u2019 television for a few weeks and learns to enumerate the objects present in these images. Most current learning algorithms do not scale well enough to handle such massive quantities of data. Experience suggests that the stochastic learning algorithms are best suited to such tasks. This is at first surprising because stochastic learning algorithms optimize the training error rather slowly. Our paper reconsiders the convergence speed in terms of how fast a learning algorithm optimizes the testing error. This reformulation shows the superiority of the well designed stochastic learning algorithm. Copyright \u00a9 2005 John Wiley & Sons, Ltd.",
      "citation_histogram": [
        [2005, 2],
        [2006, 1],
        [2007, 2],
        [2008, 14],
        [2009, 14],
        [2010, 13],
        [2011, 7],
        [2012, 6],
        [2013, 17],
        [2014, 16],
        [2015, 17],
        [2016, 13],
        [2017, 16],
        [2018, 11],
        [2019, 15],
        [2020, 11],
        [2021, 9]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:KjnAay3C9J8C",
      "title": "Convolutional Networks for Images, Speech, and Time Series. The Hand Book of Brain Theory and Neural Networks",
      "link": "https://scholar.google.com/scholar?cluster=8670998147311198366&hl=en&oi=scholarr",
      "year": 1995,
      "cited_by": 177,
      "authors": ["Yann LeCun", "Yoshua Bengio"],
      "description": null,
      "citation_histogram": [
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 2],
        [2009, 8],
        [2010, 3],
        [2011, 3],
        [2012, 2],
        [2013, 6],
        [2014, 9],
        [2015, 23],
        [2016, 32],
        [2017, 31],
        [2018, 31],
        [2019, 23]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:5ugPr518TE4C",
      "title": "UNSUPERVISED LEARNING OF SPARSE FEATURES FOR SCALABLE AUDIO CLASSIFICATION",
      "link": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.226.4659&rep=rep1&type=pdf",
      "year": 2011,
      "cited_by": 173,
      "authors": [
        "Mikael Henaff",
        "Kevin Jarrett",
        "Koray Kavukcuoglu",
        "Yann LeCun"
      ],
      "description": "In this work we present a system to automatically learn features from audio in an unsupervised manner. Our method first learns an overcomplete dictionary which can be used to sparsely decompose log-scaled spectrograms. It then trains an efficient encoder which quickly maps new inputs to approximations of their sparse representations using the learned dictionary. This avoids expensive iterative procedures usually required to infer sparse codes. We then use these sparse codes as inputs for a linear Support Vector Machine (SVM). Our system achieves 83.4% accuracy in predicting genres on the GTZAN dataset, which is competitive with current state-of-the-art approaches. Furthermore, the use of a simple linear classifier combined with a fast feature extraction system allows our approach to scale well to large datasets.",
      "citation_histogram": [
        [2011, 1],
        [2012, 25],
        [2013, 16],
        [2014, 14],
        [2015, 23],
        [2016, 13],
        [2017, 17],
        [2018, 20],
        [2019, 11],
        [2020, 14],
        [2021, 12],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:rzkGdFpNPO0C",
      "title": "Solla",
      "link": "https://scholar.google.com/scholar?cluster=7324156123177957399&hl=en&oi=scholarr",
      "year": 1990,
      "cited_by": 173,
      "authors": ["Yann Le Cun", "John S Denker", "A Sara"],
      "description": null,
      "citation_histogram": [
        [1990, 2],
        [1991, 6],
        [1992, 1],
        [1993, 4],
        [1994, 5],
        [1995, 2],
        [1996, 2],
        [1997, 2],
        [1998, 2],
        [1999, 2],
        [2000, 1],
        [2001, 1],
        [2002, 2],
        [2003, 3],
        [2004, 2],
        [2005, 1],
        [2006, 2],
        [2007, 2],
        [2008, 4],
        [2009, 2],
        [2010, 5],
        [2011, 5],
        [2012, 6],
        [2013, 24],
        [2014, 20],
        [2015, 23],
        [2016, 28],
        [2017, 12]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:_r6PDwDh8pAC",
      "title": "Vicreg: Variance-invariance-covariance regularization for self-supervised learning",
      "link": "https://arxiv.org/abs/2105.04906",
      "year": 2021,
      "cited_by": 172,
      "authors": ["Adrien Bardes", "Jean Ponce", "Yann LeCun"],
      "description": "Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. VICReg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements.",
      "citation_histogram": [
        [2021, 33],
        [2022, 138]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:_kc_bZDykSQC",
      "title": "Loss functions for discriminative training of energy-based models",
      "link": "http://proceedings.mlr.press/r5/lecun05a/lecun05a.pdf",
      "year": 2005,
      "cited_by": 167,
      "authors": ["Yann LeCun", "Fu Jie Huang"],
      "description": "Probabilistic graphical models associate a probability to each configuration of the relevant variables. Energy-based models (EBM) associate an energy to those configurations, eliminating the need for proper normalization of probability distributions. Making a decision (an inference) with an EBM consists in comparing the energies associated with various configurations of the variable to be predicted, and choosing the one with the smallest energy. Such systems must be trained discriminatively to associate low energies to the desired configurations and higher energies to undesired configurations. A wide variety of loss function can be used for this purpose. We give sufficient conditions that a loss function should satisfy so that its minimization will cause the system to approach to desired behavior. We give many specific examples of suitable loss functions, and show an application to object recognition in images.",
      "citation_histogram": [
        [2005, 11],
        [2006, 6],
        [2007, 7],
        [2008, 8],
        [2009, 9],
        [2010, 9],
        [2011, 6],
        [2012, 6],
        [2013, 6],
        [2014, 3],
        [2015, 10],
        [2016, 13],
        [2017, 12],
        [2018, 15],
        [2019, 12],
        [2020, 8],
        [2021, 14],
        [2022, 11]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:TLwS_1sUIYkC",
      "title": "Tunable efficient unitary neural networks (EUNN) and their application to RNNs",
      "link": "http://proceedings.mlr.press/v70/jing17a.html",
      "year": 2016,
      "cited_by": 165,
      "authors": [
        "Li Jing",
        "Yichen Shen",
        "Tena Dub\u010dek",
        "John Peurifoy",
        "Scott Skirlo",
        "Yann LeCun",
        "Max Tegmark",
        "Marin Solja\u010di\u0107"
      ],
      "description": "Using unitary (instead of general) matrices in artificial neural networks (ANNs) is a promising way to solve the gradient explosion/vanishing problem, as well as to enable ANNs to learn long-term correlations in the data. This approach appears particularly promising for Recurrent Neural Networks (RNNs). In this work, we present a new architecture for implementing an Efficient Unitary Neural Network (EUNNs); its main advantages can be summarized as follows. Firstly, the representation capacity of the unitary space in an EUNN is fully tunable, ranging from a subspace of SU (N) to the entire unitary space. Secondly, the computational complexity for training an EUNN is merely  per parameter. Finally, we test the performance of EUNNs on the standard copying task, the pixel-permuted MNIST digit recognition benchmark as well as the Speech Prediction Test (TIMIT). We find that our architecture significantly outperforms both other state-of-the-art unitary RNNs and the LSTM architecture, in terms of the final performance and/or the wall-clock training speed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide variety of applications.",
      "citation_histogram": [
        [2017, 9],
        [2018, 29],
        [2019, 34],
        [2020, 39],
        [2021, 32],
        [2022, 22]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:4PzMk9GH2tkC",
      "title": "Understanding Deep Architectures using a Recursive Convolutional Network",
      "link": "https://arxiv.org/abs/1312.1847",
      "year": 2014,
      "cited_by": 157,
      "authors": ["David Eigen", "Jason Rolfe", "Rob Fergus", "Yann LeCun"],
      "description": "A key challenge in designing convolutional network models is sizing them appropriately. Many factors are involved in these decisions, including number of layers, feature maps, kernel sizes, etc. Complicating this further is the fact that each of these influence not only the numbers and dimensions of the activation units, but also the total number of parameters. In this paper we focus on assessing the independent contributions of three of these linked variables: The numbers of layers, feature maps, and parameters. To accomplish this, we employ a recursive convolutional network whose weights are tied between layers; this allows us to vary each of the three factors in a controlled setting. We find that while increasing the numbers of layers and parameters each have clear benefit, the number of feature maps (and hence dimensionality of the representation) appears ancillary, and finds most of its benefit through the introduction of more weights. Our results (i) empirically confirm the notion that adding layers alone increases computational power, within the context of convolutional layers, and (ii) suggest that precise sizing of convolutional feature map dimensions is itself of little concern; more attention should be paid to the number of parameters in these layers instead.",
      "citation_histogram": [
        [2013, 1],
        [2014, 7],
        [2015, 14],
        [2016, 25],
        [2017, 22],
        [2018, 24],
        [2019, 14],
        [2020, 27],
        [2021, 17],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:ghEM2AJqZyQC",
      "title": "Feature learning and deep architectures: New directions for music informatics",
      "link": "https://link.springer.com/article/10.1007/s10844-013-0248-5",
      "year": 2013,
      "cited_by": 156,
      "authors": ["Eric J Humphrey", "Juan P Bello", "Yann LeCun"],
      "description": " As we look to advance the state of the art in content-based music informatics, there is a general sense that progress is decelerating throughout the field. On closer inspection, performance trajectories across several applications reveal that this is indeed the case, raising some difficult questions for the discipline: why are we slowing down, and what can we do about it? Here, we strive to address both of these concerns. First, we critically review the standard approach to music signal analysis and identify three specific deficiencies to current methods: hand-crafted feature design is sub-optimal and unsustainable, the power of shallow architectures is fundamentally limited, and short-time analysis cannot encode musically meaningful structure. Acknowledging breakthroughs in other perceptual AI domains, we offer that deep learning holds the potential to overcome each of these obstacles. Through conceptual\u00a0\u2026",
      "citation_histogram": [
        [2013, 3],
        [2014, 17],
        [2015, 19],
        [2016, 19],
        [2017, 27],
        [2018, 23],
        [2019, 12],
        [2020, 18],
        [2021, 10],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:lIaPce-xyHYC",
      "title": "MDETR-modulated detection for end-to-end multi-modal understanding",
      "link": "http://openaccess.thecvf.com/content/ICCV2021/html/Kamath_MDETR_-_Modulated_Detection_for_End-to-End_Multi-Modal_Understanding_ICCV_2021_paper.html",
      "year": 2021,
      "cited_by": 154,
      "authors": [
        "Aishwarya Kamath",
        "Mannat Singh",
        "Yann LeCun",
        "Gabriel Synnaeve",
        "Ishan Misra",
        "Nicolas Carion"
      ],
      "description": "Multi-modal reasoning systems rely on a pre-trained object detector to extract regions of interest from the image. However, this crucial module is typically used as a black box, trained independently of the downstream task and on a fixed vocabulary of objects and attributes. This makes it challenging for such systems to capture the long tail of visual concepts expressed in free form text. In this paper we propose MDETR, an end-to-end modulated detector that detects objects in an image conditioned on a raw text query, like a caption or a question. We use a transformer-based architecture to reason jointly over text and image by fusing the two modalities at an early stage of the model. We pre-train the network on 1.3 M text-image pairs, mined from pre-existing multi-modal datasets having explicit alignment between phrases in text and objects in the image. We then fine-tune on several downstream tasks such as phrase grounding, referring expression comprehension and segmentation, achieving state-of-the-art results on popular benchmarks. We also investigate the utility of our model as an object detector on a given label set when fine-tuned in a few-shot setting. We show that our pre-training approach provides a way to handle the long tail of object categories which have very few labelled instances. Our approach can be easily extended for visual question answering, achieving competitive performance on GQA and CLEVR. The code and models are available at https://github. com/ashkamath/mdetr.",
      "citation_histogram": [
        [2021, 24],
        [2022, 130]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:X9ykpCP0fEIC",
      "title": "Globally trained handwritten word recognizer using spatial representation, convolutional neural networks, and hidden Markov models",
      "link": "https://proceedings.neurips.cc/paper/1993/hash/3b5dca501ee1e6d8cd7b905f4e1bf723-Abstract.html",
      "year": 1993,
      "cited_by": 151,
      "authors": ["Yoshua Bengio", "Yann LeCun", "Donnie Henderson"],
      "description": "We introduce a new approach for on-line recognition of handwrit (cid: 173) ten words written in unconstrained mixed style. The preprocessor performs a word-level normalization by fitting a model of the word structure using the EM algorithm. Words are then coded into low resolution\" annotated images\" where each pixel contains informa (cid: 173) tion about trajectory direction and curvature. The recognizer is a convolution network which can be spatially replicated. From the network output, a hidden Markov model produces word scores. The entire system is globally trained to minimize word-level errors.",
      "citation_histogram": [
        [1994, 2],
        [1995, 9],
        [1996, 10],
        [1997, 3],
        [1998, 5],
        [1999, 6],
        [2000, 3],
        [2001, 5],
        [2002, 3],
        [2003, 10],
        [2004, 2],
        [2005, 3],
        [2006, 4],
        [2007, 6],
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 2],
        [2012, 4],
        [2013, 7],
        [2014, 11],
        [2015, 7],
        [2016, 9],
        [2017, 7],
        [2018, 9],
        [2019, 14],
        [2020, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:DXM8KF1schAC",
      "title": "Unsupervised learning of spatiotemporally coherent metrics",
      "link": "http://openaccess.thecvf.com/content_iccv_2015/html/Goroshin_Unsupervised_Learning_of_ICCV_2015_paper.html",
      "year": 2015,
      "cited_by": 147,
      "authors": [
        "Ross Goroshin",
        "Joan Bruna",
        "Jonathan Tompson",
        "David Eigen",
        "Yann LeCun"
      ],
      "description": "Current state-of-the-art classification and detection algorithms train deep convolutional networks using labeled data. In this work we study unsupervised feature learning with convolutional networks in the context of temporally coherent unlabeled data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning and metric learning. Using this connection we define\" temporal coherence\"--a criterion which can be used to select hyper-parameters automatically. In a transfer learning experiment, we show that the resulting encoder can be used to define a more semantically coherent metric without the use of labeled data.",
      "citation_histogram": [
        [2015, 6],
        [2016, 15],
        [2017, 21],
        [2018, 23],
        [2019, 19],
        [2020, 21],
        [2021, 20],
        [2022, 20]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:SIv7DqKytYAC",
      "title": "A unified energy-based framework for unsupervised learning",
      "link": "https://proceedings.mlr.press/v2/ranzato07a.html",
      "year": 2007,
      "cited_by": 144,
      "authors": [
        "Marc'Aurelio Ranzato",
        "Y-Lan Boureau",
        "Sumit Chopra",
        "Yann LeCun"
      ],
      "description": "We introduce a view of unsupervised learning that integrates probabilistic and nonprobabilistic methods for clustering, dimensionality reduction, and feature extraction in a unified framework. In this framework, an energy function associates low energies to input points that are similar to training samples, and high energies to unobserved points. Learning consists in minimizing the energies of training samples while ensuring that the energies of unobserved ones are higher. Some traditional methods construct the architecture so that only a small number of points can have low energy, while other methods explicitly \u201cpull up\u201d on the energies of unobserved points. In probabilistic methods the energy of unobserved points is pulled by minimizing the log partition function, an expensive, and sometimes intractable process. We explore different and more efficient methods using an energy-based approach. In particular, we show that a simple solution is to restrict the amount of information contained in codes that represent the data. We demonstrate such a method by training it on natural image patches and by applying to image denoising.",
      "citation_histogram": [
        [2007, 2],
        [2008, 4],
        [2009, 2],
        [2010, 5],
        [2011, 3],
        [2012, 6],
        [2013, 9],
        [2014, 5],
        [2015, 13],
        [2016, 7],
        [2017, 9],
        [2018, 10],
        [2019, 11],
        [2020, 18],
        [2021, 22],
        [2022, 13]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Xt3t5HrhCpYC",
      "title": "Orthogonal RNNs and Long-Memory Tasks",
      "link": null,
      "year": 2016,
      "cited_by": 141,
      "authors": ["Mikael Henaff", "Arthur Szlam", "Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [2016, 3],
        [2017, 18],
        [2018, 24],
        [2019, 19],
        [2020, 41],
        [2021, 20],
        [2022, 16]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:9ZlFYXVOiuMC",
      "title": "Automatic recognition of biological particles in microscopic images",
      "link": "https://www.sciencedirect.com/science/article/pii/S0167865506001668",
      "year": 2007,
      "cited_by": 140,
      "authors": [
        "M Ranzato",
        "PE Taylor",
        "James M House",
        "RC Flagan",
        "Yann LeCun",
        "Pietro Perona"
      ],
      "description": "A simple and general-purpose system to recognize biological particles is presented. It is composed of four stages: First (if necessary) promising locations in the image are detected and small regions containing interesting samples are extracted using a feature finder. Second, differential invariants of the brightness are computed at multiple scales of resolution. Third, after point-wise non-linear mappings to a higher dimensional feature space, this information is averaged over the whole region thus producing a vector of features for each sample that is invariant with respect to rotation and translation. Fourth, each sample is classified using a classifier obtained from a mixture-of-Gaussians generative model.This system was developed to classify 12 categories of particles found in human urine; it achieves a 93.2% correct classification rate in this application. It was subsequently trained and tested on a challenging set of\u00a0\u2026",
      "citation_histogram": [
        [2007, 4],
        [2008, 9],
        [2009, 7],
        [2010, 9],
        [2011, 9],
        [2012, 14],
        [2013, 12],
        [2014, 5],
        [2015, 10],
        [2016, 10],
        [2017, 9],
        [2018, 10],
        [2019, 11],
        [2020, 9],
        [2021, 8],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:qUcmZB5y_30C",
      "title": "Method for sending multi-media messages using emoticons",
      "link": "https://patents.google.com/patent/US6990452B1/en",
      "year": 2006,
      "cited_by": 140,
      "authors": [
        "Joern Ostermann",
        "Mehmet Reha Civanlar",
        "Eric Cosatto",
        "Hans Peter Graf",
        "Yann Andre LeCun"
      ],
      "description": "A system and method of providing sender-customization of multi-media messages through the use of emoticons is disclosed. The sender inserts the emoticons into a text message. As an animated face audibly delivers the text, emoticons associated with the message are started a predetermined period of time or number of words prior to the position of the emoticon in the message text and completed a predetermined length of time or number of words following the location of the emoticon. The sender may insert emoticons through the use of emoticon buttons that are icons available for choosing. Upon sender selections of an emoticon, an icon representing the emoticon is inserted into the text at the position of the cursor. Once an emoticon is chosen, the sender may also choose the amplitude for the emoticon and increased or decreased amplitude will be displayed in the icon inserted into the message text.",
      "citation_histogram": [
        [2005, 5],
        [2006, 3],
        [2007, 5],
        [2008, 11],
        [2009, 9],
        [2010, 6],
        [2011, 7],
        [2012, 8],
        [2013, 8],
        [2014, 5],
        [2015, 3],
        [2016, 9],
        [2017, 7],
        [2018, 5],
        [2019, 13],
        [2020, 17],
        [2021, 13],
        [2022, 6]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:4TOpqqG69KYC",
      "title": "Automatic learning rate maximization by on-line estimation of the Hessian\u2019s eigenvectors",
      "link": "https://proceedings.neurips.cc/paper/1992/hash/30bb3825e8f631cc6075c0f87bb4978c-Abstract.html",
      "year": 1993,
      "cited_by": 140,
      "authors": ["Yann LeCun", "Patrice Y Simard", "Barak Pearlmutter"],
      "description": "We propose a very simple, and well principled way of computing the optimal step size in gradient descent algorithms. The on-line version is very efficient computationally, and is applicable to large backpropagation networks trained on large data sets. The main ingredient is a technique for estimating the principal eigenvalue (s) and eigenvector (s) of the objective function's second derivative ma (cid: 173) trix (Hessian), which does not require to even calculate the Hes (cid: 173) sian. Several other applications of this technique are proposed for speeding up learning, or for eliminating useless parameters.",
      "citation_histogram": [
        [1993, 3],
        [1994, 5],
        [1995, 3],
        [1996, 5],
        [1997, 7],
        [1998, 8],
        [1999, 2],
        [2000, 4],
        [2001, 6],
        [2002, 3],
        [2003, 2],
        [2004, 6],
        [2005, 2],
        [2006, 3],
        [2007, 4],
        [2008, 1],
        [2009, 1],
        [2010, 4],
        [2011, 5],
        [2012, 1],
        [2013, 9],
        [2014, 3],
        [2015, 3],
        [2016, 9],
        [2017, 4],
        [2018, 10],
        [2019, 13],
        [2020, 11],
        [2021, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:w1MjKQ0l0TYC",
      "title": "Second order properties of error surfaces: Learning time and generalization",
      "link": "https://proceedings.neurips.cc/paper/1990/hash/758874998f5bd0c393da094e1967a72b-Abstract.html",
      "year": 1990,
      "cited_by": 139,
      "authors": ["Yann LeCun", "Ido Kanter", "Sara Solla"],
      "description": "The learning time of a simple neural network model is obtained through an analytic computation of the eigenvalue spectrum for the Hessian matrix, which describes the second order properties of the cost function in the space of coupling coefficients. The form of the eigenvalue distribution suggests new techniques for accelerating the learning process, and provides a theoretical justification for the choice of centered versus biased state variables.",
      "citation_histogram": [
        [1991, 4],
        [1992, 3],
        [1993, 3],
        [1994, 6],
        [1995, 2],
        [1996, 6],
        [1997, 4],
        [1998, 2],
        [1999, 2],
        [2000, 1],
        [2001, 2],
        [2002, 5],
        [2003, 1],
        [2004, 4],
        [2005, 3],
        [2006, 3],
        [2007, 2],
        [2008, 2],
        [2009, 1],
        [2010, 1],
        [2011, 2],
        [2012, 5],
        [2013, 1],
        [2014, 5],
        [2015, 12],
        [2016, 10],
        [2017, 14],
        [2018, 12],
        [2019, 10],
        [2020, 9]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:UebtZRa9Y70C",
      "title": "Image and video coding-emerging standards and beyond",
      "link": "https://ieeexplore.ieee.org/abstract/document/735379/",
      "year": 1998,
      "cited_by": 138,
      "authors": [
        "Barry G Haskell",
        "Paul G Howard",
        "Yann A LeCun",
        "Atul Puri",
        "Joern Ostermann",
        "M Reha Civanlar",
        "Lawrence Rabiner",
        "Leon Bottou",
        "Patrick Haffner"
      ],
      "description": "Discusses coding standards for still images and motion video. We first briefly discuss standards already in use, including: Group 3 and Group 4 for bilevel fax images; JPEG for still color images; and H.261, H.263, MPEG-1, and MPEG-2 for motion video. We then cover newly emerging standards such as JBIG1 and JBIG2 for bilevel fax images, JPEG-2000 for still color images, and H.263+ and MPEG-4 for motion video. Finally, we describe some directions beyond the standards such as hybrid coding of graphics/photo images, MPEG-7 for multimedia metadata, and possible new technologies.",
      "citation_histogram": [
        [1999, 9],
        [2000, 15],
        [2001, 17],
        [2002, 5],
        [2003, 7],
        [2004, 13],
        [2005, 11],
        [2006, 8],
        [2007, 8],
        [2008, 5],
        [2009, 8],
        [2010, 3],
        [2011, 5],
        [2012, 3],
        [2013, 4],
        [2014, 1],
        [2015, 2],
        [2016, 5],
        [2017, 1],
        [2018, 1],
        [2019, 3],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:S1WaUgt8gYIC",
      "title": "The mnist database of handwritten digits",
      "link": "https://scholar.google.com/scholar?cluster=9965202229435230351&hl=en&oi=scholarr",
      "year": 1998,
      "cited_by": 138,
      "authors": ["Lecun Yann", "Cortes Corinna", "J Christopher"],
      "description": null,
      "citation_histogram": [
        [2007, 1],
        [2008, 1],
        [2009, 2],
        [2010, 3],
        [2011, 2],
        [2012, 3],
        [2013, 10],
        [2014, 8],
        [2015, 18],
        [2016, 19],
        [2017, 24],
        [2018, 27],
        [2019, 20]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:ZfRJV9d4-WMC",
      "title": "Global training of document processing systems using graph transformer networks",
      "link": "https://ieeexplore.ieee.org/abstract/document/609370/",
      "year": 1997,
      "cited_by": 136,
      "authors": ["Leon Bottou", "Yoshua Bengio", "Yann Le Cun"],
      "description": "We propose a new machine learning paradigm called Graph Transformer Networks that extends the applicability of gradient-based learning algorithms to systems composed of modules that take graphs as inputs and produce graphs as output. Training is performed by computing gradients of a global objective function with respect to all the parameters in the system using a kind of back-propagation procedure. A complete check reading system based on these concepts is described. The system uses convolutional neural network character recognizers, combined with global training techniques to provide record accuracy on business and personal checks. It is presently deployed commercially and reads million of checks per month.",
      "citation_histogram": [
        [1998, 1],
        [1999, 2],
        [2000, 1],
        [2001, 1],
        [2002, 1],
        [2003, 2],
        [2004, 5],
        [2005, 3],
        [2006, 3],
        [2007, 10],
        [2008, 4],
        [2009, 6],
        [2010, 7],
        [2011, 14],
        [2012, 18],
        [2013, 9],
        [2014, 13],
        [2015, 7],
        [2016, 7],
        [2017, 10],
        [2018, 7]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:hqOjcs7Dif8C",
      "title": "Hierarchical constrained automatic learning neural network for character recognition",
      "link": "https://patents.google.com/patent/US5067164A/en",
      "year": 1991,
      "cited_by": 136,
      "authors": [
        "John S Denker",
        "Richard E Howard",
        "Lawrence D Jackel",
        "Yann LeCun"
      ],
      "description": "Highly accurate, reliable optical character recognition is afforded by a layered network having several layers of constrained feature detection wherein each layer of constrained feature detection includes a plurality of constrained feature maps and a corresponding plurality of feature reduction maps. Each feature reduction map is connected to only one constrained feature map in the same layer for undersampling that constrained feature map. Units in each constrained feature map of the first constrained feature detection layer respond as a function of a corresponding kernel and of different portions of the pixel image of the character captured in a receptive field associated with the unit. Units in each feature map of the second constrained feature detection layer respond as a function of a corresponding kernel and of different portions of an individual feature reduction map or a combination of several feature reduction\u00a0\u2026",
      "citation_histogram": [
        [1993, 5],
        [1994, 6],
        [1995, 6],
        [1996, 7],
        [1997, 6],
        [1998, 7],
        [1999, 1],
        [2000, 1],
        [2001, 2],
        [2002, 1],
        [2003, 4],
        [2004, 7],
        [2005, 8],
        [2006, 4],
        [2007, 7],
        [2008, 8],
        [2009, 9],
        [2010, 7],
        [2011, 8],
        [2012, 4],
        [2013, 6],
        [2014, 2],
        [2015, 4],
        [2016, 4],
        [2017, 4],
        [2018, 4],
        [2019, 1],
        [2020, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:5aszrCQfcYQC",
      "title": "Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv 2013",
      "link": "https://scholar.google.com/scholar?cluster=11511151978970788369&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 134,
      "authors": [
        "P Sermanet",
        "D Eigen",
        "X Zhang",
        "M Mathieu",
        "R Fergus",
        "Y LeCun"
      ],
      "description": null,
      "citation_histogram": [
        [2017, 10],
        [2018, 8],
        [2019, 20],
        [2020, 29],
        [2021, 36],
        [2022, 31]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:yWe6nybXSkwC",
      "title": "Deep learning tutorial",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.366.4088&rep=rep1&type=pdf",
      "year": 2013,
      "cited_by": 130,
      "authors": ["Yann LeCun", "M Ranzato"],
      "description": "Deep Learning Page 1 Y LeCun MA Ranzato Deep Learning Tutorial ICML, Atlanta, 2013-06-16 \nYann LeCun Center for Data Science & Courant Institute, NYU yann@cs.nyu.edu http://yann.lecun.com \nMarc'Aurelio Ranzato Google ranzato@google.com http://www.cs.toronto.edu/~ranzato Page \n2 Y LeCun MA Ranzato Deep Learning = Learning Representations/Features The traditional \nmodel of pattern recognition (since the late 50's) Fixed/engineered features (or fixed kernel) \n+ trainable classifier End-to-end learning / Feature learning / Deep learning Trainable \nfeatures (or kernel) + trainable classifier \u201cSimple\u201d Trainable Classifier hand-crafted Feature \nExtractor Trainable Classifier Trainable Feature Extractor Page 3 Y LeCun MA Ranzato \nThis Basic Model has not evolved much since the 50's The first learning machine: the \nPerceptron Built at Cornell in 1960 The Perceptron was a linear classifier on top of a simple \u2026",
      "citation_histogram": [
        [2014, 7],
        [2015, 16],
        [2016, 12],
        [2017, 30],
        [2018, 12],
        [2019, 17],
        [2020, 11],
        [2021, 16],
        [2022, 9]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:PyEswDtIyv0C",
      "title": "NeuFlow: Dataflow vision processing system-on-a-chip",
      "link": "https://ieeexplore.ieee.org/abstract/document/6292202/",
      "year": 2012,
      "cited_by": 128,
      "authors": [
        "Phi-Hung Pham",
        "Darko Jelaca",
        "Clement Farabet",
        "Berin Martini",
        "Yann LeCun",
        "Eugenio Culurciello"
      ],
      "description": "This paper presents a bio-inspired vision system-on-a-chip - neuFlow SoC implemented in the IBM 45 nm SOI process. The neuFlow SoC was designed to accelerate neural networks and other complex vision algorithms based on large numbers of convolutions and matrix-to-matrix operations. Post-layout characterization shows that the system delivers up to 320 GOPS with an average power consumption of 0.6 W. The power-efficiency and portability of this system is ideal for embedded vision-based devices, such as driver assistance, and robotic vision.",
      "citation_histogram": [
        [2013, 2],
        [2014, 6],
        [2015, 15],
        [2016, 24],
        [2017, 24],
        [2018, 15],
        [2019, 11],
        [2020, 11],
        [2021, 15],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:sSrBHYA8nusC",
      "title": "Large-scale FPGA-based convolutional networks",
      "link": "https://books.google.com/books?hl=en&lr=&id=c5v5USMvcMYC&oi=fnd&pg=PA399&dq=info:s0K_h8dLZVwJ:scholar.google.com&ots=honDOfLv8n&sig=TxYjlHDDgtkjpPtdC5S2_90Cw1Y",
      "year": 2011,
      "cited_by": 127,
      "authors": [
        "Cl\u00e9ment Farabet",
        "Yann LeCun",
        "Koray Kavukcuoglu",
        "Eugenio Culurciello",
        "Berin Martini",
        "Polina Akselrod",
        "Selcuk Talay"
      ],
      "description": "Many successful object recognition systems use dense features extracted on regularly spaced patches over the input image. The majority of the feature extraction systems have a common structure composed of a \ufb01lter bank (generally based on oriented edge detectors or 2D Gabor functions), a nonlinear operation (quantization, winner-take-all, sparsi\ufb01cation, normalization, and/or pointwise saturation), and \ufb01nally a pooling operation (max, average, or histogramming). For example, the scale-invariant feature transform (S IFT)(Lowe, 2004) operator applies oriented edge \ufb01lters to a small patch and determines the dominant orientation through a winner-take-all operation. Finally, the resulting sparse vectors are added (pooled) over a larger patch to form a local orientation histogram. Some recognition systems use a single stage of feature extractors (Lazebnik, Schmid, and Ponce, 2006; Dalal and Triggs, 2005; Berg, Berg, and Malik, 2005; Pinto, Cox, and DiCarlo, 2008). Other models such as HMAX-type models (Serre, Wolf, and Poggio, 2005; Mutch, and Lowe, 2006) and convolutional networks use two more layers of successive feature extractors. Different training algorithms have been used for learning the parameters of convolutional networks. In LeCun et al.(1998b) and Huang and LeCun (2006), pure supervised learning is used to update the parameters. However, recent works have focused on training with an auxiliary task (Ahmed et al., 2008) or using unsupervised objectives (Ranzato et al., 2007b; Kavukcuoglu et al., 2009; Jarrett et al., 2009; Lee et al., 2009).This chapter presents a scalable hardware architecture for large-scale multi-layered\u00a0\u2026",
      "citation_histogram": [
        [2011, 1],
        [2012, 3],
        [2013, 5],
        [2014, 13],
        [2015, 9],
        [2016, 19],
        [2017, 24],
        [2018, 14],
        [2019, 14],
        [2020, 12],
        [2021, 9],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:goOyc-W9OFwC",
      "title": "Learning to linearize under uncertainty",
      "link": "https://proceedings.neurips.cc/paper/5951-learning-to-linearize-under-uncertainty",
      "year": 2015,
      "cited_by": 125,
      "authors": ["Ross Goroshin", "Michael F Mathieu", "Yann LeCun"],
      "description": "Training deep feature hierarchies to solve supervised learning tasks has achieving state of the art performance on many problems in computer vision. However, a principled way in which to train such hierarchies in the unsupervised setting has remained elusive. In this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabelednatural video sequences. This is done by training a generative model to predict video frames. We also address the problem of inherent uncertainty in prediction by introducing a latent variables that are non-deterministic functions of the input into the network architecture.",
      "citation_histogram": [
        [2015, 7],
        [2016, 10],
        [2017, 19],
        [2018, 29],
        [2019, 17],
        [2020, 17],
        [2021, 15],
        [2022, 10]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:4oLgDUE9yTUC",
      "title": "GJ n",
      "link": "https://scholar.google.com/scholar?cluster=6576681525338856058&hl=en&oi=scholarr",
      "year": 2015,
      "cited_by": 121,
      "authors": ["Yann LeCun", "Yoshua Bengio"],
      "description": null,
      "citation_histogram": [
        [2015, 1],
        [2016, 1],
        [2017, 12],
        [2018, 14],
        [2019, 31],
        [2020, 30],
        [2021, 18],
        [2022, 13]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:e5wmG9Sq2KIC",
      "title": "Deep belief net learning in a long-range vision system for autonomous off-road driving",
      "link": "https://ieeexplore.ieee.org/abstract/document/4651217/",
      "year": 2008,
      "cited_by": 120,
      "authors": [
        "Raia Hadsell",
        "Ayse Erkan",
        "Pierre Sermanet",
        "Marco Scoffier",
        "Urs Muller",
        "Yann LeCun"
      ],
      "description": "We present a learning-based approach for long-range vision that is able to accurately classify complex terrain at distances up to the horizon, thus allowing high-level strategic planning. A deep belief network is trained with unsupervised data and a reconstruction criterion to extract features from an input image, and the features are used to train a realtime classifier to predict traversability. The online supervision is given by a stereo module that provides robust labels for nearby areas up to 12 meters distant. The approach was developed and tested on the LAGR mobile robot.",
      "citation_histogram": [
        [2008, 2],
        [2009, 5],
        [2010, 7],
        [2011, 3],
        [2012, 4],
        [2013, 1],
        [2014, 5],
        [2015, 14],
        [2016, 10],
        [2017, 16],
        [2018, 18],
        [2019, 12],
        [2020, 4],
        [2021, 8],
        [2022, 8]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:QjNCP7ux8QYC",
      "title": "Open problem: The landscape of the loss surfaces of multilayer networks",
      "link": "http://proceedings.mlr.press/v40/Choromanska15.html",
      "year": 2015,
      "cited_by": 113,
      "authors": ["Anna Choromanska", "Yann LeCun", "G\u00e9rard Ben Arous"],
      "description": "Deep learning has enjoyed a resurgence of interest in the last few years for such applications as image and speech recognition, or natural language processing. The vast majority of practical applications of deep learning focus on supervised learning, where the supervised loss function is minimized using stochastic gradient descent. The properties of this highly non-convex loss function, such as its landscape and the behavior of critical points (maxima, minima, and saddle points), as well as the reason why large-and small-size networks achieve radically different practical performance, are however very poorly understood. It was only recently shown that new results in spin-glass theory potentially may provide an explanation for these problems by establishing a connection between the loss function of the neural networks and the Hamiltonian of the spherical spin-glass models. The connection between both models relies on a number of possibly unrealistic assumptions, yet the empirical evidence suggests that the connection may exist in real. The question we pose is whether it is possible to drop some of these assumptions to establish a stronger connection between both models.",
      "citation_histogram": [
        [2015, 1],
        [2016, 8],
        [2017, 18],
        [2018, 26],
        [2019, 22],
        [2020, 11],
        [2021, 13],
        [2022, 14]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:t1niNHmIXQYC",
      "title": "A theoretical analysis of feature pooling in vision algorithms",
      "link": "https://scholar.google.com/scholar?cluster=14086802185197311097&hl=en&oi=scholarr",
      "year": 2010,
      "cited_by": 113,
      "authors": ["Y Boureau", "Jean Ponce", "Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [2011, 6],
        [2012, 12],
        [2013, 14],
        [2014, 16],
        [2015, 12],
        [2016, 17],
        [2017, 6],
        [2018, 3],
        [2019, 9],
        [2020, 7],
        [2021, 5],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:ziOE8S1-AIUC",
      "title": "Statistical machine learning and dissolved gas analysis: a review",
      "link": "https://ieeexplore.ieee.org/abstract/document/6301810/",
      "year": 2012,
      "cited_by": 112,
      "authors": ["Piotr Mirowski", "Yann LeCun"],
      "description": "Dissolved gas analysis (DGA) of the insulation oil of power transformers is an investigative tool to monitor their health and to detect impending failures by recognizing anomalous patterns of DGA concentrations. We handle the failure prediction problem as a simple data-mining task on DGA samples, optionally exploiting the transformer's age, nominal power and voltage, and consider two approaches: 1) binary classification and 2) regression of the time to failure. We propose a simple logarithmic transform to preprocess DGA data in order to deal with long-tail distributions of concentrations. We have reviewed and evaluated 15 standard statistical machine-learning algorithms on that task, and reported quantitative results on a small but published set of power transformers and on proprietary data from thousands of network transformers of a utility company. Our results confirm that nonlinear decision functions, such as\u00a0\u2026",
      "citation_histogram": [
        [2013, 3],
        [2014, 3],
        [2015, 9],
        [2016, 3],
        [2017, 9],
        [2018, 17],
        [2019, 14],
        [2020, 8],
        [2021, 22],
        [2022, 24]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:4JMBOYKVnBMC",
      "title": "Method for sending multi-media messages using customizable background images",
      "link": "https://patents.google.com/patent/US7035803B1/en",
      "year": 2006,
      "cited_by": 111,
      "authors": [
        "Joern Ostermann",
        "Barbara Buda",
        "Mehmet Reha Civanlar",
        "Eric Cosatto",
        "Hans Peter Graf",
        "Thomas M Isaacson",
        "Yann Andre LeCun"
      ],
      "description": "A system and method of providing sender customization of multi-media messages through the use of inserted images or video. The images or video may be sender-created or predefined and available to the sender via a web server. The method relates to customizing a multi-media message created by a sender for a recipient, the multi-media message having an animated entity audibly presenting speech converted from text created by the sender. The method comprises receiving at least one image from the sender, associating each at least one image with a tag, presenting the sender with options to insert the tag associated with one of the at least one image into the sender text, and after the sender inserts the tag associated with one of the at least one images into the sender text, delivering the multi-media message with the at least one image presented as background to the animated entity according to a position of\u00a0\u2026",
      "citation_histogram": [
        [2005, 3],
        [2006, 3],
        [2007, 6],
        [2008, 7],
        [2009, 13],
        [2010, 6],
        [2011, 9],
        [2012, 7],
        [2013, 4],
        [2014, 7],
        [2015, 6],
        [2016, 7],
        [2017, 11],
        [2018, 3],
        [2019, 6],
        [2020, 4],
        [2021, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:tzM49s52ZIMC",
      "title": "Reading checks with multilayer graph transformer networks",
      "link": "https://ieeexplore.ieee.org/abstract/document/599580/",
      "year": 1997,
      "cited_by": 109,
      "authors": ["Yann LeCun", "Leon Bottou", "Yoshua Bengio"],
      "description": "We propose a new machine learning paradigm called multilayer graph transformer network that extends the applicability of gradient-based learning algorithms to systems composed of modules that take graphs as input and produce graphs as output. A complete check reading system based on this concept is described. The system combines convolutional neural network character recognizers with graph-based stochastic models trained cooperatively at the document level. It is deployed commercially and reads million of business and personal checks per month with record accuracy.",
      "citation_histogram": [
        [1997, 2],
        [1998, 7],
        [1999, 3],
        [2000, 4],
        [2001, 1],
        [2002, 2],
        [2003, 1],
        [2004, 1],
        [2005, 3],
        [2006, 1],
        [2007, 2],
        [2008, 2],
        [2009, 1],
        [2010, 4],
        [2011, 2],
        [2012, 5],
        [2013, 7],
        [2014, 12],
        [2015, 8],
        [2016, 12],
        [2017, 13],
        [2018, 10],
        [2019, 5]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:EbDKHhGulSgC",
      "title": "Deep Learning Hardware: Past, Present, and Future",
      "link": "https://ieeexplore.ieee.org/abstract/document/8662396/",
      "year": 2019,
      "cited_by": 108,
      "authors": ["Yann LeCun"],
      "description": "Historically, progress in neural networks and deep learning research has been greatly influenced by the available hardware and software tools. This paper identifies trends in deep learning research that will influence hardware architectures and software platforms of the future.",
      "citation_histogram": [
        [2019, 12],
        [2020, 33],
        [2021, 39],
        [2022, 23]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:kbDB7uaqCfAC",
      "title": "Spectral networks and deep locally connected networks on graphs",
      "link": "https://scholar.google.com/scholar?cluster=17396151850239714533&hl=en&oi=scholarr",
      "year": 2014,
      "cited_by": 106,
      "authors": [
        "Joan Bruna Estrach",
        "Wojciech Zaremba",
        "Arthur Szlam",
        "Yann LeCun"
      ],
      "description": null,
      "citation_histogram": [
        [2014, 1],
        [2015, 11],
        [2016, 2],
        [2017, 1],
        [2018, 3],
        [2019, 13],
        [2020, 34],
        [2021, 40]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:MXK_kJrjxJIC",
      "title": "On the applications of multimedia processing to communications",
      "link": "https://ieeexplore.ieee.org/abstract/document/664272/",
      "year": 1998,
      "cited_by": 100,
      "authors": [
        "Richard V Cox",
        "Barry G Haskell",
        "Yann LeCun",
        "Behzad Shahraray",
        "Lawrence Rabiner"
      ],
      "description": "The challenge of multimedia processing is to provide services that seamlessly integrate text, sound, image, and video information and to do it in a way that preserves the ease of use and interactivity of conventional plain old telephone service (POTS) telephony. To achieve this goal, there are a number of technological problems that must be considered, including: compression and coding of multimedia signals, including algorithmic issues, standards issues, and transmission issues; synthesis and recognition of multimedia signals, including speech, images, handwriting, and text; organization, storage, and retrieval of multimedia signals, including the appropriate method and speed of delivery, resolution, and quality of service; access methods to the multimedia signal, including spoken natural language interfaces, agent interfaces, and media conversion tools; searching by text, speech, and image queries; browsing by\u00a0\u2026",
      "citation_histogram": [
        [1997, 2],
        [1998, 5],
        [1999, 13],
        [2000, 8],
        [2001, 5],
        [2002, 5],
        [2003, 3],
        [2004, 6],
        [2005, 5],
        [2006, 3],
        [2007, 3],
        [2008, 1],
        [2009, 3],
        [2010, 1],
        [2011, 1],
        [2012, 8],
        [2013, 3],
        [2014, 6],
        [2015, 1],
        [2016, 3],
        [2017, 2],
        [2018, 1],
        [2019, 6],
        [2020, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:3_iODIlCio4C",
      "title": "Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic",
      "link": "https://arxiv.org/abs/1901.02705",
      "year": 2019,
      "cited_by": 96,
      "authors": ["Mikael Henaff", "Alfredo Canziani", "Yann LeCun"],
      "description": "Learning a policy using only observational data is challenging because the distribution of states it induces at execution time may differ from the distribution observed during training. We propose to train a policy by unrolling a learned model of the environment dynamics over multiple time steps while explicitly penalizing two costs: the original cost the policy seeks to optimize, and an uncertainty cost which represents its divergence from the states it is trained on. We measure this second cost by using the uncertainty of the dynamics model about its own predictions, using recent ideas from uncertainty estimation for deep networks. We evaluate our approach using a large-scale observational dataset of driving behavior recorded from traffic cameras, and show that we are able to learn effective driving policies from purely observational data, with no environment interaction.",
      "citation_histogram": [
        [2019, 16],
        [2020, 26],
        [2021, 33],
        [2022, 20]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:IyMil-iDmu0C",
      "title": "Signal Recovery from Lp Pooling Representations",
      "link": null,
      "year": 2014,
      "cited_by": 95,
      "authors": ["Joan Bruna", "Arthur Szlam", "Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [2014, 2],
        [2015, 6],
        [2016, 8],
        [2017, 7],
        [2018, 15],
        [2019, 14],
        [2020, 11],
        [2021, 21],
        [2022, 10]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:KIMFRoSX18MC",
      "title": "The MNIST database of handwritten digits (2010)",
      "link": "https://scholar.google.com/scholar?cluster=7928329841691456133&hl=en&oi=scholarr",
      "year": 2009,
      "cited_by": 93,
      "authors": ["Yann LeCun", "Corinna Cortes", "Christopher JC Burges"],
      "description": null,
      "citation_histogram": [
        [2008, 3],
        [2009, 1],
        [2010, 2],
        [2011, 3],
        [2012, 3],
        [2013, 2],
        [2014, 2],
        [2015, 3],
        [2016, 4],
        [2017, 8],
        [2018, 10],
        [2019, 12],
        [2020, 12],
        [2021, 13],
        [2022, 12]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:35r97b3x0nAC",
      "title": "Word-level training of a handwritten word recognizer based on convolutional neural networks",
      "link": "https://ieeexplore.ieee.org/abstract/document/576881/",
      "year": 1994,
      "cited_by": 93,
      "authors": ["Yann LeCun", "Yoshua Bengio"],
      "description": "We introduce a new approach for online recognition of handwritten words written in unconstrained mixed style. Words are represented by low resolution \"annotated images\" where each pixel contains information about trajectory direction and curvature. The recognizer is a convolutional network which can be spatially replicated. From the network output, a hidden Markov model produces word scores. The entire system is globally trained to minimize word-level errors.",
      "citation_histogram": [
        [1994, 1],
        [1995, 1],
        [1996, 2],
        [1997, 5],
        [1998, 4],
        [1999, 3],
        [2000, 1],
        [2001, 1],
        [2002, 2],
        [2003, 2],
        [2004, 2],
        [2005, 2],
        [2006, 1],
        [2007, 3],
        [2008, 3],
        [2009, 2],
        [2010, 2],
        [2011, 6],
        [2012, 5],
        [2013, 8],
        [2014, 16],
        [2015, 12],
        [2016, 3],
        [2017, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:M3ejUd6NZC8C",
      "title": "DjVu: Analyzing and compressing scanned documents for internet distribution",
      "link": "https://ieeexplore.ieee.org/abstract/document/791865/",
      "year": 1999,
      "cited_by": 91,
      "authors": [
        "Patrick Haffner",
        "L\u00e9on Bottou",
        "Paul G Howard",
        "Yann LeCun"
      ],
      "description": "DjVu is an image compression technique specifically geared towards the compression of scanned documents in color at high resolution. Typical color magazine pages scanned at 300 dpi are compressed to between 40 and 80 kBytes, or 5 to 10 times smaller than with JPEG for a similar level of subjective quality. The foreground layer, which contains the text and drawings and requires high spatial resolution, is separated from the background layer, which contains pictures and backgrounds and requires less resolution. The foreground is compressed with a bi-tonal image compression technique that takes advantage of character shape similarities. The background is compressed with a new progressive, wavelet-based compression method. A real-time, memory-efficient version of the decoder is available as a plug-in for popular Web browsers.",
      "citation_histogram": [
        [2001, 4],
        [2002, 2],
        [2003, 4],
        [2004, 4],
        [2005, 5],
        [2006, 7],
        [2007, 7],
        [2008, 7],
        [2009, 5],
        [2010, 8],
        [2011, 1],
        [2012, 1],
        [2013, 4],
        [2014, 5],
        [2015, 5],
        [2016, 2],
        [2017, 1],
        [2018, 3],
        [2019, 2],
        [2020, 5],
        [2021, 5]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:He_SdS2HmmMC",
      "title": "Comparing Dynamics: Deep Neural Networks versus Glassy Systems",
      "link": "https://proceedings.mlr.press/v80/baity-jesi18a.html",
      "year": 2018,
      "cited_by": 90,
      "authors": [
        "Marco Baity-Jesi",
        "Levent Sagun",
        "Mario Geiger",
        "Stefano Spigler",
        "G Ben Arous",
        "Chiara Cammarota",
        "Yann LeCun",
        "Matthieu Wyart",
        "Giulio Biroli"
      ],
      "description": "We analyze numerically the training dynamics of deep neural networks (DNN) by using methods developed in statistical physics of glassy systems. The two main issues we address are the complexity of the loss-landscape and of the dynamics within it, and to what extent DNNs share similarities with glassy systems. Our findings, obtained for different architectures and data-sets, suggest that during the training process the dynamics slows down because of an increasingly large number of flat directions. At large times, when the loss is approaching zero, the system diffuses at the bottom of the landscape. Despite some similarities with the dynamics of mean-field glassy systems, in particular, the absence of barrier crossing, we find distinctive dynamical behaviors in the two cases, thus showing that the statistical properties of the corresponding loss and energy landscapes are different. In contrast, when the network is under-parametrized we observe a typical glassy behavior, thus suggesting the existence of different phases depending on whether the network is under-parametrized or over-parametrized.",
      "citation_histogram": [
        [2018, 7],
        [2019, 19],
        [2020, 19],
        [2021, 26],
        [2022, 19]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Zph67rFs4hoC",
      "title": "Method and apparatus for extracting a foreground image and a background image from a color document image",
      "link": "https://patents.google.com/patent/US5900953A/en",
      "year": 1999,
      "cited_by": 90,
      "authors": ["Leon Bottou", "Yann Andre LeCun"],
      "description": "A method and apparatus extracts a foreground image and a background image from a color document image. The color document image is divided into a plurality of multiscaled grids. Each of the grids includes a plurality of blocks and the resolution of the plurality of blocks increases for each successive grid. The background color and the foreground color of each block of the largest grid is first determined. Then the background color and the foreground color of each block of the smaller grids are determined iteratively, using biasing information from each block's corresponding block, until the background and foreground colors of all of the blocks of all of the grids have been determined. Finally, the foreground and background images are formed from the determined background color and foreground color of each block of the smallest grid.",
      "citation_histogram": [
        [2003, 1],
        [2004, 4],
        [2005, 4],
        [2006, 6],
        [2007, 11],
        [2008, 16],
        [2009, 9],
        [2010, 2],
        [2011, 8],
        [2012, 6],
        [2013, 7],
        [2014, 6],
        [2015, 3],
        [2016, 1],
        [2017, 2],
        [2018, 1],
        [2019, 1],
        [2020, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:abG-DnoFyZgC",
      "title": "Emergence of complex-like cells in a temporal product network with local receptive fields",
      "link": "https://arxiv.org/abs/1006.0448",
      "year": 2010,
      "cited_by": 85,
      "authors": ["Karo Gregor", "Yann LeCun"],
      "description": "We introduce a new neural architecture and an unsupervised algorithm for learning invariant representations from temporal sequence of images. The system uses two groups of complex cells whose outputs are combined multiplicatively: one that represents the content of the image, constrained to be constant over several consecutive frames, and one that represents the precise location of features, which is allowed to vary over time but constrained to be sparse. The architecture uses an encoder to extract features, and a decoder to reconstruct the input from the features. The method was applied to patches extracted from consecutive movie frames and produces orientation and frequency selective units analogous to the complex cells in V1. An extension of the method is proposed to train a network composed of units with local receptive field spread over a large image of arbitrary size. A layer of complex cells, subject to sparsity constraints, pool feature units over overlapping local neighborhoods, which causes the feature units to organize themselves into pinwheel patterns of orientation-selective receptive fields, similar to those observed in the mammalian visual cortex. A feed-forward encoder efficiently computes the feature representation of full images.",
      "citation_histogram": [
        [2010, 2],
        [2011, 4],
        [2012, 1],
        [2013, 9],
        [2014, 7],
        [2015, 4],
        [2016, 9],
        [2017, 7],
        [2018, 11],
        [2019, 11],
        [2020, 5],
        [2021, 7],
        [2022, 8]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:sq_mVe84DbIC",
      "title": "Shape, contour and grouping in computer vision",
      "link": "https://scholar.google.com/scholar?cluster=1074578584353410507&hl=en&oi=scholarr",
      "year": 1999,
      "cited_by": 85,
      "authors": ["Yann LeCun", "P Haffner", "L Bottou", "Y Bengio"],
      "description": null,
      "citation_histogram": [
        [2016, 2],
        [2017, 8],
        [2018, 17],
        [2019, 24],
        [2020, 17],
        [2021, 16]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:7T2F9Uy0os0C",
      "title": "Efficient learning and second order methods",
      "link": "http://www.iro.umontreal.ca/~pift6266/A06/refs/YannNipsTutorial.pdf",
      "year": 1993,
      "cited_by": 84,
      "authors": ["Yann LeCun"],
      "description": "EFFICIENT LEARNING AND SECOND\u2212ORDER METHODS EFFICIENT LEARNING AND \nSECOND\u2212ORDER METHODS Page 1 Yann Le Cun Adaptive Systems Research Dept AT&T \nBell Laboratories Holmdel, NJ USA EFFICIENT LEARNING AND SECOND\u2212ORDER METHODS \nEFFICIENT LEARNING AND SECOND\u2212ORDER METHODS Page 2 OVERVIEW 1 \u2212 Plain \nBackprop: how to make it work \u2212 Basic concepts, terminology and notation \u2212 Intuitive analysis \nand practical tricks 2\u2212 The convergence of gradient descent \u2212 A little theory \u2212 Quadratic forms, \nHessians, and Eigenvalues \u2212 maximum learning rate, minimum learning time \u2212 How GD works \nin simple cases \u2212 a single 2\u2212input neuron \u2212 stochastic vs batch update \u2212 Transformation laws \n\u2212 shifting, scaling, and rotating the input \u2212 the non\u2212invariance of GD \u2212 The minimal multilayer \nnetwork 3 \u2212 Second order methods \u2212 Newton\u2019s algorithm, and why it does not work. \u2026",
      "citation_histogram": [
        [1995, 7],
        [1996, 1],
        [1997, 3],
        [1998, 3],
        [1999, 2],
        [2000, 2],
        [2001, 9],
        [2002, 3],
        [2003, 2],
        [2004, 2],
        [2005, 1],
        [2006, 6],
        [2007, 2],
        [2008, 10],
        [2009, 2],
        [2010, 3],
        [2011, 4],
        [2012, 4],
        [2013, 5],
        [2014, 4],
        [2015, 1],
        [2016, 2],
        [2017, 2],
        [2018, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:uLbwQdceFCQC",
      "title": "Comparing different neural network architectures for classifying handwritten digits",
      "link": "https://ieeexplore.ieee.org/abstract/document/118570/",
      "year": 1989,
      "cited_by": 84,
      "authors": [
        "I Guyon",
        "I Poujaud",
        "L Personnaz",
        "G Dreyfus",
        "J Denker",
        "Y LeCun"
      ],
      "description": "An evaluation is made of several neural network classifiers, comparing their performance on a typical problem, namely handwritten digit recognition. For this purpose, the authors use a database of handwritten digits, with relatively uniform handwriting styles. The authors propose a novel way of organizing the network architectures by training several small networks so as to deal separately with subsets of the problem, and then combining the results. This approach works in conjunction with various techniques including: layered networks with one or several layers of adaptive connections, fully connected recursive networks, ad hoc networks with no adaptive connections, and architectures with second-degree polynomial decision surfaces.< >",
      "citation_histogram": [
        [1989, 1],
        [1990, 4],
        [1991, 5],
        [1992, 8],
        [1993, 9],
        [1994, 11],
        [1995, 8],
        [1996, 2],
        [1997, 3],
        [1998, 8],
        [1999, 1],
        [2000, 3],
        [2001, 4],
        [2002, 2],
        [2003, 2],
        [2004, 2],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 2],
        [2011, 1],
        [2012, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:tCoNjB6AT50C",
      "title": "Deep learning. nature, 521 (7553), 436-444",
      "link": "https://scholar.google.com/scholar?cluster=8355582564199737689&hl=en&oi=scholarr",
      "year": 2015,
      "cited_by": 83,
      "authors": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton"],
      "description": null,
      "citation_histogram": [
        [2018, 2],
        [2019, 1],
        [2020, 11],
        [2021, 36],
        [2022, 32]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:ZHR7-34Bl2wC",
      "title": "The handbook of brain theory and neural networks. chapter Convolutional Networks for Images, Speech, and Time Series",
      "link": "https://scholar.google.com/scholar?cluster=3361897788053282617&hl=en&oi=scholarr",
      "year": 1998,
      "cited_by": 83,
      "authors": ["Yann LeCun", "Yoshua Bengio"],
      "description": null,
      "citation_histogram": [
        [2014, 1],
        [2015, 5],
        [2016, 4],
        [2017, 14],
        [2018, 17],
        [2019, 25],
        [2020, 11],
        [2021, 4],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:UmJFWc0aipQC",
      "title": "Predicting Future Instance Segmentations by Forecasting Convolutional Features",
      "link": "http://openaccess.thecvf.com/content_ECCV_2018/html/Pauline_Luc_Predicting_Future_Instance_ECCV_2018_paper.html",
      "year": 2018,
      "cited_by": 79,
      "authors": [
        "Pauline Luc",
        "Camille Couprie",
        "Yann Lecun",
        "Jakob Verbeek"
      ],
      "description": "Anticipating future events is an important prerequisite towards intelligent behavior. Video forecasting has been studied as a proxy task towards this goal. Recent work has shown that to predict semantic segmentation of future frames, forecasting at the semantic level is more effective than forecasting RGB frames and then segmenting these. In this paper we consider the more challenging problem of future instance segmentation, which additionally segments out individual objects. To deal with a varying number of output labels per image, we develop a predictive model in the space of fixed-sized convolutional features of the Mask R-CNN instance segmentation model. We apply the\" detection head\" of Mask R-CNN on the predicted features to produce the instance segmentation of future frames. Experiments show that this approach significantly improves over strong baselines based on optical flow and repurposed instance segmentation architectures.",
      "citation_histogram": [
        [2018, 4],
        [2019, 15],
        [2020, 16],
        [2021, 27],
        [2022, 17]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:wvYxNZNCP7wC",
      "title": "Discriminative Recurrent Sparse Auto-Encoders",
      "link": "https://arxiv.org/abs/1301.3775",
      "year": 2013,
      "cited_by": 79,
      "authors": ["Jason Tyler Rolfe", "Yann LeCun"],
      "description": "We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit all the power of deep networks, while substantially reducing the number of trainable parameters. From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.",
      "citation_histogram": [
        [2013, 4],
        [2014, 5],
        [2015, 11],
        [2016, 12],
        [2017, 16],
        [2018, 9],
        [2019, 9],
        [2020, 6],
        [2021, 2],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:P6jpBLdrFncC",
      "title": "A Mathematical Motivation for Complex-Valued Convolutional Networks",
      "link": "https://direct.mit.edu/neco/article-abstract/28/5/815/8157",
      "year": 2016,
      "cited_by": 78,
      "authors": [
        "Mark Tygert",
        "Joan Bruna",
        "Soumith Chintala",
        "Yann LeCun",
        "Serkan Piantino",
        "Arthur Szlam"
      ],
      "description": " A complex-valued convolutional network (convnet) implements the repeated application of the following composition of three operations, recursively applying the composition to an input vector of nonnegative real numbers: (1) convolution with complex-valued vectors, followed by (2) taking the absolute value of every entry of the resulting vectors, followed by (3) local averaging. For processing real-valued random vectors, complex-valued convnets can be viewed as data-driven multiscale windowed power spectra, data-driven multiscale windowed absolute spectra, data-driven multiwavelet absolute values, or (in their most general configuration) data-driven nonlinear multiwavelet packets. Indeed, complex-valued convnets can calculate multiscale windowed spectra when the convnet filters are windowed complex-valued exponentials. Standard real-valued convnets, using rectified linear units (ReLUs\u00a0\u2026",
      "citation_histogram": [
        [2016, 2],
        [2017, 7],
        [2018, 13],
        [2019, 17],
        [2020, 9],
        [2021, 19],
        [2022, 11]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:3s1wT3WcHBgC",
      "title": "An fpga-based stream processor for embedded real-time vision with convolutional networks",
      "link": "https://ieeexplore.ieee.org/abstract/document/5457611/",
      "year": 2009,
      "cited_by": 77,
      "authors": ["Cl\u00e9ment Farabet", "Cyril Poulet", "Yann LeCun"],
      "description": "Many recent visual recognition systems can be seen as being composed of multiple layers of convolutional filter banks, interspersed with various types of non-linearities. This includes Convolutional Networks, HMAX-type architectures, as well as systems based on dense SIFT features or Histogram of Gradients. This paper describes a highly-compact and low power embedded system that can run such vision systems at very high speed. A custom board built around a Xilinx Virtex-4 FPGA was built and tested. It measures 70 \u00d7 80 mm, and the complete system-FPGA, camera, memory chips, flash-consumes 15 watts in peak, and is capable of more than 4 \u00d7 10 9  multiply-accumulate operations per second in real vision application. This enables real-time implementations of object detection, object recognition, and vision-based navigation algorithms in small-size robots, micro-UAVs, and hand-held devices. Real-time\u00a0\u2026",
      "citation_histogram": [
        [2009, 1],
        [2010, 2],
        [2011, 8],
        [2012, 4],
        [2013, 6],
        [2014, 5],
        [2015, 7],
        [2016, 9],
        [2017, 11],
        [2018, 6],
        [2019, 8],
        [2020, 4],
        [2021, 6]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:DkZNVXde3BIC",
      "title": "Runtime reconfigurable dataflow processor with multi-port memory access module",
      "link": "https://patents.google.com/patent/US10078620B2/en",
      "year": 2018,
      "cited_by": 75,
      "authors": ["Cl\u00e9ment Farabet", "Yann LeCun"],
      "description": "A processor includes a plurality of processing tiles, wherein each tile is configured at runtime to perforin a configurable operation. A first subset of tiles are configured to perform in a pipeline a first plurality of configurable operations in parallel. A second subset of tiles are configured to perform a second plurality of configurable operations in parallel with the first plurality of configurable operations. The process also includes a multi-port memory access module operably connected to the plurality of tiles via a data bus configured to control access to a memory and to provide data to two or more processing tiles simultaneously. The processor also includes a controller operably connected to the plurality of tiles and the multi-port memory access module via a runtime bus. The processor configures the tiles and the multi-port memory access module to execute a computation.",
      "citation_histogram": [
        [2017, 1],
        [2018, 7],
        [2019, 24],
        [2020, 18],
        [2021, 16],
        [2022, 6]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:m5Mwo8ouzesC",
      "title": "The mind of a mouse",
      "link": "https://www.sciencedirect.com/science/article/pii/S0092867420310011",
      "year": 2020,
      "cited_by": 74,
      "authors": [
        "Larry F Abbott",
        "Davi D Bock",
        "Edward M Callaway",
        "Winfried Denk",
        "Catherine Dulac",
        "Adrienne L Fairhall",
        "Ila Fiete",
        "Kristen M Harris",
        "Moritz Helmstaedter",
        "Viren Jain",
        "Narayanan Kasthuri",
        "Yann LeCun",
        "Jeff W Lichtman",
        "Peter B Littlewood",
        "Liqun Luo",
        "John HR Maunsell",
        "R Clay Reid",
        "Bruce R Rosen",
        "Gerald M Rubin",
        "Terrence J Sejnowski",
        "H Sebastian Seung",
        "Karel Svoboda",
        "David W Tank",
        "Doris Tsao",
        "David C Van Essen"
      ],
      "description": "Large scientific projects in genomics and astronomy are influential not because they answer any single question but because they enable investigation of continuously arising new questions from the same data-rich sources. Advances in automated mapping of the brain\u2019s synaptic connections (connectomics) suggest that the complicated circuits underlying brain function are ripe for analysis. We discuss benefits of mapping a mouse brain at the level of synapses.",
      "citation_histogram": [
        [2020, 5],
        [2021, 35],
        [2022, 34]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:HoB7MX3m0LUC",
      "title": "Regularized estimation of image statistics by score matching",
      "link": "https://proceedings.neurips.cc/paper/2010/hash/6f3e29a35278d71c7f65495871231324-Abstract.html",
      "year": 2010,
      "cited_by": 72,
      "authors": ["Durk P Kingma", "Yann Cun"],
      "description": "Score Matching is a recently-proposed criterion for training high-dimensional density models for which maximum likelihood training is intractable. It has been applied to learning natural image statistics but has so-far been limited to simple models due to the difficulty of differentiating the loss with respect to the model parameters. We show how this differentiation can be automated with an extended version of the double-backpropagation algorithm. In addition, we introduce a regularization term for the Score Matching loss that enables its use for a broader range of problem by suppressing instabilities that occur with finite training sample sizes and quantized input values. Results are reported for image denoising and super-resolution.",
      "citation_histogram": [
        [2011, 5],
        [2012, 1],
        [2013, 4],
        [2014, 3],
        [2015, 2],
        [2016, 14],
        [2017, 5],
        [2018, 3],
        [2019, 10],
        [2020, 7],
        [2021, 12],
        [2022, 6]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:jFemdcug13IC",
      "title": "Comparison between frame-constrained fix-pixel-value and frame-free spiking-dynamic-pixel convnets for visual processing",
      "link": "https://www.frontiersin.org/articles/10.3389/fnins.2012.00032/full",
      "year": 2012,
      "cited_by": 71,
      "authors": [
        "Cl\u00e9ment Farabet",
        "Rafael Paz",
        "Jose P\u00e9rez-Carrasco",
        "Carlos Zamarre\u00f1o-Ramos",
        "Alejandro Linares-Barranco",
        "Yann LeCun",
        "Eugenio Culurciello",
        "Teresa Serrano-Gotarredona",
        "Bernabe Linares-Barranco"
      ],
      "description": "Most scene segmentation and categorization architectures for the extraction of features in images and patches make exhaustive use of 2D convolution operations for template matching, template search and denoising. Convolutional Neural Networks (ConvNets) are one example of such architectures that can implement general-purpose bio-inspired vision systems. In standard digital computers 2D convolutions are usually expensive in terms of resource consumption and impose severe limitations for efficient real-time applications. Nevertheless, neuro-cortex inspired solutions, like dedicated Frame-Based or Frame-Free Spiking ConvNet Convolution Processors, are advancing real-time visual processing. These two approaches share the neural inspiration, but each of them solves the problem in different ways. Frame-Based ConvNets process frame by frame video in- formation in a very robust and fast way that requires to use and share the available hardware resources (such as: multipliers, adders). Hardware resources are fixed and time multiplexed by fetching data in and out. Thus memory bandwidth and size is important for good performance. On the other hand, spike-based convolution processors are a frame-free alternative that is able to perform convolution of a spike-based source of visual information with very low latency, which makes ideal for very high speed applications. However, hardware resources need to be available all the time and cannot be time-multiplexed. Thus, hardware should be modular, reconfigurable and expansible. Hardware implementations in both VLSI custom integrated circuits (digital and analog) and FPGA have\u00a0\u2026",
      "citation_histogram": [
        [2012, 1],
        [2013, 4],
        [2014, 2],
        [2015, 4],
        [2016, 7],
        [2017, 2],
        [2018, 12],
        [2019, 11],
        [2020, 14],
        [2021, 10],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:kF1pexMAQbMC",
      "title": "Semantic road segmentation via multi-scale ensembles of learned features",
      "link": "https://link.springer.com/chapter/10.1007/978-3-642-33868-7_58",
      "year": 2012,
      "cited_by": 70,
      "authors": [
        "Jose M Alvarez",
        "Yann LeCun",
        "Theo Gevers",
        "Antonio M Lopez"
      ],
      "description": " Semantic segmentation refers to the process of assigning an object label (e.g., building, road, sidewalk, car, pedestrian) to every pixel in an image. Common approaches formulate the task as a random field labeling problem modeling the interactions between labels by combining local and contextual features such as color, depth, edges, SIFT or HoG. These models are trained to maximize the likelihood of the correct classification given a training set. However, these approaches rely on hand\u2013designed features (e.g., texture, SIFT or HoG) and a higher computational time required in the inference process. Therefore, in this paper, we focus on estimating the unary potentials of a conditional random field via ensembles of learned features. We propose an algorithm based on convolutional neural networks to learn local features from training data at different scales and resolutions. Then, diversification\u00a0\u2026",
      "citation_histogram": [
        [2013, 2],
        [2014, 7],
        [2015, 8],
        [2016, 7],
        [2017, 10],
        [2018, 6],
        [2019, 11],
        [2020, 5],
        [2021, 8],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:NJ774b8OgUMC",
      "title": "Constrained neural network for unconstrained handwritten digit recognition",
      "link": "https://ci.nii.ac.jp/naid/10008948210/",
      "year": 1990,
      "cited_by": 69,
      "authors": ["Yann LE CUN"],
      "description": "CiNii \u8ad6\u6587 - Constrained neural network for unconstrained handwritten digit recognition CiNii \n\u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \n\u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \n\u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \n\u691c\u7d22 \u691c\u7d22 \u691c\u7d22 [7/12\u66f4\u65b0]2022\u5e744\u67081\u65e5\u304b\u3089\u306eCiNii Articles\u306eCiNii Research\u3078\u306e\u7d71\u5408\u306b\u3064\u3044\u3066 \nConstrained neural network for unconstrained handwritten digit recognition LE CUN Y. \u88ab\n\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 LE CUN Y. \u53ce\u9332\u520a\u884c\u7269 First International Workshop Frontiers Handwriting \nRecognition, CEMPARMI First International Workshop Frontiers Handwriting Recognition, \nCEMPARMI, 1990 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Segmentation Free Shared Weight \nNetworks for Automatic Vehicle Detection GADER Paul D. , MIRAMONTI Joseph R. , WON \u2026",
      "citation_histogram": [
        [1990, 1],
        [1991, 1],
        [1992, 6],
        [1993, 6],
        [1994, 5],
        [1995, 8],
        [1996, 7],
        [1997, 5],
        [1998, 4],
        [1999, 5],
        [2000, 2],
        [2001, 1],
        [2002, 4],
        [2003, 1],
        [2004, 1],
        [2005, 2],
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 1],
        [2012, 2],
        [2013, 1],
        [2014, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:RiW20FJDrgsC",
      "title": "Design: Design inspiration from generative networks",
      "link": "https://openaccess.thecvf.com/content_eccv_2018_workshops/w14/html/Sbai_DesIGN_Design_Inspiration_from_Generative_Networks_ECCVW_2018_paper.html",
      "year": 2018,
      "cited_by": 68,
      "authors": [
        "Othman Sbai",
        "Mohamed Elhoseiny",
        "Antoine Bordes",
        "Yann LeCun",
        "Camille Couprie"
      ],
      "description": "Can an algorithm create original and compelling fashion designs to serve as an inspirational assistant? To help answer this question, we design and investigate different image generation models associated with different loss functions to boost novelty in fashion generation. The dimensions of our explorations include:(i) different Generative Adversarial Networks architectures that start from noise vectors to generate fashion items,(ii) a new loss function that encourages novelty, and (iii) a generation process following the key elements of fashion design (disentangling shape and texture). A key challenge of this study is the evaluation of generated designs and the retrieval of best ones, hence we put together an evaluation protocol associating automatic metrics and human experimental studies. We show that our proposed creativity loss yields better overall appreciation than the one employed in Creative Adversarial Networks. In the end, about 61% of our images are thought to be created by human designers rather than by a computer while also being considered original per our human subject experiments, and our proposed loss scores the highest compared to existing losses in both novelty and likability.",
      "citation_histogram": [
        [2018, 3],
        [2019, 13],
        [2020, 19],
        [2021, 18],
        [2022, 12]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:tKAzc9rXhukC",
      "title": "Reading handwritten digits: A ZIP code recognition system",
      "link": "https://ieeexplore.ieee.org/abstract/document/144441/",
      "year": 1992,
      "cited_by": 68,
      "authors": [
        "Ofer Matan",
        "Henry S. Baird",
        "Jane Bromley",
        "Christopher J. C. Burges",
        "John S. Denker",
        "Lawrence D. Jackel",
        "Yann LeCun",
        "Edwin P. D. Pednault",
        "William D Satterfield",
        "Charles E. Stenard",
        "Timothy J. Thompson"
      ],
      "description": "A neural network algorithm-based system that reads handwritten ZIP codes appearing on real US mail is described. The system uses a recognition-based segmenter, that is a hybrid of connected-components analysis (CCA), vertical cuts, and a neural network recognizer. Connected components that are single digits are handled by CCA. CCs that are combined or dissected digits are handled by the vertical-cut segmenter. The four main stages of processing are preprocessing, in which noise is removed and the digits are deslanted, CCA segmentation and recognition, vertical-cut-point estimation and segmentation, and directly lookup. The system was trained and tested on approximately 10000 images, five- and nine-digit ZIP code fields taken from real mail.< >",
      "citation_histogram": [
        [1992, 3],
        [1993, 5],
        [1994, 2],
        [1995, 2],
        [1996, 4],
        [1997, 2],
        [1998, 4],
        [1999, 3],
        [2000, 2],
        [2001, 1],
        [2002, 1],
        [2003, 2],
        [2004, 2],
        [2005, 1],
        [2006, 1],
        [2007, 3],
        [2008, 3],
        [2009, 2],
        [2010, 4],
        [2011, 3],
        [2012, 8],
        [2013, 2],
        [2014, 2],
        [2015, 3],
        [2016, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:4IpgxnMJogoC",
      "title": "Connectionism in perspective",
      "link": "https://scholar.google.com/scholar?cluster=4720535213137093745&hl=en&oi=scholarr",
      "year": 1989,
      "cited_by": 67,
      "authors": [
        "Y LeCun",
        "R Pfeifer",
        "Z Schreter",
        "F Fogelman",
        "L Steels"
      ],
      "description": null,
      "citation_histogram": [
        [1990, 25],
        [1991, 3],
        [1992, 1],
        [1993, 2],
        [1994, 1],
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 1],
        [1999, 1],
        [2000, 6],
        [2001, 1],
        [2002, 5],
        [2003, 1],
        [2004, 2],
        [2005, 3],
        [2006, 5],
        [2007, 2],
        [2008, 3],
        [2009, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:rEjdzzKT7SMC",
      "title": "Quand la machine apprend: la r\u00e9volution des neurones artificiels et de l'apprentissage profond",
      "link": "https://scholar.google.com/scholar?cluster=14770218080077525486&hl=en&oi=scholarr",
      "year": 2019,
      "cited_by": 66,
      "authors": ["Yann Le Cun"],
      "description": "Nous vivons une r\u00e9volution inou\u00efe, inimaginable il y a encore cinquante ans, celle de la machine qui apprend, et qui apprend par elle-m\u00eame. Au lieu d\u2019ex\u00e9cuter les ordres d\u2019un programme, la machine peut d\u00e9sormais acqu\u00e9rir par elle-m\u00eame, par l\u2019exp\u00e9rience, les capacit\u00e9s n\u00e9cessaires pour accomplir les t\u00e2ches qui lui sont assign\u00e9es, y compris celles que l\u2019on croyait r\u00e9serv\u00e9es \u00e0 l\u2019humain. Les applications sont immenses: reconnaissance des formes, des voix, des images et des visages, voiture autonome, traduction de centaines de langues, d\u00e9tection des tumeurs dans les images m\u00e9dicales... Yann Le Cun est \u00e0 l\u2019origine de cette r\u00e9volution. Il est en effet l\u2019un des inventeurs de l\u2019apprentissage profond, le deep learning, qui caract\u00e9rise un r\u00e9seau de neurones artificiels dont l\u2019architecture et le fonctionnement s\u2019 inspirent du cerveau. C\u2019est \u00e0 la naissance de cette nouvelle forme d\u2019intelligence, \u00e0 l\u2019\u00e9mergence d\u2019un syst\u00e8me quasiment auto-organisateur, que nous convie Yann Le Cun. Un livre qui \u00e9voque la d\u00e9marche intellectuelle d\u2019un inventeur au carrefour de l\u2019informatique et des neurosciences. Un livre qui \u00e9claire l\u2019avenir de l\u2019intelligence artificielle, ses enjeux, ses promesses et ses risques. Un livre passionnant, clair et accessible, qui nous fait p\u00e9n\u00e9trer au c\u0153ur de la machine et nous fait d\u00e9couvrir un nouveau monde fascinant, qui est d\u00e9j\u00e0 le n\u00f4tre. Yann Le Cun, laur\u00e9at du prix Turing, est professeur \u00e0 New York University et dirige la recherche fondamentale chez Facebook.",
      "citation_histogram": [
        [2019, 2],
        [2020, 15],
        [2021, 32],
        [2022, 17]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Ncwx4PHgTB8C",
      "title": "System and method for biometric authentication in connection with camera equipped devices",
      "link": "https://patents.google.com/patent/US20140068740A1/en",
      "year": 2014,
      "cited_by": 64,
      "authors": ["Yann LeCun", "Adam Perold", "Yang Wang", "Sagar Waghmare"],
      "description": "The present invention relates generally to the use of biometric technology for authentication and identification, and more particularly to non-contact based solutions for authenticating and identifying users, via computers, such as mobile devices, to selectively permit or deny access to various resources. In the present invention authentication and/or identification is performed using an image or a set of images of an individual's palm through a process involving the following key steps:(1) detecting the palm area using local classifiers;(2) extracting features from the region (s) of interest; and (3) computing the matching score against user models stored in a database, which can be augmented dynamically through a learning process.",
      "citation_histogram": [
        [2014, 1],
        [2015, 2],
        [2016, 2],
        [2017, 7],
        [2018, 16],
        [2019, 14],
        [2020, 12],
        [2021, 10]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:t7zJ5fGR-2UC",
      "title": "Double backpropagation increasing generalization performance",
      "link": "https://ieeexplore.ieee.org/abstract/document/155328/",
      "year": 1991,
      "cited_by": 63,
      "authors": ["Harris Drucker", "Yann Le Cun"],
      "description": "One test of a training algorithm is how well the algorithm generalizes from the training data to the test data. It is shown that a training algorithm termed double back-propagation improves generalization by simultaneously minimizing the normal energy term found in back-propagation and an additional energy term that is related to the sum of the squares of the input derivatives (gradients). In normal back-propagation training, minimizing the energy function tends to push the input gradient to zero. However, this is not always possible. Double back-propagation explicitly pushes the input gradients to zero, making the minimum broader, and increases the generalization on the test data. The authors show the improvement over normal back-propagation on four candidate architectures with a training set of 320 handwritten numbers and a test set of size 180.< >",
      "citation_histogram": [
        [1992, 4],
        [1993, 1],
        [1994, 1],
        [1995, 1],
        [1996, 3],
        [1997, 1],
        [1998, 3],
        [1999, 8],
        [2000, 14],
        [2001, 9],
        [2002, 9],
        [2003, 9]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:aGxtzYPGyQEC",
      "title": "Generalization and network design strategies. Connectionism in perspective",
      "link": "https://scholar.google.com/scholar?cluster=8655185990866104096&hl=en&oi=scholarr",
      "year": 1989,
      "cited_by": 63,
      "authors": ["Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [1989, 1],
        [1990, 1],
        [1991, 2],
        [1992, 2],
        [1993, 3],
        [1994, 2],
        [1995, 2],
        [1996, 5],
        [1997, 1],
        [1998, 2],
        [1999, 1],
        [2000, 1],
        [2001, 2],
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 2],
        [2006, 1],
        [2007, 6],
        [2008, 1],
        [2009, 3],
        [2010, 5],
        [2011, 8],
        [2012, 4],
        [2013, 5]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:YOwf2qJgpHMC",
      "title": "Time delay neural network for printed and cursive handwritten character recognition",
      "link": "https://patents.google.com/patent/US5105468A/en",
      "year": 1992,
      "cited_by": 62,
      "authors": ["Isabelle Guyon", "John S Denker", "Yann LeCun"],
      "description": "A time delay neural network is defined having feature detection layers which are constrained for extracting features and subsampling a sequence of feature vectors input to the particular feature detection layer. Output from the network for both digit and uppercase letters is provided by an output classification layer which is fully connected to the final feature detection layer. Each feature vector relates to coordinate information about the original character preserved in a temporal order together with additional information related to the original character at the particular coordinate point. Such additional information may include local geometric information, local pen information, and phantom stroke coordinate information relating to connecting segments between the end point of one stroke and the beginning point of another stroke.",
      "citation_histogram": [
        [1994, 6],
        [1995, 5],
        [1996, 10],
        [1997, 6],
        [1998, 11],
        [1999, 2],
        [2000, 7],
        [2001, 2],
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 2],
        [2006, 2],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:sCWLdL-sCz8C",
      "title": "Explorations on high dimensional landscapes",
      "link": "https://arxiv.org/abs/1412.6615",
      "year": 2014,
      "cited_by": 61,
      "authors": [
        "Levent Sagun",
        "V Ugur Guney",
        "Gerard Ben Arous",
        "Yann LeCun"
      ],
      "description": "Finding minima of a real valued non-convex function over a high dimensional space is a major challenge in science. We provide evidence that some such functions that are defined on high dimensional domains have a narrow band of values whose pre-image contains the bulk of its critical points. This is in contrast with the low dimensional picture in which this band is wide. Our simulations agree with the previous theoretical work on spin glasses that proves the existence of such a band when the dimension of the domain tends to infinity. Furthermore our experiments on teacher-student networks with the MNIST dataset establish a similar phenomenon in deep networks. We finally observe that both the gradient descent and the stochastic gradient descent methods can reach this level within the same number of steps.",
      "citation_histogram": [
        [2015, 5],
        [2016, 8],
        [2017, 8],
        [2018, 11],
        [2019, 6],
        [2020, 9],
        [2021, 9],
        [2022, 5]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:ldfaerwXgEUC",
      "title": "Dynamic factor graphs for time series modeling",
      "link": "https://link.springer.com/chapter/10.1007/978-3-642-04174-7_9",
      "year": 2009,
      "cited_by": 61,
      "authors": ["Piotr Mirowski", "Yann LeCun"],
      "description": " This article presents a method for training Dynamic Factor Graphs (DFG) with continuous latent state variables. A DFG includes factors modeling joint probabilities between hidden and observed variables, and factors modeling dynamical constraints on hidden variables. The DFG assigns a scalar energy to each configuration of hidden and observed variables. A gradient-based inference procedure finds the minimum-energy state sequence for a given observation sequence. Because the factors are designed to ensure a constant partition function, they can be trained by minimizing the expected energy over training sequences with respect to the factors\u2019 parameters. These alternated inference and parameter updates can be seen as a deterministic EM-like procedure. Using smoothing regularizers, DFGs are shown to reconstruct chaotic attractors and to separate a mixture of independent oscillatory sources\u00a0\u2026",
      "citation_histogram": [
        [2009, 1],
        [2010, 3],
        [2011, 3],
        [2012, 3],
        [2013, 1],
        [2014, 6],
        [2015, 5],
        [2016, 4],
        [2017, 4],
        [2018, 7],
        [2019, 6],
        [2020, 8],
        [2021, 8],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:KOc9rAu6-V4C",
      "title": "Haffner,\u201cGradient-based learning applied to document recognition,\u201d",
      "link": "https://scholar.google.com/scholar?cluster=3597552345296283904&hl=en&oi=scholarr",
      "year": 1998,
      "cited_by": 61,
      "authors": ["Y LeCun", "L Bottou", "Y Bengio"],
      "description": null,
      "citation_histogram": [
        [2014, 1],
        [2015, 2],
        [2016, 4],
        [2017, 1],
        [2018, 8],
        [2019, 43]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:kzcrU_BdoSEC",
      "title": "Word normalization for online handwritten word recognition",
      "link": "https://ieeexplore.ieee.org/abstract/document/576966/",
      "year": 1994,
      "cited_by": 61,
      "authors": ["Yoshua Bengio", "Yann Le Cun"],
      "description": "We introduce a new approach to normalizing words written with an electronic stylus that applies to all styles of handwriting (upper case, lower case, printed, cursive, or mixed). A geometrical model of the word spatial structure is fitted to the pen trajectory using the expectation-maximisation algorithm. The fitting process maximizes the likelihood of the trajectory given the model and a set a priors on its parameters. The method was evaluated and integrated to a recognition system that combines neural networks and hidden Markov models.",
      "citation_histogram": [
        [1994, 2],
        [1995, 1],
        [1996, 2],
        [1997, 5],
        [1998, 5],
        [1999, 2],
        [2000, 3],
        [2001, 4],
        [2002, 4],
        [2003, 5],
        [2004, 4],
        [2005, 5],
        [2006, 1],
        [2007, 1],
        [2008, 4],
        [2009, 4],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 1],
        [2014, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:9u2w3wkYHSMC",
      "title": "Automata networks and artificial intelligence",
      "link": "https://dl.acm.org/doi/abs/10.5555/42727.42735",
      "year": 1987,
      "cited_by": 61,
      "authors": [
        "Francois Fogleman Souli\u00e9",
        "Patrick Gallinari",
        "Yann Le Cun",
        "Sylvie Thiria"
      ],
      "description": "Automata networks and artificial intelligence | Centre National de Recherche Scientifique on \nAutomata networks in computer science: theory and applications ACM Digital Library home \nACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced \nSearch Journals Magazines Proceedings Books SIGs Conferences People More Search ACM \nDigital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More \nHomeBrowse by TitleProceedingsCentre National de Recherche Scientifique on Automata \nnetworks in computer science: theory and applicationsAutomata networks and artificial \nintelligence Article Share on Automata networks and artificial intelligence Authors: Francois \nFogleman Souli\u00e9 View Profile , Patrick Gallinari View Profile , Yann Le Cun View Profile , \nSylvie Thiria View Profile Authors Info & Claims Centre National de Recherche Scientifique on \u2026",
      "citation_histogram": [
        [1987, 2],
        [1988, 4],
        [1989, 4],
        [1990, 8],
        [1991, 4],
        [1992, 5],
        [1993, 2],
        [1994, 3],
        [1995, 1],
        [1996, 1],
        [1997, 2],
        [1998, 2],
        [1999, 1],
        [2000, 2],
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 2],
        [2005, 2],
        [2006, 2],
        [2007, 2],
        [2008, 2],
        [2009, 5]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:0VGYH9MJNTkC",
      "title": "PhD thesis: Modeles connexionnistes de l'apprentissage (connectionist learning models)",
      "link": "https://nyuscholars.nyu.edu/en/publications/phd-thesis-modeles-connexionnistes-de-lapprentissage-connectionis",
      "year": 1987,
      "cited_by": 61,
      "authors": ["Yann LeCun"],
      "description": "PhD thesis: Modeles connexionnistes de l'apprentissage (connectionist learning models) \u2014 \nNYU Scholars Skip to main navigation Skip to search Skip to main content NYU Scholars \nHome NYU Scholars Logo Help & FAQ Home Profiles Research Units Research output \nSearch by expertise, name or affiliation PhD thesis: Modeles connexionnistes de l'apprentissage \n(connectionist learning models) Yann Lecun Computer Science Research output: Book/Report \n\u203a Other report Overview Original language English (US) Publisher Universite P. et M. Curie (Paris \n6) State Published - Jun 1987 Cite this APA Standard Harvard Vancouver Author BIBTEX \nRIS Lecun, Y. (1987). PhD thesis: Modeles connexionnistes de l'apprentissage (connectionist \nlearning models). Universite P. et M. Curie (Paris 6). PhD thesis : Modeles connexionnistes \nde l'apprentissage (connectionist learning models). / Lecun, Yann. Universite P. et M. \u2026",
      "citation_histogram": [
        [1989, 1],
        [1990, 1],
        [1991, 1],
        [1992, 1],
        [1993, 1],
        [1994, 2],
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 1],
        [1999, 2],
        [2000, 4],
        [2001, 6],
        [2002, 3],
        [2003, 8],
        [2004, 5],
        [2005, 5],
        [2006, 12],
        [2007, 5]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:HDshCWvjkbEC",
      "title": "Online learning for offroad robots: Using spatial label propagation to learn long-range traversability",
      "link": "http://www.roboticsproceedings.org/rss03/p03.pdf",
      "year": 2007,
      "cited_by": 60,
      "authors": [
        "Raia Hadsell",
        "Pierre Sermanet",
        "Jan Ben",
        "A Erkan",
        "Jeff Han",
        "Beat Flepp",
        "Urs Muller",
        "Yann LeCun"
      ],
      "description": "We present a solution to the problem of long-range obstacle/path recognition in autonomous robots. The system uses sparse traversability information from a stereo module to train a classifier online. The trained classifier can then predict the traversability of the entire scene. A distance-normalized image pyramid makes it possible to efficiently train on each frame seen by the robot, using large windows that contain contextual information as well as shape, color, and texture. Traversability labels are initially obtained for each target using a stereo module, then propagated to other views of the same target using temporal and spatial concurrences, thus training the classifier to be viewinvariant. A ring buffer simulates short-term memory and ensures that the discriminative learning is balanced and consistent. This long-range obstacle detection system sees obstacles and paths at 30-40 meters, far beyond the maximum stereo range of 12 meters, and adapts very quickly to new environments. Experiments were run on the LAGR robot platform.",
      "citation_histogram": [
        [2007, 1],
        [2008, 7],
        [2009, 7],
        [2010, 7],
        [2011, 3],
        [2012, 4],
        [2013, 3],
        [2014, 4],
        [2015, 1],
        [2016, 5],
        [2017, 4],
        [2018, 7],
        [2019, 3],
        [2020, 1],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=100&pagesize=100&citation_for_view=WLN3QrAAAAAJ:LPZeul_q3PIC",
      "title": "Convolutional matching pursuit and dictionary training",
      "link": "https://arxiv.org/pdf/1010.0422",
      "year": 2010,
      "cited_by": 57,
      "authors": ["Arthur Szlam", "Koray Kavukcuoglu", "Yann LeCun"],
      "description": "One of the most succesful recent signal processing paradigms has been the sparse coding/dictionary design model [8, 4]. In this model, we try to represent a given d\u00d7 n data matrix X of n points in Rd written as columns via a solution to the problem",
      "citation_histogram": [
        [2010, 1],
        [2011, 2],
        [2012, 4],
        [2013, 4],
        [2014, 8],
        [2015, 9],
        [2016, 9],
        [2017, 11],
        [2018, 2],
        [2019, 5],
        [2020, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:_9cyEV96HHsC",
      "title": "Which encoding is the best for text classification in Chinese, English, Japanese and Korean?",
      "link": "https://arxiv.org/abs/1708.02657",
      "year": 2017,
      "cited_by": 56,
      "authors": ["Xiang Zhang", "Yann LeCun"],
      "description": "This article offers an empirical study on the different ways of encoding Chinese, Japanese, Korean (CJK) and English languages for text classification. Different encoding levels are studied, including UTF-8 bytes, characters, words, romanized characters and romanized words. For all encoding levels, whenever applicable, we provide comparisons with linear models, fastText and convolutional networks. For convolutional networks, we compare between encoding mechanisms using character glyph images, one-hot (or one-of-n) encoding, and embedding. In total there are 473 models, using 14 large-scale text classification datasets in 4 languages including Chinese, English, Japanese and Korean. Some conclusions from these results include that byte-level one-hot encoding based on UTF-8 consistently produces competitive results for convolutional networks, that word-level n-grams linear models are competitive even without perfect word segmentation, and that fastText provides the best result using character-level n-gram encoding but can overfit when the features are overly rich.",
      "citation_histogram": [
        [2017, 1],
        [2018, 7],
        [2019, 18],
        [2020, 11],
        [2021, 10],
        [2022, 7]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:oufuJXaDW1QC",
      "title": "E\ufb03cient estimation of word representations in vector space",
      "link": "https://scholar.google.com/scholar?cluster=12380824889101034525&hl=en&oi=scholarr",
      "year": 2013,
      "cited_by": 56,
      "authors": [
        "Tomas Mikolov",
        "Kai Chen",
        "Greg Corrado",
        "Je\ufb00rey Dean",
        "Y Bengio",
        "Y LeCun"
      ],
      "description": null,
      "citation_histogram": [
        [2013, 1],
        [2014, 1],
        [2015, 3],
        [2016, 4],
        [2017, 5],
        [2018, 10],
        [2019, 13],
        [2020, 9],
        [2021, 10]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:_Ybze24A_UAC",
      "title": "Shortest path segmentation: A method for training a neural network to recognize character strings",
      "link": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/sps.pdf",
      "year": 1992,
      "cited_by": 56,
      "authors": [
        "CJC Burges",
        "O Matan",
        "Y LeCun",
        "JS Denker",
        "LD Jackel",
        "CE Stenard",
        "CR Nohl",
        "JI Ben"
      ],
      "description": "We describe a method which combines dynamic programming and a neural net recognizer for segmenting and recognizing character strings. The method selects the optimal consistent combination of cuts from a set of candidate cuts generated using heuristics. The optimal segmentation is found by representing the image, the candidate segments, and their scores, as a graph in which the shortest path corresponds to the optimal interpretation. The scores are given by the neural net outputs for each segment. A significant advantage of the method is that the labor required to manually segment images is eliminated: the dynamic programming stage both performs the segmentation and provides inputs and desired outputs to the neural network during training. The training examples thus generated contain examples of both characters and\" non-characters\".The system was trained on approximately 7000 unsegmented\u00a0\u2026",
      "citation_histogram": [
        [1992, 2],
        [1993, 8],
        [1994, 6],
        [1995, 4],
        [1996, 3],
        [1997, 2],
        [1998, 1],
        [1999, 3],
        [2000, 2],
        [2001, 1],
        [2002, 2],
        [2003, 6],
        [2004, 3],
        [2005, 3],
        [2006, 1],
        [2007, 2],
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 2],
        [2012, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Q7hiZQKJ-pAC",
      "title": "Glomo: Unsupervised learning of transferable relational graphs",
      "link": "https://proceedings.neurips.cc/paper/2018/hash/5dbc8390f17e019d300d5a162c3ce3bc-Abstract.html",
      "year": 2018,
      "cited_by": 55,
      "authors": [
        "Zhilin Yang",
        "Jake Zhao",
        "Bhuwan Dhingra",
        "Kaiming He",
        "William W Cohen",
        "Russ R Salakhutdinov",
        "Yann LeCun"
      ],
      "description": "Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (eg, words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden units), or embedding-free units such as image pixels.",
      "citation_histogram": [
        [2018, 2],
        [2019, 16],
        [2020, 15],
        [2021, 14],
        [2022, 6]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:9xDRhSErrBIC",
      "title": "Adversarially regularized autoencoders for generating discrete structures",
      "link": "https://scholar.google.com/scholar?cluster=13779294102647658049&hl=en&oi=scholarr",
      "year": 2017,
      "cited_by": 55,
      "authors": ["Yoon Kim", "Kelly Zhang", "Alexander M Rush", "Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [2017, 5],
        [2018, 16],
        [2019, 21],
        [2020, 7],
        [2021, 2],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:DrR-2ekChdkC",
      "title": "Saturating Auto-Encoder",
      "link": "https://arxiv.org/abs/1301.3577",
      "year": 2013,
      "cited_by": 55,
      "authors": ["Rostislav Goroshin", "Yann LeCun"],
      "description": "We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE's ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.",
      "citation_histogram": [
        [2013, 5],
        [2014, 5],
        [2015, 6],
        [2016, 14],
        [2017, 4],
        [2018, 3],
        [2019, 6],
        [2020, 4],
        [2021, 5],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:J-pR_7NvFogC",
      "title": "Hardware requirements for neural network pattern classifiers: a case study and implementation",
      "link": "https://ieeexplore.ieee.org/abstract/document/124378/",
      "year": 1992,
      "cited_by": 55,
      "authors": [
        "BE Boser",
        "E Sackinger",
        "J Bromley",
        "Y LeCun",
        "RE Howard",
        "LD Jackel"
      ],
      "description": "A special-purpose chip, optimized for computational needs of neural networks and performing over 2000 multiplications and additions simultaneously, is described. Its data path is particularly suitable for the convolutional architectures typical in pattern classification networks but can also be configured for fully connected or feedback topologies. A development system permits rapid prototyping of new applications and analysis of the impact of the specialized hardware on system performance. The power and flexibility of the processor are demonstrated with a neural network for handwritten character recognition containing over 133000 connections.< >",
      "citation_histogram": [
        [1992, 1],
        [1993, 7],
        [1994, 6],
        [1995, 4],
        [1996, 1],
        [1997, 5],
        [1998, 3],
        [1999, 1],
        [2000, 1],
        [2001, 3],
        [2002, 1],
        [2003, 2],
        [2004, 4],
        [2005, 1],
        [2006, 1],
        [2007, 4],
        [2008, 2],
        [2009, 1],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:cTLm0pkq19gC",
      "title": "Singularity of the hessian in deep learning",
      "link": "https://scholar.google.com/scholar?cluster=14502096827037385776&hl=en&oi=scholarr",
      "year": 2016,
      "cited_by": 54,
      "authors": ["Levent Sagun", "L\u00e9on Bottou", "Yann LeCun"],
      "description": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how overparametrized the system is, and for the edges indicating the complexity of the input data.",
      "citation_histogram": [
        [2016, 1],
        [2017, 9],
        [2018, 11],
        [2019, 12],
        [2020, 11],
        [2021, 7],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:dnWPDgH667kC",
      "title": "Deep learning & convolutional networks.",
      "link": "https://pdfs.semanticscholar.org/3154/d217c6fca87aedc99f47bdd6ed9b2be47c0c.pdf",
      "year": 2015,
      "cited_by": 54,
      "authors": ["Yann LeCun"],
      "description": "Deep Learning & Convolutional Networks Page 1 Y LeCun Deep Learning & Convolutional \nNetworks Yann LeCun Facebook AI Research & Center for Data Science, NYU yann@cs.nyu.edu \nhttp://yann.lecun.com Page 2 Y LeCun Machine Learning (Supervised Lerning) training a \nmachine to distinguish cars from airplanes. We show it lots of examples of cars and airplanes \nEach time, we adjust the \u201cknobs\u201d so it produces a good answer PLANE CAR Page 3 Y LeCun \nMachine Learning training a machine to distinguish cars from airplanes. We show it lots of \nexamples of cars and airplanes Each time, we adjust the \u201cknobs\u201d so it produces a good \nanswer PLANE CAR Page 4 Y LeCun Large-Scale Machine Learning: the reality Hundreds \nof millions of knobs Thousands of categories Millions of training samples Recognizing each \nsample may take billions of operations But these operations are simple multiplications and \u2026",
      "citation_histogram": [
        [2016, 5],
        [2017, 6],
        [2018, 3],
        [2019, 4],
        [2020, 9],
        [2021, 15],
        [2022, 10]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:TFP_iSt0sucC",
      "title": "A multirange architecture for collision\u2010free off\u2010road robot navigation",
      "link": "https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20270",
      "year": 2009,
      "cited_by": 54,
      "authors": [
        "Pierre Sermanet",
        "Raia Hadsell",
        "Marco Scoffier",
        "Matt Grimes",
        "Jan Ben",
        "Ayse Erkan",
        "Chris Crudele",
        "Urs Miller",
        "Yann LeCun"
      ],
      "description": " We present a multilayered mapping, planning, and command execution system developed and tested on the LAGR mobile robot. Key to robust performance under uncertainty is the combination of a short\u2010range perception system operating at high frame rate and low resolution and a long\u2010range, adaptive vision system operating at lower frame rate and higher resolution. The short\u2010range module performs local planning and obstacle avoidance with fast reaction times, whereas the long\u2010range module performs strategic visual planning. Probabilistic traversability labels provided by the perception modules are combined and accumulated into a robot\u2010centered hyperbolic\u2010polar map with a 200\u2010m effective range. Instead of using a dynamical model of the robot for short\u2010range planning, the system uses a large lookup table of physically possible trajectory segments recorded on the robot in a wide variety of driving\u00a0\u2026",
      "citation_histogram": [
        [2009, 3],
        [2010, 10],
        [2011, 4],
        [2012, 3],
        [2013, 3],
        [2014, 3],
        [2015, 6],
        [2016, 2],
        [2017, 5],
        [2018, 3],
        [2019, 4],
        [2020, 3],
        [2021, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:bEWYMUwI8FkC",
      "title": "Energy-based models in document recognition and computer vision",
      "link": "https://ieeexplore.ieee.org/abstract/document/4378728/",
      "year": 2007,
      "cited_by": 54,
      "authors": ["Yann LeCun", "Sumit Chopra", "M Ranzato", "F-J Huang"],
      "description": "The machine learning and pattern recognition communities are facing two challenges: solving the normalization problem, and solving the deep learning problem. The normalization problem is related to the difficulty of training probabilistic models over large spaces while keeping them properly normalized. In recent years, the ML and natural language communities have devoted considerable efforts to circumventing this problem by developing \"un-normalized\" learning models for tasks in which the output is highly structured (e.g. English sentences). This class of models was in fact originally developed during the 90's in the handwriting recognition community, and includes graph transformer networks, conditional random fields, hidden Markov SVMs, and maximum margin Markov networks. We describe these models within the unifying framework of \"energy-based models\" (EBM). The deep learning problem is related\u00a0\u2026",
      "citation_histogram": [
        [2008, 5],
        [2009, 3],
        [2010, 4],
        [2011, 2],
        [2012, 4],
        [2013, 2],
        [2014, 5],
        [2015, 6],
        [2016, 1],
        [2017, 5],
        [2018, 3],
        [2019, 3],
        [2020, 3],
        [2021, 5],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:3r0_5JwaG5wC",
      "title": "Understanding dimensional collapse in contrastive self-supervised learning",
      "link": "https://arxiv.org/abs/2110.09348",
      "year": 2021,
      "cited_by": 53,
      "authors": ["Li Jing", "Pascal Vincent", "Yann LeCun", "Yuandong Tian"],
      "description": "Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. Joint embedding approach bases on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. It has been shown that non-contrastive methods suffer from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. Here, we show that dimensional collapse also happens in contrastive learning. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on a trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.",
      "citation_histogram": [
        [2021, 2],
        [2022, 51]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:4DMP91E08xMC",
      "title": "Off line recognition of handwritten postal words using neural networks",
      "link": "https://www.worldscientific.com/doi/abs/10.1142/9789812797926_0004",
      "year": 1993,
      "cited_by": 52,
      "authors": [
        "Christopher J. C.  Burges",
        "JI Ben",
        "John S.  Denker",
        "Yann LeCun",
        "Craig R.  Nohl"
      ],
      "description": " We describe a method, \u201cShortest Path Segmentation\u201d (SPS), which combines dynamic programming and a neural net recognizer for segmenting and recognizing character strings. We describe the application of this method to two problems: recognition of handwritten ZIP Codes, and recognition of handwritten words. For the ZIP Codes, we also used the method to automatically segment the images during training: the dynamic programming stage both performs the segmentation and provides inputs and desired outputs to the neural network. Results are reported for a test set of 2642 unsegmented handwritten 212 dpi binary ZIP Code (5- and 9-digit) images. For handwritten word recognition, we combined SPS with a \u201cSpace Displacement Neural Network\u201d approach, in which a single-character-recognition network is extended over the entire word image, and in which SPS techniques are then used to rank order a\u00a0\u2026",
      "citation_histogram": [
        [1992, 1],
        [1993, 2],
        [1994, 6],
        [1995, 6],
        [1996, 3],
        [1997, 5],
        [1998, 2],
        [1999, 3],
        [2000, 3],
        [2001, 2],
        [2002, 1],
        [2003, 2],
        [2004, 2],
        [2005, 3],
        [2006, 1],
        [2007, 1],
        [2008, 2],
        [2009, 1],
        [2010, 3],
        [2011, 1],
        [2012, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:XD-gHx7UXLsC",
      "title": "Handwritten character recognition using neural network architectures",
      "link": "https://www.academia.edu/download/48116451/Handwritten_Character_Recognition_Using_20160817-24888-309i8l.pdf",
      "year": 1990,
      "cited_by": 52,
      "authors": [
        "Ofer Matan",
        "RK Kiang",
        "CE Stenard",
        "B Boser",
        "JS Denker",
        "Don Henderson",
        "RE Howard",
        "W Hubbard",
        "LD Jackel",
        "Yann Le Cun"
      ],
      "description": "We have developed a neural-network architecture for recognizing handwritten digits. This network has 1% error rate with about 7% reject rate on handwritten zipcode digits provided by the US Postal Service. In this paper, we discuss implementing this architecture in a real-world character recogn\u0131tion system. The main issue is the tradeoff between cost and benefits such as accuracy and speed. A method for combining independently trained networks to achieve higher per-formance at relatively low cost is presented. Accurate estimates of the probability of correct recognition, as well as runner-up probabilities, are of ever-increasing importance as recog-nition systems move out of the lab into the real world. Per-character probabilities give us the information necessary for calculating per-field or multi-field probabilities. We discuss a method for normalizing output activations levels, thus providing a normalized score\u00a0\u2026",
      "citation_histogram": [
        [1992, 2],
        [1993, 2],
        [1994, 2],
        [1995, 1],
        [1996, 3],
        [1997, 1],
        [1998, 2],
        [1999, 1],
        [2000, 2],
        [2001, 4],
        [2002, 2],
        [2003, 7],
        [2004, 4],
        [2005, 4],
        [2006, 4],
        [2007, 7],
        [2008, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:wy5MF_2MSNEC",
      "title": "Large-scale kernel machines",
      "link": "https://scholar.google.com/scholar?cluster=7098895998837690032&hl=en&oi=scholarr",
      "year": 2007,
      "cited_by": 51,
      "authors": [
        "Yoshua Bengio",
        "Yann LeCun",
        "L Bottou",
        "O Chapelle",
        "D DeCoste",
        "J Weston"
      ],
      "description": null,
      "citation_histogram": [
        [2008, 1],
        [2009, 1],
        [2010, 5],
        [2011, 3],
        [2012, 4],
        [2013, 3],
        [2014, 3],
        [2015, 2],
        [2016, 2],
        [2017, 10],
        [2018, 7],
        [2019, 4],
        [2020, 4],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:GYFkgKAhzLcC",
      "title": "Boosting and other machine learning algorithms",
      "link": "https://www.sciencedirect.com/science/article/pii/B9781558603356500155",
      "year": 1994,
      "cited_by": 51,
      "authors": [
        "Harris Drucker",
        "Corinna Cortes",
        "Lawrence D Jackel",
        "Yann LeCun",
        "Vladimir Vapnik"
      ],
      "description": "In an optical character recognition problem, we compare (as a function of training set size) the performance of three neural network based ensemble methods (two versions of boosting and a committee of neural networks trained independently) to that of a single network. In boosting, the number of patterns actually used for training is a subset of all potential training patterns. Based on either a fixed computational cost or training set size criterion, some version of boosting is best We also compare (for a fixed training set size) boosting to the following algorithms: optimal margin classifiers, tangent distance, local learning, k-nearest neighbor, and a large weight sharing network with the boosting algorithm showing the best performance.",
      "citation_histogram": [
        [1995, 2],
        [1996, 6],
        [1997, 9],
        [1998, 4],
        [1999, 5],
        [2000, 4],
        [2001, 2],
        [2002, 1],
        [2003, 3],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 2],
        [2010, 3],
        [2011, 1],
        [2012, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:94js0qVkt3AC",
      "title": "MNIST handwritten digit database. AT&T Labs (2010)",
      "link": "https://scholar.google.com/scholar?cluster=12800938739924436968&hl=en&oi=scholarr",
      "year": 2020,
      "cited_by": 50,
      "authors": ["Y LeCun", "C Cortes", "C Burges"],
      "description": null,
      "citation_histogram": [
        [2016, 1],
        [2017, 4],
        [2018, 6],
        [2019, 4],
        [2020, 15],
        [2021, 8],
        [2022, 11]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:-jghkW3WqMUC",
      "title": "Unsupervised feature learning from temporal data",
      "link": "https://arxiv.org/abs/1504.02518",
      "year": 2015,
      "cited_by": 49,
      "authors": [
        "Ross Goroshin",
        "Joan Bruna",
        "Jonathan Tompson",
        "David Eigen",
        "Yann LeCun"
      ],
      "description": "Current state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric.",
      "citation_histogram": [
        [2014, 1],
        [2015, 6],
        [2016, 12],
        [2017, 7],
        [2018, 6],
        [2019, 6],
        [2020, 4],
        [2021, 4],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:uYElc5AnwZoC",
      "title": "MNIST handwritten digit database",
      "link": "https://scholar.google.com/scholar?cluster=14405812226351467566&hl=en&oi=scholarr",
      "year": 2010,
      "cited_by": 49,
      "authors": ["L Yann", "C Corinna", "B Chris"],
      "description": null,
      "citation_histogram": [
        [2015, 3],
        [2016, 1],
        [2017, 3],
        [2018, 10],
        [2019, 4],
        [2020, 6],
        [2021, 14],
        [2022, 8]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:CQQJn-Gxto4C",
      "title": "Advances in Neural Information Processing Systems 2 ed DS Touretzky",
      "link": "https://scholar.google.com/scholar?cluster=12024430993910508352&hl=en&oi=scholarr",
      "year": 1990,
      "cited_by": 49,
      "authors": ["Yann LeCun", "JS Denker", "SA Solla"],
      "description": null,
      "citation_histogram": [
        [1991, 1],
        [1992, 2],
        [1993, 2],
        [1994, 2],
        [1995, 1],
        [1996, 2],
        [1997, 1],
        [1998, 1],
        [1999, 1],
        [2000, 1],
        [2001, 4],
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 1],
        [2006, 2],
        [2007, 3],
        [2008, 5],
        [2009, 7],
        [2010, 10]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:RHpTSmoSYBkC",
      "title": "Mapping and planning under uncertainty in mobile robots with long-range perception",
      "link": "https://ieeexplore.ieee.org/abstract/document/4651203/",
      "year": 2008,
      "cited_by": 48,
      "authors": [
        "Pierre Sermanet",
        "Raia Hadsell",
        "Marco Scoffier",
        "Urs Muller",
        "Yann LeCun"
      ],
      "description": "Recent advances in self-supervised learning have enabled very long-range visual detection of obstacles and pathways (to 100 meters or more). Unfortunately, the category and range of regions at such large distances come with a considerable amount of uncertainty. We present a mapping and planning system that accurately represents range and category uncertainties, and accumulates the evidence from multiple frames in a principled way. The system relies on a hyperbolicpolar map centered on the robot with a 200 m radius. Map cells are histograms that accumulate evidence obtained from a self-supervised object classifier operating on image windows. The performance of the system is demonstrated on the LAGR off-road robot platform.",
      "citation_histogram": [
        [2008, 4],
        [2009, 3],
        [2010, 4],
        [2011, 4],
        [2012, 5],
        [2013, 5],
        [2014, 3],
        [2015, 3],
        [2016, 2],
        [2017, 2],
        [2018, 4],
        [2019, 6],
        [2020, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:JdYeQSb646wC",
      "title": "Unsupervised image matching and object discovery as optimization",
      "link": "http://openaccess.thecvf.com/content_CVPR_2019/html/Vo_Unsupervised_Image_Matching_and_Object_Discovery_as_Optimization_CVPR_2019_paper.html",
      "year": 2019,
      "cited_by": 46,
      "authors": [
        "Huy V Vo",
        "Francis Bach",
        "Minsu Cho",
        "Kai Han",
        "Yann LeCun",
        "Patrick P\u00e9rez",
        "Jean Ponce"
      ],
      "description": "Learning with complete or partial supervision is power-ful but relies on ever-growing human annotation efforts. As a way to mitigate this serious problem, as well as to serve specific applications, unsupervised learning has emerged as an important field of research. In computer vision, unsu-pervised learning comes in various guises. We focus here on the unsupervised discovery and matching of object cate-gories among images in a collection, following the work of Cho et al.[12]. We show that the original approach can be reformulated and solved as a proper optimization problem. Experiments on several benchmarks establish the merit of our approach.",
      "citation_histogram": [
        [2019, 5],
        [2020, 12],
        [2021, 13],
        [2022, 16]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:pyW8ca7W8N0C",
      "title": "Dynamic auto-encoders for semantic indexing",
      "link": "https://www.academia.edu/download/30543453/DAE_CameraReady.pdf",
      "year": 2010,
      "cited_by": 45,
      "authors": ["Piotr Mirowski", "M Ranzato", "Yann LeCun"],
      "description": "We present a new algorithm for topic modeling, text classification and retrieval, tailored to sequences of time-stamped documents. Based on the auto-encoder architecture, our nonlinear multi-layer model is trained stage-wise to produce increasingly more compact representations of bags-of-words at the document or paragraph level, thus performing a semantic analysis. It also incorporates simple temporal dynamics on the latent representations, to take advantage of the inherent structure of sequences of documents, and can simultaneously perform a supervised classification or regression on document labels. Learning this model is done by maximizing the joint likelihood of the model, and we use an approximate gradient-based MAP inference. We demonstrate that by minimizing a weighted cross-entropy loss between histograms of word occurrences and their reconstruction, we directly minimize the topic-model perplexity, and show that our topic model obtains lower perplexity than Latent Dirichlet Allocation on the NIPS and State of the Union datasets. We illustrate how the dynamical constraints help the learning while enabling to visualize the topic trajectory. Finally, we demonstrate state-of-the-art information retrieval and classification results on the Reuters collection, as well as an application to volatility forecasting from financial news.",
      "citation_histogram": [
        [2010, 1],
        [2011, 3],
        [2012, 2],
        [2013, 2],
        [2014, 4],
        [2015, 9],
        [2016, 5],
        [2017, 8],
        [2018, 4],
        [2019, 2],
        [2020, 3],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:n0_S8QYMK-AC",
      "title": "A hierarchical loss and its problems when classifying non-hierarchically",
      "link": "https://arxiv.org/abs/1709.01062",
      "year": 2017,
      "cited_by": 44,
      "authors": ["Cinna Wu", "Mark Tygert", "Yann LeCun"],
      "description": "Failing to distinguish between a sheepdog and a skyscraper should be worse and penalized more than failing to distinguish between a sheepdog and a poodle; after all, sheepdogs and poodles are both breeds of dogs. However, existing metrics of failure (so-called \"loss\" or \"win\") used in textual or visual classification/recognition via neural networks seldom leverage a-priori information, such as a sheepdog being more similar to a poodle than to a skyscraper. We define a metric that, inter alia, can penalize failure to distinguish between a sheepdog and a skyscraper more than failure to distinguish between a sheepdog and a poodle. Unlike previously employed possibilities, this metric is based on an ultrametric tree associated with any given tree organization into a semantically meaningful hierarchy of a classifier's classes. An ultrametric tree is a tree with a so-called ultrametric distance metric such that all leaves are at the same distance from the root. Unfortunately, extensive numerical experiments indicate that the standard practice of training neural networks via stochastic gradient descent with random starting points often drives down the hierarchical loss nearly as much when minimizing the standard cross-entropy loss as when trying to minimize the hierarchical loss directly. Thus, this hierarchical loss is unreliable as an objective for plain, randomly started stochastic gradient descent to minimize; the main value of the hierarchical loss may be merely as a meaningful metric of success of a classifier.",
      "citation_histogram": [
        [2018, 8],
        [2019, 10],
        [2020, 12],
        [2021, 9],
        [2022, 5]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:dONrx3-W1TkC",
      "title": "Structured sparse coding via lateral inhibition",
      "link": "http://yann.lecun.org/exdb/publis/pdf/gregor-nips-11.pdf",
      "year": 2011,
      "cited_by": 43,
      "authors": ["Karol Gregor", "Arthur Szlam", "Yann LeCun"],
      "description": "This work describes a conceptually simple method for structured sparse coding and dictionary design. Supposing a dictionary with K atoms, we introduce a structure as a set of penalties or interactions between every pair of atoms. We describe modifications of standard sparse coding algorithms for inference in this setting, and describe experiments showing that these algorithms are efficient. We show that interesting dictionaries can be learned for interactions that encode tree structures or locally connected structures. Finally, we show that our framework allows us to learn the values of the interactions from the data, rather than having them pre-specified.",
      "citation_histogram": [
        [2011, 2],
        [2012, 5],
        [2013, 7],
        [2014, 6],
        [2015, 5],
        [2016, 4],
        [2017, 2],
        [2018, 3],
        [2019, 5],
        [2020, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:hFOr9nPyWt4C",
      "title": "Module for constructing trainable modular network in which each module inputs and outputs data structured as a graph",
      "link": "https://patents.google.com/patent/US6128606A/en",
      "year": 2000,
      "cited_by": 43,
      "authors": ["Yoshua Bengio", "Leon Bottou", "Yann Andre LeCun"],
      "description": "A machine learning paradigm called Graph Transformer Networks extends the applicability of gradient-based learning algorithms to systems composed of modules that take graphs as inputs and produce graphs as output. Training is performed by computing gradients of a global objective function with respect to all the parameters in the system using a kind of back-propagation procedure. A complete check reading system based on these concept is described. The system uses convolutional neural network character recognizers, combined with global training techniques to provides record accuracy on business and personal checks.",
      "citation_histogram": [
        [2006, 5],
        [2007, 2],
        [2008, 4],
        [2009, 3],
        [2010, 1],
        [2011, 3],
        [2012, 1],
        [2013, 2],
        [2014, 2],
        [2015, 6],
        [2016, 1],
        [2017, 2],
        [2018, 3],
        [2019, 8]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:RtDPZMhf-s8C",
      "title": "Writer independent and writer adaptive neural network for on-line character recognition",
      "link": "https://nyuscholars.nyu.edu/en/publications/writer-independent-and-writer-adaptive-neural-network-for-on-line",
      "year": 1992,
      "cited_by": 43,
      "authors": [
        "I Guyon",
        "D Henderson",
        "P Albrecht",
        "Yann Lecun",
        "JS Denker"
      ],
      "description": "Writer independent and writer adaptive neural network for on-line character recognition \u2014 NYU \nScholars Skip to main navigation Skip to search Skip to main content NYU Scholars Home NYU \nScholars Logo Help & FAQ Home Profiles Research Units Research output Search by \nexpertise, name or affiliation Writer independent and writer adaptive neural network for on-line \ncharacter recognition I. Guyon, D. Henderson, P. Albrecht, Yann Lecun, JS Denker Computer \nScience Research output: Chapter in Book/Report/Conference proceeding \u203a Chapter \n(peer-reviewed) \u203a peer-review Overview Original language English (US) Title of host publication \nFrom pixels to features III Subtitle of host publication Frontiers in handwriting recognition \nEditors S. Impedovo, JC Simon Publisher Elsevier Pages 493-506 State Published - 1992 Cite \nthis APA Standard Harvard Vancouver Author BIBTEX RIS Guyon, I., Henderson, D., Albrecht, \u2026",
      "citation_histogram": [
        [1991, 1],
        [1992, 2],
        [1993, 4],
        [1994, 3],
        [1995, 7],
        [1996, 2],
        [1997, 2],
        [1998, 1],
        [1999, 1],
        [2000, 2],
        [2001, 1],
        [2002, 5],
        [2003, 2],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 2],
        [2008, 1],
        [2009, 1],
        [2010, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:-f6ydRqryjwC",
      "title": "Hierarchical constrained automatic learning network for character recognition",
      "link": "https://patents.google.com/patent/US5058179A/en",
      "year": 1991,
      "cited_by": 41,
      "authors": [
        "John S Denker",
        "Richard E Howard",
        "Lawrence D Jackel",
        "Yann LeCun"
      ],
      "description": "Highly accurate, reliable optical character recognition is afforded by a hierarchically layered network having several layers of parallel constrained feature detection for localized feature extraction followed by several fully connected layers for dimensionality reduction. Character classification is also performed in the ultimate fully connected layer. Each layer of parallel constrained feature detection comprises a plurality of constrained feature maps and a corresponding plurality of kernels wherein a predetermined kernel is directly related to a single constrained feature map. Undersampling is performed from layer to layer.",
      "citation_histogram": [
        [1994, 4],
        [1995, 3],
        [1996, 3],
        [1997, 4],
        [1998, 1],
        [1999, 2],
        [2000, 1],
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 2],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 4],
        [2010, 3],
        [2011, 3],
        [2012, 2],
        [2013, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:1Ye0OR6EYb4C",
      "title": "Fast approximations to structured sparse coding and applications to object classification",
      "link": "https://link.springer.com/chapter/10.1007/978-3-642-33715-4_15",
      "year": 2012,
      "cited_by": 40,
      "authors": ["Arthur Szlam", "Karol Gregor", "Yann LeCun"],
      "description": " We describe a method for fast approximation of sparse coding. A given input vector is passed through a binary tree. Each leaf of the tree contains a subset of dictionary elements. The coefficients corresponding to these dictionary elements are allowed to be nonzero and their values are calculated quickly by multiplication with a precomputed pseudoinverse. The tree parameters, the dictionary, and the subsets of the dictionary corresponding to each leaf are learned. In the process of describing this algorithm, we discuss the more general problem of learning the groups in group structured sparse modeling. We show that our method creates good sparse representations by using it in the object recognition framework of [1,2]. Implementing our own fast version of the SIFT descriptor the whole system runs at 20 frames per second on 321 \u00d7481 sized images on a laptop with a quad-core cpu, while sacrificing very\u00a0\u2026",
      "citation_histogram": [
        [2012, 1],
        [2013, 2],
        [2014, 5],
        [2015, 8],
        [2016, 8],
        [2017, 7],
        [2018, 3],
        [2019, 1],
        [2020, 3],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:pqnbT2bcN3wC",
      "title": "A sparse and locally shift invariant feature extractor applied to document images",
      "link": "https://ieeexplore.ieee.org/abstract/document/4377108/",
      "year": 2007,
      "cited_by": 40,
      "authors": ["M Ranzato", "Yann LeCun"],
      "description": "We describe an unsupervised learning algorithm for extracting sparse and locally shift-invariant features. We also devise a principled procedure for learning hierarchies of invariant features. Each feature detector is composed of a set of trainable convolutional filters followed by a max-pooling layer over non-overlapping windows, and a point-wise sigmoid non-linearity. A second stage of more invariant features is fed with patches provided by the first stage feature extractor, and is trained in the same way. The method is used to pre-train the first four layers of a deep convolutional network which achieves state-of-the-art performance on the MNIST dataset of handwritten digits. The final testing error rate is equal to 0.42%. Preliminary experiments on compression of bitonal document images show very promising results in terms of compression ratio and reconstruction error.",
      "citation_histogram": [
        [2007, 1],
        [2008, 1],
        [2009, 2],
        [2010, 1],
        [2011, 2],
        [2012, 4],
        [2013, 1],
        [2014, 2],
        [2015, 3],
        [2016, 2],
        [2017, 5],
        [2018, 1],
        [2019, 2],
        [2020, 2],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:-_dYPAW6P2MC",
      "title": "Graphical system for automated segmentation and recognition for image recognition systems",
      "link": "https://patents.google.com/patent/US5487117A/en",
      "year": 1996,
      "cited_by": 40,
      "authors": ["Christopher J Burges", "Yann A Le Cun", "Ofer Matan"],
      "description": "Apparatus and processes are described for the automatic recognition of alphanumeric images. A set of cuts are made to the image which include incorrect segmentations. The resulting\" cells\" comprising in their totality the created segments of the image are then analyzed to determine which cells are legal neighbors and which are not. All cells which are legal neighbors are then presented as connected nodes. A pruning of nodes which are related to certain predetermined image cuts is effected. Each set of remaining connected nodes is then presented to a recognizer which identifies the image and assigns a specified probability to the output. Many cells which are not legal neighbors are thereby not presented to the recognizer, thus saving substantially on computations per recognized image.",
      "citation_histogram": [
        [1996, 1],
        [1997, 4],
        [1998, 3],
        [1999, 3],
        [2000, 4],
        [2001, 1],
        [2002, 1],
        [2003, 2],
        [2004, 4],
        [2005, 2],
        [2006, 4],
        [2007, 1],
        [2008, 1],
        [2009, 2],
        [2010, 2],
        [2011, 2],
        [2012, 2],
        [2013, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:g-FVFPYC6a8C",
      "title": "Binary embeddings with structured hashed projections",
      "link": "http://proceedings.mlr.press/v48/choromanska16.html",
      "year": 2015,
      "cited_by": 38,
      "authors": [
        "Anna Choromanska",
        "Krzysztof Choromanski",
        "Mariusz Bojarski",
        "Tony Jebara",
        "Sanjiv Kumar",
        "Yann LeCun"
      ],
      "description": "We consider the hashing mechanism for constructing binary embeddings, that involves pseudo-random projections followed by nonlinear (sign function) mappings. The pseudo-random projection is described by a matrix, where not all entries are independent random variables but instead a fixed \u201cbudget of randomness\u201d is distributed across the matrix. Such matrices can be edfficiently stored in sub-quadratic or even linear space, provide reduction in randomness usage (ie number of required random values), and very often lead to computational speed ups. We prove several theoretical results showing that projections via various structured matrices followed by nonlinear mappings accurately preserve the angular distance between input high-dimensional vectors. To the best of our knowledge, these results are the first that give theoretical ground for the use of general structured matrices in the nonlinear setting. We empirically verify our theoretical findings and show the dependence of learning via structured hashed projections on the performance of neural network as well as nearest neighbor classifier.",
      "citation_histogram": [
        [2016, 4],
        [2017, 7],
        [2018, 10],
        [2019, 6],
        [2020, 4],
        [2021, 6],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:g5m5HwL7SMYC",
      "title": "Eblearn: Open-source energy-based learning in c++",
      "link": "https://ieeexplore.ieee.org/abstract/document/5366626/",
      "year": 2009,
      "cited_by": 38,
      "authors": ["Pierre Sermanet", "Koray Kavukcuoglu", "Yann LeCun"],
      "description": "Energy-based learning (EBL) is a general framework to describe supervised and unsupervised training methods for probabilistic and non-probabilistic factor graphs. An energy-based model associates a scalar energy to configurations of inputs, outputs, and latent variables. Learning machines can be constructed by assembling modules and loss functions. Gradient-based learning procedures are easily implemented through semi-automatic differentiation of complex models constructed by assembling predefined modules. We introduce an open-source and cross-platform C++ library called EBLearn to enable the construction of energy-based learning models. EBLearn is composed of two major components, libidx: an efficient and flexible multi-dimensional tensor library, and libeblearn: an object-oriented library of trainable modules and learning algorithms. The latter has facilities for such models as convolutional\u00a0\u2026",
      "citation_histogram": [
        [2010, 2],
        [2011, 7],
        [2012, 3],
        [2013, 4],
        [2014, 8],
        [2015, 3],
        [2016, 3],
        [2017, 2],
        [2018, 2],
        [2019, 1],
        [2020, 1],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:t6usbXjVLHcC",
      "title": "Pattern Recognition and Neural Networks",
      "link": "http://www-labs.iro.umontreal.ca/~lisa/pointeurs/handbook-patrec.pdf",
      "year": 1995,
      "cited_by": 38,
      "authors": ["Yann LeCun", "Yoshua Benhio"],
      "description": "Pattern Recognition (PR) addresses the problem of classifying objects, often represented as vectors or as strings of symbols, into categories. The di culty is to synthesize, and then to e ciently compute, the classi cation function that maps objects to categories, given that objects in a category can have widely varying input representations. In most instances, the task is known to the designer through a set of example patterns whose categories are known, and through general, a priori knowledge about the task, such as:\\the category of an object is not changed when the object is slightly translated or rotated in space\".Historically, the eld of PR started with the early e orts in Neural Networks (Perceptrons, Adalines...). While in the past, NNs have somewhat played the role of an outsider in PR, the recent progress in learning algorithms (and the availability of powerful hardware) have made them the method of choice for many PR applications.",
      "citation_histogram": [
        [1999, 1],
        [2000, 3],
        [2001, 1],
        [2002, 2],
        [2003, 1],
        [2004, 2],
        [2005, 5],
        [2006, 1],
        [2007, 4],
        [2008, 1],
        [2009, 3],
        [2010, 3],
        [2011, 4],
        [2012, 3],
        [2013, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:KrOX6H5u0oYC",
      "title": "EVALUATION OF NETWORK ARCHITECTURES ON TEST LEARNING TASKS.",
      "link": "https://nyuscholars.nyu.edu/en/publications/evaluation-of-network-architectures-on-test-learning-tasks",
      "year": 1987,
      "cited_by": 38,
      "authors": [
        "F Fogelman Soulie",
        "P Gallinari",
        "Yann Le Cun",
        "S Thiria"
      ],
      "description": "Two experiments with the gradient back propagation (GBP) algorithm have been carried out to determine how the architecture affects the performances of the network. The two examples have been designed to investigate two different properties of the networks: their memorization capacities and their ability to generalize by synthesizing appropriate predicates. The first experiment is an extension to the GBP algorithm of previous work comparing the memorization and generalization abilities of various network models on simple associative memory tasks. In the second experiment a network is taught to detect the presence of a given pattern in a signal.",
      "citation_histogram": [
        [1988, 4],
        [1989, 6],
        [1990, 5],
        [1991, 2],
        [1992, 3],
        [1993, 3],
        [1994, 1],
        [1995, 1],
        [1996, 2],
        [1997, 1],
        [1998, 1],
        [1999, 2],
        [2000, 1],
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:VxjqAHCqNGUC",
      "title": "Convolutional nets and watershed cuts for real-time semantic labeling of rgbd videos",
      "link": "https://www.jmlr.org/papers/volume15/couprie14a/couprie14a.pdf",
      "year": 2014,
      "cited_by": 37,
      "authors": [
        "Camille Couprie",
        "Cl\u00e9ment Farabet",
        "Laurent Najman",
        "Yann LeCun"
      ],
      "description": "This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on handcrafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. Using a frame by frame labeling, we obtain nearly state-of-the-art performance on the NYU-v2 depth data set with an accuracy of 64.5%. We then show that the labeling can be further improved by exploiting the temporal consistency in the video sequence of the scene. To that goal, we present a method producing temporally consistent superpixels from a streaming video. Among the different methods producing superpixel segmentations of an image, the graph-based approach of Felzenszwalb and Huttenlocher is broadly employed. One of its interesting properties is that the regions are computed in a greedy manner in quasi-linear time by using a minimum spanning tree. In a framework exploiting minimum spanning trees all along, we propose an efficient video segmentation approach that computes temporally consistent pixels in a causal manner, filling the need for causal and real-time applications. We illustrate the labeling of indoor scenes in video sequences that could be processed in real-time using appropriate hardware such as an FPGA.",
      "citation_histogram": [
        [2015, 3],
        [2016, 8],
        [2017, 9],
        [2018, 3],
        [2019, 7],
        [2020, 1],
        [2021, 3],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:TlpoogIpr_IC",
      "title": "Learning stable group invariant representations with convolutional networks",
      "link": "https://arxiv.org/abs/1301.3537",
      "year": 2013,
      "cited_by": 37,
      "authors": ["Joan Bruna", "Arthur Szlam", "Yann LeCun"],
      "description": "Transformation groups, such as translations or rotations, effectively express part of the variability observed in many recognition problems. The group structure enables the construction of invariant signal representations with appealing mathematical properties, where convolutions, together with pooling operators, bring stability to additive and geometric perturbations of the input. Whereas physical transformation groups are ubiquitous in image and audio applications, they do not account for all the variability of complex signal classes. We show that the invariance properties built by deep convolutional networks can be cast as a form of stable group invariance. The network wiring architecture determines the invariance group, while the trainable filter coefficients characterize the group action. We give explanatory examples which illustrate how the network architecture controls the resulting invariance group. We also explore the principle by which additional convolutional layers induce a group factorization enabling more abstract, powerful invariant representations.",
      "citation_histogram": [
        [2014, 2],
        [2015, 1],
        [2016, 3],
        [2017, 15],
        [2018, 3],
        [2019, 1],
        [2020, 3],
        [2021, 7]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:OBSaB-F7qqsC",
      "title": "Memoires associatives distribuees",
      "link": "https://scholar.google.com/scholar?cluster=8489102503862447275&hl=en&oi=scholarr",
      "year": 1987,
      "cited_by": 37,
      "authors": [
        "Patrick Gallinari",
        "Yann LeCun",
        "Sylvie Thiria",
        "Francoise Fogelman-Soulie"
      ],
      "description": null,
      "citation_histogram": [
        [2008, 4],
        [2009, 2],
        [2010, 3],
        [2011, 3],
        [2012, 2],
        [2013, 6],
        [2014, 2],
        [2015, 2],
        [2016, 1],
        [2017, 3],
        [2018, 3],
        [2019, 5],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:f9jR0vFhilIC",
      "title": "The Power and Limits of Deep Learning: In his IRI Medal address, Yann LeCun maps the development of machine learning techniques and suggests what the future may hold.",
      "link": "https://www.tandfonline.com/doi/abs/10.1080/08956308.2018.1516928",
      "year": 2018,
      "cited_by": 36,
      "authors": ["Yann LeCun"],
      "description": "Artificial intelligence (AI) is advancing very rapidly. I\u2019ve had a front-row seat for a lot of the recent progress\u2014first at Bell Labs (which was renamed AT&T Labs in 1996, while I was there) and then at the NEC Research Institute in Princeton. I joined academia after these stints in industrial research. Now, I have one foot in each world, or at least half a foot in academia and one-and-a-half feet in industry. I joined Facebook at the end of 2013 for the purpose of building Facebook AI Research (FAIR), which was created because Mark Zuckerberg knew that AI was going to be a critical piece of technology for Facebook and for connecting people with each other. I spend most of my time at Facebook, but I still teach, and I have a lab with PhD students at NYU. I think the two are complementary worlds, and I find that very enriching. FAIR was founded to focus on research in AI. In the past, the field of AI focused on tasks that are\u00a0\u2026",
      "citation_histogram": [
        [2018, 1],
        [2019, 9],
        [2020, 7],
        [2021, 12],
        [2022, 7]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:ZeXyd9-uunAC",
      "title": "Real time voice processing with audiovisual feedback: toward autonomous agents with perfect pitch",
      "link": "https://proceedings.neurips.cc/paper/2002/hash/43351f7bf9a215be70c2c2caa7555002-Abstract.html",
      "year": 2003,
      "cited_by": 36,
      "authors": [
        "Lawrence K Saul",
        "Daniel D Lee",
        "Charles L Isbell",
        "Yann LeCun"
      ],
      "description": "We have implemented a real time front end for detecting voiced speech and estimating its fundamental frequency. The front end performs the signal processing for voice-driven agents that attend to the pitch contours of human speech and provide continuous audiovisual feedback. The algorithm we use for pitch tracking has several distinguishing features: it makes no use of FFTs or autocorrelation at the pitch period; it updates the pitch incrementally on a sample-by-sample basis; it avoids peak picking and does not require interpolation in time or frequency to obtain high resolution estimates; and it works reliably over a four octave range, in real time, without the need for postprocessing to produce smooth contours. The algorithm is based on two simple ideas in neural computation: the introduction of a purposeful nonlinearity, and the error signal of a least squares fit. The pitch tracker is used in two real time multimedia applications: a voice-to-MIDI player that synthesizes electronic music from vocalized melodies, and an audiovisual Karaoke machine with multimodal feedback. Both applications run on a laptop and display the user\u2019s pitch scrolling across the screen as he or she sings into the computer.",
      "citation_histogram": [
        [2003, 5],
        [2004, 6],
        [2005, 4],
        [2006, 5],
        [2007, 4],
        [2008, 3],
        [2009, 1],
        [2010, 2],
        [2011, 1],
        [2012, 1],
        [2013, 1],
        [2014, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:OBae9N4Z9bMC",
      "title": "Model-based planning with discrete and continuous actions",
      "link": "https://arxiv.org/abs/1705.07177",
      "year": 2017,
      "cited_by": 35,
      "authors": ["Mikael Henaff", "William F Whitney", "Yann LeCun"],
      "description": "Action planning using learned and differentiable forward models of the world is a general approach which has a number of desirable properties, including improved sample complexity over model-free RL methods, reuse of learned models across different tasks, and the ability to perform efficient gradient-based optimization in continuous action spaces. However, this approach does not apply straightforwardly when the action space is discrete. In this work, we show that it is in fact possible to effectively perform planning via backprop in discrete action spaces, using a simple paramaterization of the actions vectors on the simplex combined with input noise when training the forward model. Our experiments show that this approach can match or outperform model-free RL and discrete planning methods on gridworld navigation tasks in terms of performance and/or planning time while using limited environment interactions, and can additionally be used to perform model-based control in a challenging new task where the action space combines discrete and continuous actions. We furthermore propose a policy distillation approach which yields a fast policy network which can be used at inference time, removing the need for an iterative planning procedure.",
      "citation_histogram": [
        [2018, 2],
        [2019, 6],
        [2020, 7],
        [2021, 15],
        [2022, 5]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:_Re3VWB3Y0AC",
      "title": "Memory-based character recognition using a transformation invariant metric",
      "link": "https://ieeexplore.ieee.org/abstract/document/576916/",
      "year": 1994,
      "cited_by": 35,
      "authors": ["Patrice Y Simard", "Yann Le Cun", "John S Denker"],
      "description": "Memory-based classification algorithms such as radial basis functions or K-nearest neighbors often rely on simple distances (Euclidean distance, Hamming distance, etc.), which are rarely meaningful on pattern vectors. More complex better suited distance measures are often expensive and rather ad-hoc. We propose a new distance measure which: 1) can be made locally invariant to any set of transformations of the input; and 2) can be computed efficiently. We tested the method on large handwritten character databases provided by the US Post Office and NIST. Using invariances with respect to translation, rotation, scaling, skewing and line thickness, the method outperformed all other systems on a small (less than 10,000 patterns) database and was competitive on our largest (60,000 patterns) database.",
      "citation_histogram": [
        [1996, 1],
        [1997, 7],
        [1998, 1],
        [1999, 2],
        [2000, 1],
        [2001, 5],
        [2002, 2],
        [2003, 4],
        [2004, 1],
        [2005, 3],
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:2KloaMYe4IUC",
      "title": "An efficient algorithm for learning invariance in adaptive classifiers",
      "link": "https://www.computer.org/csdl/proceedings-article/icpr/1992/00201861/12OmNzwHvcI",
      "year": 1992,
      "cited_by": 35,
      "authors": [
        "Patrice Simard",
        "Yann Le Cun",
        "J Denker",
        "Bernard Victorri"
      ],
      "description": "In many machine learning applications, one has not only training data but also some high-level information about certain invariances that the system should exhibit. In character recognition, for example, the answer should be invariant with respect to small spatial distortions in the input images (translations, rotations, scale changes, etcetera). The authors have implemented a scheme that minimizes the derivative of the classifier outputs with respect to distortion operators. This not only produces tremendous speed advantages, but also provides a powerful language for specifying what generalizations the network can perform.<>",
      "citation_histogram": [
        [1992, 1],
        [1993, 1],
        [1994, 1],
        [1995, 3],
        [1996, 1],
        [1997, 3],
        [1998, 2],
        [1999, 2],
        [2000, 2],
        [2001, 3],
        [2002, 2],
        [2003, 2],
        [2004, 3],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 2],
        [2010, 1],
        [2011, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:ZHo1McVdvXMC",
      "title": "Discovering the hidden structure of house prices with a non-parametric latent manifold model",
      "link": "https://dl.acm.org/doi/abs/10.1145/1281192.1281214",
      "year": 2007,
      "cited_by": 34,
      "authors": [
        "Sumit Chopra",
        "Trivikraman Thampy",
        "John Leahy",
        "Andrew Caplin",
        "Yann LeCun"
      ],
      "description": "In many regression problems, the variable to be predicted depends not only on a sample-specific feature vector, but also on an unknown (latent) manifold that must satisfy known constraints. An example is house prices, which depend on the characteristics of the house, and on the desirability of the neighborhood, which is not directly measurable. The proposed method comprises two trainable components. The first one is a parametric model that predicts the\" intrinsic\" price of the house from its description. The second one is a smooth, non-parametric model of the latent\" desirability\" manifold. The predicted price of a house is the product of its intrinsic price and desirability. The two components are trained simultaneously using a deterministic form of the EM algorithm. The model was trained on a large dataset of houses from Los Angeles county. It produces better predictions than pure parametric and non-parametric\u00a0\u2026",
      "citation_histogram": [
        [2009, 2],
        [2010, 2],
        [2011, 2],
        [2012, 1],
        [2013, 5],
        [2014, 7],
        [2015, 2],
        [2016, 7],
        [2017, 2],
        [2018, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:HjGq7OYTVFUC",
      "title": "Self-supervised Learning: The Dark Matter of Intelligence",
      "link": "https://scholar.google.com/scholar?cluster=4072501699057660320&hl=en&oi=scholarr",
      "year": 2021,
      "cited_by": 33,
      "authors": ["Yann LeCun", "Ishan Misra"],
      "description": null,
      "citation_histogram": [
        [2021, 13],
        [2022, 20]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:35N4QoGY0k4C",
      "title": "Time-delay neural networks and independent component analysis for eeg-based prediction of epileptic seizures propagation",
      "link": "https://www.aaai.org/Papers/AAAI/2007/AAAI07-321.pdf?q=independent-component-analysis-algorithms-and-applications",
      "year": 2007,
      "cited_by": 33,
      "authors": ["Piotr W Mirowski", "Deepak Madhavan", "Yann LeCun"],
      "description": "This research focuses on the development of a machine learning technique based on Time-Delay Neural Networks (TDNN) and Independent Component Analysis (ICA), to analyze EEG signal dynamics related to the initiation and propagation of epileptic seizures. We aim at designing a generative model to simulate EEG time-series after alteration of specific localized channels (electrodes) in order to explore the effects of brain surgery ex-vivo.",
      "citation_histogram": [
        [2007, 1],
        [2008, 2],
        [2009, 1],
        [2010, 1],
        [2011, 3],
        [2012, 2],
        [2013, 2],
        [2014, 1],
        [2015, 3],
        [2016, 5],
        [2017, 3],
        [2018, 5],
        [2019, 1],
        [2020, 1],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:JzGFD3-rS6kC",
      "title": "Differentially-and non-differentially-private random decision trees",
      "link": "https://arxiv.org/abs/1410.6973",
      "year": 2014,
      "cited_by": 32,
      "authors": [
        "Mariusz Bojarski",
        "Anna Choromanska",
        "Krzysztof Choromanski",
        "Yann LeCun"
      ],
      "description": "We consider supervised learning with random decision trees, where the tree construction is completely random. The method is popularly used and works well in practice despite the simplicity of the setting, but its statistical mechanism is not yet well-understood. In this paper we provide strong theoretical guarantees regarding learning with random decision trees. We analyze and compare three different variants of the algorithm that have minimal memory requirements: majority voting, threshold averaging and probabilistic averaging. The random structure of the tree enables us to adapt these methods to a differentially-private setting thus we also propose differentially-private versions of all three schemes. We give upper-bounds on the generalization error and mathematically explain how the accuracy depends on the number of random decision trees. Furthermore, we prove that only logarithmic (in the size of the dataset) number of independently selected random decision trees suffice to correctly classify most of the data, even when differential-privacy guarantees must be maintained. We empirically show that majority voting and threshold averaging give the best accuracy, also for conservative users requiring high privacy guarantees. Furthermore, we demonstrate that a simple majority voting rule is an especially good candidate for the differentially-private classifier since it is much less sensitive to the choice of forest parameters than other methods.",
      "citation_histogram": [
        [2015, 2],
        [2016, 3],
        [2017, 6],
        [2018, 7],
        [2019, 2],
        [2020, 3],
        [2021, 9]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:hC7cP41nSMkC",
      "title": "Reverse TDNN: an architecture for trajectory generation",
      "link": "https://proceedings.neurips.cc/paper/1991/hash/fde9264cf376fffe2ee4ddf4a988880d-Abstract.html",
      "year": 1992,
      "cited_by": 32,
      "authors": ["Patrice Simard", "Yann LeCun"],
      "description": "The backpropagation algorithm can be used for both recognition and gen (cid: 173) eration of time trajectories. When used as a recognizer, it has been shown that the performance of a network can be greatly improved by adding structure to the architecture. The same is true in trajectory generation. In particular a new architecture corresponding to a\" reversed\" TDNN is proposed. Results show dramatic improvement of performance in the gen (cid: 173) eration of hand-written characters. A combination of TDNN and reversed TDNN for compact encoding is also suggested.",
      "citation_histogram": [
        [1992, 1],
        [1993, 4],
        [1994, 7],
        [1995, 3],
        [1996, 1],
        [1997, 1],
        [1998, 2],
        [1999, 1],
        [2000, 1],
        [2001, 2],
        [2002, 3],
        [2003, 4],
        [2004, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:4fKUyHm3Qg0C",
      "title": "Method, system, and computer-accessible medium for classification of at least one ICTAL state",
      "link": "https://patents.google.com/patent/US9443141B2/en",
      "year": 2016,
      "cited_by": 31,
      "authors": [
        "Piotr W Mirowski",
        "Deepak Madhavan",
        "Yann LeCun",
        "Ruben Kuzniecky"
      ],
      "description": "An exemplary methodology, procedure, system, method and computer-accessible medium can be provided for receiving physiological data for the subject, extracting one or more patterns of features from the physiological data, and classifying the at least one state of the subject using a spatial structure and a temporal structure of the one or more patterns of features, wherein at least one of the at least one state is an ictal state.",
      "citation_histogram": [
        [2017, 5],
        [2018, 5],
        [2019, 4],
        [2020, 2],
        [2021, 9],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:08ZZubdj9fEC",
      "title": "System and method for sending multi-media messages using emoticons",
      "link": "https://patents.google.com/patent/US7921013B1/en",
      "year": 2011,
      "cited_by": 31,
      "authors": [
        "Joern Ostermann",
        "Mehmet Reha Civanlar",
        "Eric Cosatto",
        "Hans Peter Graf",
        "Yann Andre LeCun"
      ],
      "description": "A system and method of providing sender-customization of multi-media messages through the use of emoticons is disclosed. The sender inserts the emoticons into a text message. As an animated face audibly delivers the text, emoticons associated with the message are started a predetermined period of time or number of words prior to the position of the emoticon in the message text and completed a predetermined length of time or number of words following the location of the emoticon. The sender may insert emoticons through the use of emoticon buttons that are icons available for choosing. Upon sender selections of an emoticon, an icon representing the emoticon is inserted into the text at the position of the cursor. Once an emoticon is chosen, the sender may also choose the amplitude for the emoticon and increased or decreased amplitude will be displayed in the icon inserted into the message text.",
      "citation_histogram": [
        [2010, 1],
        [2011, 2],
        [2012, 5],
        [2013, 1],
        [2014, 1],
        [2015, 4],
        [2016, 3],
        [2017, 5],
        [2018, 3],
        [2019, 5]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:tp0eXr8pwPYC",
      "title": "Scanning the Technology",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.5.6603&rep=rep1&type=pdf",
      "year": 1998,
      "cited_by": 31,
      "authors": [
        "Richard V Cox",
        "Barry G Haskell",
        "Yann Lecun",
        "Behzad Shahraray",
        "Lawrence Rabiner"
      ],
      "description": "I. INTRODUCTION In a very real sense, virtually every individual has had experience with multimedia systems of one type or another. Perhaps the most common multimedia experiences are reading the daily newspaper or watching television. These may not seem like the exotic multimedia experiences that are discussed daily in the media or on television, but nonetheless, these are multimedia experiences. Before proceeding further, it is worthwhile to define exactly what constitutes a multimedia experience or a multimedia signal so we can focus clearly on a set of technological needs for creating a rich multimedia communications experience. The dictionary definition of multimedia is: including or involving the use of several media of communication, entertainment, or expression. A more technological definition of multimedia, as it applies to communications systems, might be the following: integration of two or more of the following media for the purpose of transmission, storage, access, and content creation:\u2022 text;\u2022 images;\u2022 graphics;\u2022 speech;",
      "citation_histogram": [
        [1998, 1],
        [1999, 3],
        [2000, 1],
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 2],
        [2005, 9],
        [2006, 3],
        [2007, 2],
        [2008, 1],
        [2009, 2],
        [2010, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:4EsMycecMEYC",
      "title": "A theoretical framework for back-propagation",
      "link": "https://nyuscholars.nyu.edu/en/publications/a-theoretical-framework-for-back-propagation",
      "year": 1992,
      "cited_by": 31,
      "authors": ["Yann Lecun"],
      "description": "A theoretical framework for back-propagation \u2014 NYU Scholars Skip to main navigation Skip to \nsearch Skip to main content NYU Scholars Home NYU Scholars Logo Help & FAQ Home \nProfiles Research Units Research output Search by expertise, name or affiliation A theoretical \nframework for back-propagation Yann Lecun Computer Science Research output: Chapter in \nBook/Report/Conference proceeding \u203a Chapter (peer-reviewed) \u203a peer-review Overview \nOriginal language English (US) Title of host publication Artificial neural networks Subtitle of \nhost publication Concepts and theory Editors P. Mehra, B. Wah Place of Publication Los \nAlamitos, CA Publisher IEEE Computer Society Press State Published - 1992 Cite this APA \nStandard Harvard Vancouver Author BIBTEX RIS Lecun, Y. (1992). A theoretical framework for \nback-propagation. In P. Mehra, & B. Wah (Eds.), Artificial neural networks: Concepts and theory \u2026",
      "citation_histogram": [
        [2003, 3],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 8],
        [2011, 4],
        [2012, 4],
        [2013, 5],
        [2014, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:kWvqk_afx_IC",
      "title": "GEMINI: gradient estimation through matrix inversion after noise injection",
      "link": "https://proceedings.neurips.cc/paper/1988/hash/a0a080f42e6f13b3a2df133f073095dd-Abstract.html",
      "year": 1989,
      "cited_by": 31,
      "authors": ["Yann LeCun", "Conrad C Galland", "Geoffrey E Hinton"],
      "description": "Learning procedures that measure how random perturbations of unit ac (cid: 173) tivities correlate with changes in reinforcement are inefficient but simple to implement in hardware. Procedures like back-propagation (Rumelhart, Hinton and Williams, 1986) which compute how changes in activities af (cid: 173) fect the output error are much more efficient, but require more complex hardware. GEMINI is a hybrid procedure for multilayer networks, which shares many of the implementation advantages of correlational reinforce (cid: 173) ment procedures but is more efficient. GEMINI injects noise only at the first hidden layer and measures the resultant effect on the output error. A linear network associated with each hidden layer iteratively inverts the matrix which relates the noise to the error change, thereby obtaining the error-derivatives. No back-propagation is involved, thus allowing un (cid: 173) known non-linearities in the system. Two simulations demonstrate the effectiveness of GEMINI.",
      "citation_histogram": [
        [1990, 1],
        [1991, 1],
        [1992, 2],
        [1993, 1],
        [1994, 1],
        [1995, 3],
        [1996, 3],
        [1997, 2],
        [1998, 2],
        [1999, 1],
        [2000, 2],
        [2001, 1],
        [2002, 2],
        [2003, 3],
        [2004, 3],
        [2005, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:vV6vV6tmYwMC",
      "title": "Reassessing FHA risk",
      "link": "https://www.nber.org/papers/w15802",
      "year": 2010,
      "cited_by": 30,
      "authors": [
        "Diego Aragon",
        "Andrew Caplin",
        "Sumit Chopra",
        "John V Leahy",
        "Yann LeCun",
        "Marco Scoffier",
        "Joseph Tracy"
      ],
      "description": "Federal Housing Administration (FHA) insurance has doubled over the past two years and is projected to redouble to $1.5 trillion over the next five. Despite clear signs of strain in the FHA\u2019s Mutual Mortgage Insurance Fund, a recent actuarial review indicates that the FHA will not need any form of government support. We identify four risk factors that make such a funding request more likely; the review underestimates how many FHA borrowers are underwater and in economic distress; it uses measures of house values that lower loss estimates; it does not incorporate early-warning signals of future losses that are available from mortgage delinquency; and it ignores potential risks associated with recent down-payment assistant programs despite higher losses on previous programs of this type. We propose measures that could be taken to improve the predictive accuracy of FHA risk assessment.",
      "citation_histogram": [
        [2010, 1],
        [2011, 5],
        [2012, 3],
        [2013, 5],
        [2014, 3],
        [2015, 2],
        [2016, 3],
        [2017, 2],
        [2018, 1],
        [2019, 1],
        [2020, 3],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:yD5IFk8b50cC",
      "title": "Machine learning and the spatial structure of house prices and housing returns",
      "link": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1316046",
      "year": 2008,
      "cited_by": 30,
      "authors": [
        "Andrew Caplin",
        "Sumit Chopra",
        "John V Leahy",
        "Yann LeCun",
        "Trivikraman Thampy"
      ],
      "description": "Economists do not have reliable measures of current house values, let alone housing returns. This ignorance underlies the illiquidity of mortgage-backed securities, which in turn feeds back to deepen the sub-prime crisis. Using a massive new data tape of housing transactions in LA, we demonstrate systematic patterns in the error associated with using the ubiquitous repeat sales methodology to understand house values. In all periods, the resulting indices under-predict sales prices of less expensive homes, and over-predict prices of more expensive homes. The recent period has produced errors that are not only unprecedentedly large in absolute value, but highly systematic: after a few years in which the indices under-predicted prices, they now significantly over-predict them. We introduce new machine learning techniques from computer science to correct for prediction errors that have geographic origins. The results are striking. Accounting for geography significantly reduces the extent of the prediction error, removes many of the systematic patterns, and results in far less deterioration in model performance in the recent period.",
      "citation_histogram": [
        [2012, 2],
        [2013, 1],
        [2014, 3],
        [2015, 1],
        [2016, 3],
        [2017, 1],
        [2018, 5],
        [2019, 5],
        [2020, 5],
        [2021, 1],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:EF0m1YoOS5EC",
      "title": "Learning in high dimension always amounts to extrapolation",
      "link": "https://arxiv.org/abs/2110.09485",
      "year": 2021,
      "cited_by": 29,
      "authors": ["Randall Balestriero", "Jerome Pesenti", "Yann LeCun"],
      "description": "The notion of interpolation and extrapolation is fundamental in various fields from deep learning to function approximation. Interpolation occurs for a sample  whenever this sample falls inside or on the boundary of the given dataset's convex hull. Extrapolation occurs when  falls outside of that convex hull. One fundamental (mis)conception is that state-of-the-art algorithms work so well because of their ability to correctly interpolate training data. A second (mis)conception is that interpolation happens throughout tasks and datasets, in fact, many intuitions and theories rely on that assumption. We empirically and theoretically argue against those two points and demonstrate that on any high-dimensional (100) dataset, interpolation almost surely never happens. Those results challenge the validity of our current interpolation/extrapolation definition as an indicator of generalization performances.",
      "citation_histogram": [
        [2020, 1],
        [2021, 3],
        [2022, 24]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:7Pm5v_kJJ6IC",
      "title": "Universum prescription: Regularization using unlabeled data",
      "link": "https://ojs.aaai.org/index.php/AAAI/article/view/10768",
      "year": 2017,
      "cited_by": 29,
      "authors": ["Xiang Zhang", "Yann LeCun"],
      "description": "This paper shows that simply prescribing\" none of the above\" labels to unlabeled data has a beneficial regularization effect to supervised learning. We call it universum prescription by the fact that the prescribed labels cannot be one of the supervised labels. In spite of its simplicity, universum prescription obtained competitive results in training deep convolutional networks for CIFAR-10, CIFAR-100, STL-10 and ImageNet datasets. A qualitative justification of these approaches using Rademacher complexity is presented. The effect of a regularization parameter\u2014probability of sampling from unlabeled data\u2014is also studied empirically.",
      "citation_histogram": [
        [2016, 1],
        [2017, 3],
        [2018, 4],
        [2019, 2],
        [2020, 12],
        [2021, 5]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:mWEH9CqjF64C",
      "title": "Causal graph-based video segmentation",
      "link": "https://ieeexplore.ieee.org/abstract/document/6738875/",
      "year": 2013,
      "cited_by": 29,
      "authors": ["Camille Couprie", "Cl\u00e9ment Farabet", "Yann LeCun"],
      "description": "Among the different methods producing superpixel segmentations of an image, the graph-based approach of Felzenszwalb and Huttenlocher is broadly employed. One of its interesting properties is that the regions are computed in a greedy manner in quasi-linear time by using a minimum spanning tree. The algorithm may be trivially extended to video segmentation by considering a video as a 3D volume, however, this can not be the case for causal segmentation, when subsequent frames are unknown. In a framework exploiting minimum spanning trees all along, we propose an efficient video segmentation approach that computes temporally consistent pixels in a causal manner, filling the need for causal and real time applications.",
      "citation_histogram": [
        [2013, 2],
        [2014, 6],
        [2015, 6],
        [2016, 6],
        [2017, 2],
        [2018, 3],
        [2019, 1],
        [2020, 1],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:tkaPQYYpVKoC",
      "title": "Unsupervised Learning of Feature Hierarchies",
      "link": "https://search.proquest.com/openview/1ad11ebef0d26847dda0f17356da0114/1?pq-origsite=gscholar&cbl=18750",
      "year": 2009,
      "cited_by": 28,
      "authors": ["Marc'Aurelio Ranzato"],
      "description": "The applicability of machine learning methods is often limited by the amount of available labeled data, and by the ability (or inability) of the designer to produce good internal representations and good similarity measures for the input data vectors. The aim of this thesis is to alleviate these two limitations by proposing algorithms to learn good internal representations, and invariant feature hierarchies from unlabeled data. These methods go beyond traditional supervised learning algorithms, and rely on unsupervised, and semi-supervised learning.",
      "citation_histogram": [
        [2010, 2],
        [2011, 2],
        [2012, 3],
        [2013, 2],
        [2014, 1],
        [2015, 4],
        [2016, 5],
        [2017, 1],
        [2018, 2],
        [2019, 1],
        [2020, 1],
        [2021, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:dhFuZR0502QC",
      "title": "Color documents on the web with DjVu",
      "link": "https://ieeexplore.ieee.org/abstract/document/821605/",
      "year": 1999,
      "cited_by": 28,
      "authors": [
        "Patrick Haffner",
        "Yann LeCun",
        "Leon Bottou",
        "Paul Howard",
        "Pascal Vincent",
        "Bill Riemers"
      ],
      "description": "We present a new image compression technique called \"DjVu\" that is specifically geared towards the compression of scanned documents in color at high resolution. With DjVu, a magazine page in color at 300 dpi typically occupies between 40 KB and 80 KB, approximately 5 to 10 times better than JPEG for a similar level of readability. Using a combination of hidden Markov model techniques and MDL-driven heuristics, DjVu first classifies each pixel in the image as either foreground (text, drawings) or background (pictures, photos, paper texture). The pixel categories form a bitonal image which is compressed using a pattern matching technique that takes advantage of the similarities between character shapes. A progressive, wavelet-based compression technique, combined with a masking algorithm, is then used to compress the foreground and background images at lower resolutions while minimizing the number\u00a0\u2026",
      "citation_histogram": [
        [2000, 1],
        [2001, 1],
        [2002, 2],
        [2003, 1],
        [2004, 1],
        [2005, 4],
        [2006, 6],
        [2007, 2],
        [2008, 5],
        [2009, 1],
        [2010, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:evX43VCCuoAC",
      "title": "Browsing through high quality document images with DjVu",
      "link": "https://ieeexplore.ieee.org/abstract/document/670431/",
      "year": 1998,
      "cited_by": 28,
      "authors": [
        "Patrick Haffner",
        "Leon Bottou",
        "Paul G Howard",
        "Patrice Simard",
        "Yoshua Bengio",
        "Yann LeCun"
      ],
      "description": "Presents a new image compression technique called \"DjVu\" that is specifically geared towards the compression of high-resolution, high-quality images of scanned documents in color. With DjVu, any screen connected to the Internet can access and display images of scanned pages while faithfully reproducing the font, color, drawings, pictures and paper texture. A typical magazine page in color at 300 dpi can be compressed down to between 40 to 60 KBytes, approximately 5 to 10 times better than JPEG for a similar level of subjective quality. Black-and-white documents are typically 15 to 30 KBytes at 300 dpi, or 4 to 8 times better than CCITT-G4. A real-time, memory-efficient version of the decoder was implemented, and is available as a plug-in for popular Web browsers.",
      "citation_histogram": [
        [1998, 1],
        [1999, 2],
        [2000, 1],
        [2001, 2],
        [2002, 1],
        [2003, 1],
        [2004, 4],
        [2005, 1],
        [2006, 3],
        [2007, 1],
        [2008, 3],
        [2009, 1],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 1],
        [2014, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:-pSblevFsSkC",
      "title": "others. 1995",
      "link": "https://scholar.google.com/scholar?cluster=17122956305966863245&hl=en&oi=scholarr",
      "year": 1995,
      "cited_by": 28,
      "authors": ["Yann LeCun", "Yoshua Bengio"],
      "description": null,
      "citation_histogram": [
        [2016, 1],
        [2017, 5],
        [2018, 5],
        [2019, 5],
        [2020, 4],
        [2021, 4],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:8aAMN6PqWdYC",
      "title": "D.: OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks arXiv",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.750.1547",
      "year": 2014,
      "cited_by": 27,
      "authors": [
        "Pierre Sermanet",
        "David Eigen",
        "Xiang Zhang",
        "Micha\u00ebl Mathieu",
        "Rob Fergus",
        "Yann LeCun"
      ],
      "description": "CiteSeerX \u2014 D.: OverFeat : Integrated Recognition , Localization and Detection using \nConvolutional Networks arXiv Documents Authors Tables Log in Sign up MetaCart DMCA \nDonate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced \nSearch Include Citations Tables: DMCA D.: OverFeat : Integrated Recognition , Localization \nand Detection using Convolutional Networks arXiv (2014) Cached Download as a PDF \nDownload Links [yann.lecun.com] [www.nvidia.co.uk] [arxiv.org] Save to List Add to Collection \nCorrect Errors Monitor Changes by Pierre Sermanet , David Eigen , Xiang Zhang , Michael \nMathieu , Rob Fergus , Yann Lecun Venue: 1312 . 6229v3 [ cs . CV ] 14 Citations: 168 - 15 self \nSummary Citations Active Bibliography Co-citation Clustered Documents Version History Share \nFacebook Twitter Reddit Bibsonomy OpenURL Abstract ar Powered by: Apache Solr About \u2026",
      "citation_histogram": [
        [2014, 1],
        [2015, 2],
        [2016, 1],
        [2017, 6],
        [2018, 2],
        [2019, 8],
        [2020, 2],
        [2021, 1],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:uJ-U7cs_P_0C",
      "title": "Handwritten digit recognition: Applications of neural net chips and automatic learning",
      "link": "https://link.springer.com/chapter/10.1007/978-3-642-76153-9_35",
      "year": 1990,
      "cited_by": 27,
      "authors": [
        "Y LeCun",
        "LD Hackel",
        "B Boser",
        "JS Denker",
        "HP Graf",
        "I Gyon",
        "D Henderson",
        "RE Howard",
        "W Hubbard"
      ],
      "description": " We describe two neural-net approaches to digit recognition. One method uses a neural-network chip to perform line thinning and local feature extraction. This preprocessing stage was designed by hand and did not involve any learning. However, automatic learning was used in the final classification step. The chip can process about 100 characters/sec, but the interface to the host computer limits the throughput to about 1 character/sec. The other method uses constrained automatic learning on pixel images with no preprocessing other than segmentation and size-normalization. It appears that good generalization performance cannot be obtained unless some a priori knowledge about the task is built into the system. This paper demonstrates how such knowledge can be integrated into a back-propagation network by providing hints and constraints on the architecture. Most of the computational burden\u00a0\u2026",
      "citation_histogram": [
        [1991, 1],
        [1992, 2],
        [1993, 1],
        [1994, 1],
        [1995, 1],
        [1996, 4],
        [1997, 3],
        [1998, 2],
        [1999, 3],
        [2000, 7],
        [2001, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:SnGPuo6Feq8C",
      "title": "Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients",
      "link": "https://arxiv.org/abs/1301.3764",
      "year": 2013,
      "cited_by": 26,
      "authors": ["Tom Schaul", "Yann LeCun"],
      "description": "Recent work has established an empirically successful framework for adapting learning rates for stochastic gradient descent (SGD). This effectively removes all needs for tuning, while automatically reducing learning rates over time on stationary problems, and permitting learning rates to grow appropriately in non-stationary tasks. Here, we extend the idea in three directions, addressing proper minibatch parallelization, including reweighted updates for sparse or orthogonal gradients, improving robustness on non-smooth loss functions, in the process replacing the diagonal Hessian estimation procedure that may not always be available by a robust finite-difference approximation. The final algorithm integrates all these components, has linear complexity and is hyper-parameter free.",
      "citation_histogram": [
        [2013, 3],
        [2014, 3],
        [2015, 1],
        [2016, 3],
        [2017, 4],
        [2018, 3],
        [2019, 3],
        [2020, 3],
        [2021, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:SjuI4pbJlxcC",
      "title": "Structured sparse coding via lateral inhibition",
      "link": "https://proceedings.neurips.cc/paper/2011/hash/fae0b27c451c728867a567e8c1bb4e53-Abstract.html",
      "year": 2011,
      "cited_by": 26,
      "authors": ["Arthur Szlam", "Karol Gregor", "Yann Cun"],
      "description": "This work describes a conceptually simple method for structured sparse coding and dictionary design. Supposing a dictionary with K atoms, we introduce a structure as a set of penalties or interactions between every pair of atoms. We describe modifications of standard sparse coding algorithms for inference in this setting, and describe experiments showing that these algorithms are efficient. We show that interesting dictionaries can be learned for interactions that encode tree structures or locally connected structures. Finally, we show that our framework allows us to learn the values of the interactions from the data, rather than having them pre-specified.",
      "citation_histogram": [
        [2014, 2],
        [2015, 3],
        [2016, 5],
        [2017, 6],
        [2018, 5],
        [2019, 3],
        [2020, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:R3hNpaxXUhUC",
      "title": "Discriminative feature and model design for automatic speech recognition.",
      "link": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.32.5640&rep=rep1&type=pdf",
      "year": 1997,
      "cited_by": 26,
      "authors": ["Mazin Rahim", "Yoshua Bengio", "Yann LeCun"],
      "description": "A system for discriminative feature and model design is presented for automatic speech recognition. Training based on minimum classi cation error with a single objective function is applied for designing a set of parallel networks performing feature transformation and a set of hidden Markov models performing speech recognition. This paper compares the use of linear and non-linear functional transformations when applied to conventional recognition features, such as spectrum or cepstrum. It also provides a framework for integrated feature and model training when using class-speci c transformations. Experimental results on telephone-based connected digit recognition are presented.",
      "citation_histogram": [
        [1998, 4],
        [1999, 3],
        [2000, 3],
        [2001, 2],
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 3],
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 1],
        [2012, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:25zTQ8aaf3EC",
      "title": "A spectral regularizer for unsupervised disentanglement",
      "link": "https://arxiv.org/abs/1812.01161",
      "year": 2018,
      "cited_by": 25,
      "authors": ["Aditya Ramesh", "Youngduck Choi", "Yann LeCun"],
      "description": "A generative model with a disentangled representation allows for independent control over different aspects of the output. Learning disentangled representations has been a recent topic of great interest, but it remains poorly understood. We show that even for GANs that do not possess disentangled representations, one can find curved trajectories in latent space over which local disentanglement occurs. These trajectories are found by iteratively following the leading right-singular vectors of the Jacobian of the generator with respect to its input. Based on this insight, we describe an efficient regularizer that aligns these vectors with the coordinate axes, and show that it can be used to induce disentangled representations in GANs, in a completely unsupervised manner.",
      "citation_histogram": [
        [2019, 1],
        [2020, 5],
        [2021, 9],
        [2022, 10]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:69ZgNCALVd0C",
      "title": "Pushing stochastic gradient towards second-order methods\u2013backpropagation learning with transformations in nonlinearities",
      "link": "https://link.springer.com/chapter/10.1007/978-3-642-42054-2_55",
      "year": 2013,
      "cited_by": 25,
      "authors": [
        "Tommi Vatanen",
        "Tapani Raiko",
        "Harri Valpola",
        "Yann LeCun"
      ],
      "description": " Recently, we proposed to transform the outputs of each hidden neuron in a multi-layer perceptron network to have zero output and zero slope on average, and use separate shortcut connections to model the linear dependencies instead. We continue the work by firstly introducing a third transformation to normalize the scale of the outputs of each hidden neuron, and secondly by analyzing the connections to second order optimization methods. We show that the transformations make a simple stochastic gradient behave closer to second-order optimization methods and thus speed up learning. This is shown both in theory and with experiments. The experiments on the third transformation show that while it further increases the speed of learning, it can also hurt performance by converging to a worse local optimum, where both the inputs and outputs of many hidden neurons are close to zero.",
      "citation_histogram": [
        [2012, 1],
        [2013, 2],
        [2014, 4],
        [2015, 4],
        [2016, 1],
        [2017, 3],
        [2018, 1],
        [2019, 3],
        [2020, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:maZDTaKrznsC",
      "title": "Adaptive long range vision in unstructured terrain",
      "link": "https://ieeexplore.ieee.org/abstract/document/4399622/",
      "year": 2007,
      "cited_by": 25,
      "authors": [
        "Ayse Naz Erkan",
        "Raia Hadsell",
        "Pierre Sermanet",
        "Jan Ben",
        "Urs Muller",
        "Yann LeCun"
      ],
      "description": "A novel probabilistic online learning framework for autonomous off-road robot navigation is proposed. The system is purely vision-based and is particularly designed for predicting traversability in unknown or rapidly changing environments. It uses self-supervised learning to quickly adapt to novel terrains after processing a small number of frames, and it can recognize terrain elements such as paths, man-made structures, and natural obstacles at ranges up to 30 meters. The system is developed on the LAGR mobile robot platform and the performance is evaluated using multiple metrics, including ground truth.",
      "citation_histogram": [
        [2007, 1],
        [2008, 3],
        [2009, 3],
        [2010, 5],
        [2011, 2],
        [2012, 2],
        [2013, 1],
        [2014, 1],
        [2015, 2],
        [2016, 2],
        [2017, 2],
        [2018, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:YFjsv_pBGBYC",
      "title": "A general segmentation scheme for DjVu document compression",
      "link": "https://books.google.com/books?hl=en&lr=&id=L1AYpP47NT8C&oi=fnd&pg=PA17&dq=info:OmJda_aEtfEJ:scholar.google.com&ots=eMgifg0Zd6&sig=1zxq-Pv84SjFGCl0xZN7vVGW7AA",
      "year": 2002,
      "cited_by": 25,
      "authors": ["Patrick Haffner", "Leon Bottou", "Yann Lecun", "L Vincent"],
      "description": "We describe the \u201cDjVu\u201d(D\u00e9j\u00e0 Vu) technology: an efficient document image compression methodology, a file format, and a delivery platform that together, enable instant access to high quality documents from essentially any platform, over any connection. Originally developed for scanned color documents, it was recently expanded to electronic docu-ments, so DjVu has now truly become a universal document interchange format.With DjVu, a color magazine page scanned at 300dpi typically occu-pies between 40KB and 80KB, ie approximately 5 to 10 times smaller than JPEG for a similar level of readability (the typical compression ratio is 500: 1). Converting electronic documents to DjVu also offers substantial advantages, as described in the paper. The technology re-lies on a classification of each pixel as either foreground (text, drawing)",
      "citation_histogram": [
        [2002, 1],
        [2003, 3],
        [2004, 2],
        [2005, 2],
        [2006, 2],
        [2007, 3],
        [2008, 2],
        [2009, 4],
        [2010, 2],
        [2011, 1],
        [2012, 2],
        [2013, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:RRCMsQZi3KMC",
      "title": "A theoretical argument for complex-valued convolutional networks",
      "link": "https://www.researchgate.net/profile/Joan-Bruna/publication/273471588_A_theoretical_argument_for_complex-valued_convolutional_networks/links/55afc70608ae32092e06e6ab/A-theoretical-argument-for-complex-valued-convolutional-networks.pdf",
      "year": 2015,
      "cited_by": 24,
      "authors": [
        "Joan Bruna",
        "Soumith Chintala",
        "Yann LeCun",
        "Serkan Piantino",
        "Arthur Szlam",
        "Mark Tygert"
      ],
      "description": "A complex-valued convolutional network (convnet) implements the repeated application of the following composition of three operations, recursively applying the composition to an input vector of nonnegative real numbers:(1) convolution with several complex-valued vectors followed by (2) taking the absolute value of every entry of the resulting vectors followed by (3) local averaging. For processing real-valued random vectors, complex-valued convnets can be viewed as \u201cdata-driven multiscale windowed power spectra,\u201d\u201cdata-driven multiscale windowed absolute spectra,\u201d\u201cdata-driven multiwavelet absolute values,\u201d or (in their most general configuration)\u201cdata-driven nonlinear multiwavelet packets.\u201d Indeed, complex-valued convnets can calculate multiscale windowed spectra when the convnet filters are windowed complex-valued exponentials. Standard real-valued convnets, using rectified linear units (ReLUs), sigmoidal (for example, logistic or tanh) nonlinearities, max. pooling, etc., do not obviously exhibit the same exact correspondence with data-driven wavelets (whereas for complex-valued convnets, the correspondence is much more than just a vague analogy).This note develops \u201cdata-driven multiscale windowed spectra\u201d for certain stochastic processes that are common in the modeling of time series (such as audio) and natural images (including patterns and textures). We motivate the construction of such multiscale spectra in the form of \u201clocal averages of multiwavelet absolute values\u201d or (in the most general configuration)\u201cnonlinear multiwavelet packets\u201d and connect these to certain \u201ccomplex-valued convolutional networks.\u201d A textbook\u00a0\u2026",
      "citation_histogram": [
        [2015, 1],
        [2016, 2],
        [2017, 4],
        [2018, 7],
        [2019, 2],
        [2020, 6],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:bgW0xdllhO4C",
      "title": "Learning about an exponential amount of conditional distributions",
      "link": "https://proceedings.neurips.cc/paper/2019/hash/5a0c828364dbf6dd406139dab7b25398-Abstract.html",
      "year": 2019,
      "cited_by": 23,
      "authors": [
        "Mohamed Belghazi",
        "Maxime Oquab",
        "Yann LeCun",
        "David Lopez-Paz"
      ],
      "description": "We introduce the Neural Conditioner (NC), a self-supervised machine able to learn about all the conditional distributions of a random vector X. The NC is a function NC (x\u22c5 a, a, r) that leverages adversarial training to match each conditional distribution P (Xr| Xa= xa). After training, the NC generalizes to sample from conditional distributions never seen, including the joint distribution. The NC is also able to auto-encode examples, providing data representations useful for downstream classification tasks. In sum, the NC integrates different self-supervised tasks (each being the estimation of a conditional distribution) and levels of supervision (partially observed data) seamlessly into a single learning experience.",
      "citation_histogram": [
        [2019, 2],
        [2020, 9],
        [2021, 9],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:oHSet2Z0r48C",
      "title": "Model-based planning in discrete action spaces",
      "link": "https://openreview.net/forum?id=YykAT_QgfKw",
      "year": 2017,
      "cited_by": 23,
      "authors": ["Mikael Henaff", "William F Whitney", "Yann LeCun"],
      "description": "Action planning using learned and differentiable forward models of the world is a general approach which has a number of desirable properties, including improved sample complexity over model-free RL methods, reuse of learned models across different tasks, and the ability to perform efficient gradient-based optimization in continuous action spaces. However, this approach does not apply straightforwardly when the action space is discrete. In this work, we show that it is in fact possible to effectively perform planning via backprop in discrete action spaces, using a simple paramaterization of the actions vectors on the simplex combined with input noise when training the forward model. Our experiments show that this approach can match or outperform model-free RL and discrete planning methods on gridworld navigation tasks in terms of performance and/or planning time while using limited environment interactions\u00a0\u2026",
      "citation_histogram": [
        [2017, 3],
        [2018, 7],
        [2019, 5],
        [2020, 2],
        [2021, 5],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:rn9M3PrGu7MC",
      "title": "Fast approximation of rotations and Hessians matrices",
      "link": "https://arxiv.org/abs/1404.7195",
      "year": 2014,
      "cited_by": 22,
      "authors": ["Michael Mathieu", "Yann LeCun"],
      "description": "A new method to represent and approximate rotation matrices is introduced. The method represents approximations of a rotation matrix  with linearithmic complexity, i.e. with  rotations over pairs of coordinates, arranged in an FFT-like fashion. The approximation is \"learned\" using gradient descent. It allows to represent symmetric matrices  as  where  is a diagonal matrix. It can be used to approximate covariance matrix of Gaussian models in order to speed up inference, or to estimate and track the inverse Hessian of an objective function by relating changes in parameters to changes in gradient along the trajectory followed by the optimization procedure. Experiments were conducted to approximate synthetic matrices, covariance matrices of real data, and Hessian matrices of objective functions involved in machine learning problems.",
      "citation_histogram": [
        [2014, 1],
        [2015, 1],
        [2016, 2],
        [2017, 5],
        [2018, 6],
        [2019, 4],
        [2020, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:NMxIlDl6LWMC",
      "title": "A multi-range vision strategy for autonomous offroad navigation",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.4055&rep=rep1&type=pdf",
      "year": 2007,
      "cited_by": 21,
      "authors": [
        "Raia Hadsell",
        "Ayse Erkan",
        "Pierre Sermanet",
        "Jan Ben",
        "Koray Kavukcuoglu",
        "Urs Muller",
        "Yann LeCun"
      ],
      "description": "Vision-based navigation and obstacle detection must be sophisticated in order to perform well in complicated and diverse terrain, but that complexity comes at the expense of increased system latency between image capture and actuator signals. Increased latency, or a longer control loop, degrades the reactivity of the robot. We present a navigational framework that uses a self-supervised, learningbased obstacle detector without paying a price in latency and reactivity. A long-range obstacle detector uses online learning to accurately see paths and obstacles at ranges up to 30 meters, while a fast, short-range obstacle detector avoids obstacles at up to 5 meters. The learning-based long-range module is discussed in detail, and field experiments are described which demonstrate the success of the overall system.",
      "citation_histogram": [
        [2007, 2],
        [2008, 1],
        [2009, 3],
        [2010, 2],
        [2011, 1],
        [2012, 2],
        [2013, 1],
        [2014, 1],
        [2015, 1],
        [2016, 1],
        [2017, 1],
        [2018, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:iH-uZ7U-co4C",
      "title": "Training system for neural networks",
      "link": "https://patents.google.com/patent/US5572628A/en",
      "year": 1996,
      "cited_by": 21,
      "authors": [
        "John S Denker",
        "Yann A LeCun",
        "Patrice Y Simard",
        "Bernard Victorri"
      ],
      "description": "In order for neural network technology to make useful determinations of the identity of letters and numbers that are processed in real time at a postal service sorting center, it is necessary for the neural network to\" learn\" to recognize accurately the many shapes and sizes in which each letter or number are formed on the address surface of the envelope by postal service users. It has been realized that accuracy in the recognition of many letters and numbers is not appreciably sacrificed if the neural network is instructed to identify those characteristics of each letter or number which are in the category\" invariant.\" Then, rather than requiring the neural network to recognize all gradations of shape, location, size, etc. of the identified invariant characteristic, a generalized and bounded description of the invariant segments is used which requires far less inputting of sample data and less processing of information relating to an\u00a0\u2026",
      "citation_histogram": [
        [1999, 2],
        [2000, 2],
        [2001, 1],
        [2002, 2],
        [2003, 5],
        [2004, 1],
        [2005, 1],
        [2006, 2],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:1zNUifcpCKoC",
      "title": "M\u00e9moires associatives distribu\u00e9es: une comparaison (distributed associative memories: a comparison)",
      "link": "https://nyuscholars.nyu.edu/en/publications/memoires-associatives-distribuees-une-comparaison-distributed-ass",
      "year": 1987,
      "cited_by": 21,
      "authors": [
        "Patrick Gallinari",
        "Yann Lecun",
        "Sylvie Thiria",
        "F Fogelman Soulie"
      ],
      "description": "Memoires associatives distribuees: Une comparaison (Distributed associative memories: A \ncomparison) \u2014 NYU Scholars Skip to main navigation Skip to search Skip to main content \nNYU Scholars Home NYU Scholars Logo Help & FAQ Home Profiles Research Units \nResearch output Search by expertise, name or affiliation Memoires associatives distribuees: \nUne comparaison (Distributed associative memories: A comparison) P. Gallinari, Yann \nLecun, S. Thiria, F. Fogelman Soulie Computer Science Research output: Chapter in Book/Report/Conference \nproceeding \u203a Conference contribution Overview Original language English (US) Title of host \npublication Proceedings of COGNITIVA 87, Paris, La Villette, May 1987 Publisher Cesta-Afcet \nState Published - 1987 Cite this APA Standard Harvard Vancouver Author BIBTEX RIS \nGallinari, P., Lecun, Y., Thiria, S., & Fogelman Soulie, F. (1987). Memoires associatives \u2026",
      "citation_histogram": [
        [2012, 1],
        [2013, 1],
        [2014, 2],
        [2015, 2],
        [2016, 2],
        [2017, 3],
        [2018, 5],
        [2019, 5]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:lQh10hhnIEIC",
      "title": "Prediction under uncertainty with error-encoding networks",
      "link": "https://arxiv.org/abs/1711.04994",
      "year": 2017,
      "cited_by": 20,
      "authors": ["Mikael Henaff", "Junbo Zhao", "Yann LeCun"],
      "description": "In this work we introduce a new framework for performing temporal predictions in the presence of uncertainty. It is based on a simple idea of disentangling components of the future state which are predictable from those which are inherently unpredictable, and encoding the unpredictable components into a low-dimensional latent variable which is fed into a forward model. Our method uses a supervised training objective which is fast and easy to train. We evaluate it in the context of video prediction on multiple datasets and show that it is able to consistently generate diverse predictions without the need for alternating minimization over a latent space or adversarial training.",
      "citation_histogram": [
        [2018, 4],
        [2019, 3],
        [2020, 4],
        [2021, 1],
        [2022, 8]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:j3f4tGmQtD8C",
      "title": "Efficient conversion of digital documents to multilayer raster formats",
      "link": "https://ieeexplore.ieee.org/abstract/document/953829/",
      "year": 2001,
      "cited_by": 20,
      "authors": ["L Bottou", "Patrick Haffner", "Yann LeCun"],
      "description": "How can we turn the description of a digital (i.e. electronically produced) document into something that is efficient for multi-layer raster formats? It is first shown that a foreground/background segmentation without overlapping foreground components can be more efficient for viewing or printing. Then, a new algorithm that prevents overlaps between foreground components while optimizing both the document quality and compression ratio is derived from the minimum description length (MDL) criterion. This algorithm makes the DjVu compression format significantly, more efficient on electronically produced documents. Comparisons with other formats are provided.",
      "citation_histogram": [
        [2003, 2],
        [2004, 1],
        [2005, 2],
        [2006, 1],
        [2007, 1],
        [2008, 3],
        [2009, 1],
        [2010, 1],
        [2011, 2],
        [2012, 1],
        [2013, 1],
        [2014, 1],
        [2015, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:BqipwSGYUEgC",
      "title": "An analog neural network processor and its application to high-speed character recognition.",
      "link": "http://yann.lecun.org/exdb/publis/pdf/boser-91.pdf",
      "year": 1991,
      "cited_by": 20,
      "authors": [
        "Bernhard E Boser",
        "Eduard S\u00e4ckinger",
        "Jane Bromley",
        "Yann LeCun",
        "Richard E Howard",
        "Lawrence D Jackel"
      ],
      "description": "Abst'ra. ct\u2014A high-speed programmable neural network chip and its application to character recognition are described. A network with over 130,000 connections has been implemented on a single chip and operates at a rate of over 1000 classi\ufb01cations per second. The chip performs up to 2000 multiplications and additions simultaneously. Its datapath is suitable particularly for the convolutional architectures that are typical in pattern classi\ufb01cation networks, but can also be con\ufb01gured for fully connected or feedback topologies. Computations are performed with 6Bits accuracy for the weights and 3Bits for the states. The chip uses analog processing internally for higher density and reduced power dissipation, but all input/output is digital to simplify system integration.",
      "citation_histogram": [
        [1991, 1],
        [1992, 3],
        [1993, 4],
        [1994, 1],
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 1],
        [1999, 1],
        [2000, 1],
        [2001, 2],
        [2002, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:j8SEvjWlNXcC",
      "title": "Hardware requirements for neural-net optical character recognition",
      "link": "https://ieeexplore.ieee.org/abstract/document/5726759/",
      "year": 1990,
      "cited_by": 20,
      "authors": [
        "Lawrence D Jackel",
        "B Boser",
        "John S Denker",
        "Hans Peter Graf",
        "Yann Le Cun",
        "Isabelle Guyon",
        "Donnie Henderson",
        "Richard E Howard",
        "W Hubbard",
        "Sara A Solla"
      ],
      "description": "Hardware architectures for character recognition are discussed, and choices for possible circuits are outlined. An advanced (and working) reconfigurable neural-net chip that mixes analog and digital processing is described. It is found that different approaches to image recognition often lead to neural-net architectures that have limited connectivity and repeated use of the same set of weights. This architecture is ideal for time-multiplexing (a combined parallel-series processing) on hardware systems that would be too small to evaluate the entire network in parallel. To make this process efficient, a chip needs to have shift registers to format the input data and additional registers to store intermediate results. Within this framework, it is possible to design chips that have broad utility, large connection capacity, and high speed. This was demonstrated by a new chip with 32000 reconfigurable connections",
      "citation_histogram": [
        [1991, 1],
        [1992, 4],
        [1993, 2],
        [1994, 3],
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 1],
        [1999, 3],
        [2000, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:OmZH4w0uHW8C",
      "title": "Optical character recognition and neural-net chips",
      "link": "https://link.springer.com/chapter/10.1007/978-94-009-0643-3_33",
      "year": 1990,
      "cited_by": 20,
      "authors": [
        "Y Le Cun",
        "LD Jackel",
        "HP Graf",
        "B Boser",
        "JS Denker",
        "I Guyon",
        "D Henderson",
        "RE Howard",
        "W Hubbard",
        "Sara A Solla"
      ],
      "description": " Neural Network research has always interested hardware designers, theoreticians, and application engineers. But until recently, the common ground between these groups was limited: the neural-net chips were too small to implement any full-size application, and the algorithms were too complicated (or the applications not interesting enough) to be implemented on a chip. The merging of these efforts is now made possible by the simultaneous emergence of powerful chips and successful, real-world applications of neural networks. Here, we discuss how the compute-intensive part of a handwritten digit recognizer, based on a highly structured backpropagation network, can be implemented on a general purpose neural-network chip containing 32k binary synapses. Using techniques based on the second-order properties of the error function, we show that very little accuracy on the weights and states is\u00a0\u2026",
      "citation_histogram": [
        [1991, 2],
        [1992, 8],
        [1993, 3],
        [1994, 1],
        [1995, 1],
        [1996, 1],
        [1997, 2],
        [1998, 1],
        [1999, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:MOUuoOoUJb4C",
      "title": "SN: A simulator for connectionist models",
      "link": "https://nyuscholars.nyu.edu/en/publications/sn-a-simulator-for-connectionist-models",
      "year": 1988,
      "cited_by": 20,
      "authors": ["L\u00e9on Bottou", "Yann LeCun"],
      "description": "Sn: A simulator for connectionist models \u2014 NYU Scholars Skip to main navigation Skip to \nsearch Skip to main content NYU Scholars Home NYU Scholars Logo Help & FAQ Home \nProfiles Research Units Research output Search by expertise, name or affiliation Sn: A \nsimulator for connectionist models Leon Bottou, Yann Lecun Computer Science Research \noutput: Chapter in Book/Report/Conference proceeding \u203a Conference contribution Overview \nOriginal language English (US) Title of host publication Proceedings of NeuroNimes 88, Nimes, \nFrance State Published - 1988 Cite this APA Standard Harvard Vancouver Author BIBTEX RIS \nBottou, L., & Lecun, Y. (1988). Sn: A simulator for connectionist models. In Proceedings of \nNeuroNimes 88, Nimes, France Sn: A simulator for connectionist models. / Bottou, Leon; Lecun, \nYann. Proceedings of NeuroNimes 88, Nimes, France. 1988. Research output: Chapter in Book/\u2026",
      "citation_histogram": [
        [1989, 6],
        [1990, 2],
        [1991, 1],
        [1992, 2],
        [1993, 1],
        [1994, 2],
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 1],
        [1999, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:osi8XriVlOYC",
      "title": "La plus belle histoire de l'intelligence: des origines aux neurones artificiels: vers une nouvelle \u00e9tape de l'\u00e9volution",
      "link": "https://scholar.google.com/scholar?cluster=11518359077177487207&hl=en&oi=scholarr",
      "year": 2018,
      "cited_by": 19,
      "authors": ["Stanislas Dehaene", "Yann Le Cun", "Jacques Girardon"],
      "description": "D'o\u00f9 vient l'intelligence? Est-elle une exclusivit\u00e9 humaine? Les machines peuvent-elles nous d\u00e9passer? Elle a \u00e9merg\u00e9 avec la vie, s' est d\u00e9velopp\u00e9e au fil de l'\u00e9volution, s' est magnifi\u00e9e avec l'esp\u00e8ce humaine... Gr\u00e2ce \u00e0 cette myst\u00e9rieuse intelligence, nous avons tout invent\u00e9: l'outil, le langage, l'\u00e9criture, l'\u00e9ducation, la science, et la facult\u00e9 de nous interroger sur le monde. Aujourd'hui, cette belle histoire conna\u00eet une r\u00e9volution sans pr\u00e9c\u00e9dent. Pour la premi\u00e8re fois, le cerveau humain peut visualiser son propre fonctionnement. Pour la premi\u00e8re fois, il transf\u00e8re une partie de son intelligence dans des machines capables d'apprentissage. Au fil d'un dialogue fascinant, le grand sp\u00e9cialiste du cerveau Stanislas Dehaene et celui des neurones artificiels Yann Le Cun racontent, avec Jacques Girardon, cette longue aventure, des origines animales \u00e0 nos jours, et s' interrogent sur notre futur. Les ordinateurs vont-ils bient\u00f4t \u00e9prouver des \u00e9motions, se doter d'une morale? L'art, la beaut\u00e9, la capacit\u00e9 d'improviser, d'anticiper, sont-ils \u00e0 la port\u00e9e de cerveaux immat\u00e9riels? Ce que les auteurs esquissent ici, ce n'est rien moins que la prochaine \u00e9tape de notre \u00e9volution. \u00c0 l'\u00e9vidence, la lecture d'un tel livre change d\u00e9j\u00e0 radicalement le regard que nous portons sur nous-m\u00eames.",
      "citation_histogram": [
        [2019, 3],
        [2020, 2],
        [2021, 7],
        [2022, 7]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:l1jknz_x7mgC",
      "title": "L\u2019apprentissage profond, une r\u00e9volution en intelligence artificielle",
      "link": "https://journals.openedition.org/lettre-cdf/3227",
      "year": 2016,
      "cited_by": 19,
      "authors": ["Yann LeCun"],
      "description": "Yann LeCun, sp\u00e9cialiste de l\u2019apprentissage automatique des machines (machine learning), est l\u2019un des p\u00e8res du Deep Learning (apprentissage profond), une m\u00e9thode \u00e0 laquelle il se consacre depuis trente ans, malgr\u00e9 le scepticisme qu\u2019il rencontre au d\u00e9part dans la communaut\u00e9 scientifique.  Le Deep Learning, qui fait appel \u00e0 la fois aux connaissances en neurosciences, aux math\u00e9matiques et aux progr\u00e8s technologiques, est aujourd\u2019hui pl\u00e9biscit\u00e9 comme une v\u00e9ritable r\u00e9volution dans le domaine de l\u2019in...",
      "citation_histogram": [
        [2017, 3],
        [2018, 1],
        [2019, 4],
        [2020, 5],
        [2021, 4],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:_OG5zn83XeQC",
      "title": "Decoupled contrastive learning",
      "link": "https://arxiv.org/abs/2110.06848",
      "year": 2021,
      "cited_by": 18,
      "authors": [
        "Chun-Hsiao Yeh",
        "Cheng-Yao Hong",
        "Yen-Chi Hsu",
        "Tyng-Luh Liu",
        "Yubei Chen",
        "Yann LeCun"
      ],
      "description": "Contrastive learning (CL) is one of the most successful paradigms for self-supervised learning (SSL). In a principled way, it considers two augmented \"views\" of the same image as positive to be pulled closer, and all other images negative to be pushed further apart. However, behind the impressive success of CL-based techniques, their formulation often relies on heavy-computation settings, including large sample batches, extensive training epochs, etc. We are thus motivated to tackle these issues and aim at establishing a simple, efficient, and yet competitive baseline of contrastive learning. Specifically, we identify, from theoretical and empirical studies, a noticeable negative-positive-coupling (NPC) effect in the widely used cross-entropy (InfoNCE) loss, leading to unsuitable learning efficiency with respect to the batch size. Indeed the phenomenon tends to be neglected in that optimizing infoNCE loss with a small-size batch is effective in solving easier SSL tasks. By properly addressing the NPC effect, we reach a decoupled contrastive learning (DCL) objective function, significantly improving SSL efficiency. DCL can achieve competitive performance, requiring neither large batches in SimCLR, momentum encoding in MoCo, or large epochs. We demonstrate the usefulness of DCL in various benchmarks, while manifesting its robustness being much less sensitive to suboptimal hyperparameters. Notably, our approach achieves  ImageNet top-1 accuracy using batch size 256 within 200 epochs pre-training, outperforming its baseline SimCLR by . With further optimized hyperparameters, DCL can improve the accuracy to . We believe\u00a0\u2026",
      "citation_histogram": [
        [2021, 1],
        [2022, 17]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:2POsdo3tyOAC",
      "title": "Guest editorial: Deep learning",
      "link": "https://link.springer.com/article/10.1007/s11263-015-0813-1",
      "year": 2015,
      "cited_by": 18,
      "authors": ["Marc\u2019Aurelio Ranzato", "Geoffrey Hinton", "Yann LeCun"],
      "description": "Deep Learning methods aim at learning feature hierarchies. Applications of deep learning to vision tasks date back to convolutional networks in the early 1990s. These methods have been the subject of a recent surge of interest for two main reasons: when labeled data is scarce, unsupervised learning algorithms can learn useful feature hierarchies. When labeled data is abundant, supervised methods can be used to train very large networks on very large datasets through the use of high-performance computers. Such large networks have been shown to outperform previous state-of-theart methods on several perceptual tasks, including categorylevel object recognition, object detection and semantic segmentation.In \u201cStacked Predictive Sparse Decomposition for Classification of Histology Sections\u201d(doi: 10.1007/s11263-014-0790-9) the authors propose the use of an unsupervised feature learning algorithm for the\u00a0\u2026",
      "citation_histogram": [
        [2016, 6],
        [2017, 2],
        [2018, 2],
        [2019, 1],
        [2020, 1],
        [2021, 5],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:XiVPGOgt02cC",
      "title": "Traffic signs and pedestrians vision with multi-scale convolutional networks",
      "link": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.387.2598&rep=rep1&type=pdf",
      "year": 2011,
      "cited_by": 18,
      "authors": ["Pierre Sermanet", "Koray Kavukcuoglu", "Yann LeCun"],
      "description": "Convolutional Networks (ConvNets) are biologically-inspired multi-stage architectures that automatically learn hierarchies of invariant features. While many popular vision approaches use hand-crafted features such as HOG or SIFT, ConvNets learn features at every levels from data that are tuned to the task at hand. The traditional ConvNet architecture was modified by feeding 1st stage features in addition to 2nd stage features to the classifier. We apply these multi-scale ConvNets to the tasks of traffic sign classification and pedestrian detection and establish new accuracy records, above human performance for road signs. We also show an significant accuracy gain on the pedestrian task when using unsupervised pre-training with Convolutional Predictive Sparse Coding [1](ConvPSD). The ConvNet was implemented using the EBLearn C++ open-source package 1 [2].",
      "citation_histogram": [
        [2012, 2],
        [2013, 1],
        [2014, 4],
        [2015, 4],
        [2016, 2],
        [2017, 1],
        [2018, 2],
        [2019, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:p6f6DfXMsGMC",
      "title": "Handwritten digit recognition with a back-propagation network, 1989",
      "link": "https://scholar.google.com/scholar?cluster=5405317029112171719&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 18,
      "authors": [
        "Y LeCun",
        "B Boser",
        "J Denker",
        "D Henderson",
        "RE Howard",
        "W Hubbard",
        "LD Jackel"
      ],
      "description": null,
      "citation_histogram": [
        [2000, 1],
        [2001, 2],
        [2002, 1],
        [2003, 11],
        [2004, 1],
        [2005, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:sFUlmsclzkgC",
      "title": "What\u2019s wrong with deep learning",
      "link": "https://www.researchgate.net/profile/Sreenivas-Sremath-Tirumala/post/How-do-artificial-neural-network-plays-a-role-in-image-processing/attachment/59d6209879197b807797f10d/AS%3A291571766448128%401446527534428/download/Lecun+Deep+learning+keynote+2015.pdf",
      "year": 2015,
      "cited_by": 16,
      "authors": ["Yann LeCun"],
      "description": "What's Wrong With Deep Learning? Page 1 Y LeCun What's Wrong With Deep Learning? \nYann LeCun Facebook AI Research & Center for Data Science, NYU yann@cs.nyu.edu http://yann.lecun.com \nPage 2 Y LeCun Plan The motivation for ConvNets and Deep Learning: end-to-end learning \nIntegrating feature extractor, classifier, contextual post-processor A bit of archeology: ideas that \nhave been around for a while Kernels with stride, non-shared local connections, metric \nlearning... \u201cfully convolutional\u201d training What's missing from deep learning? 1. Theory 2. \nReasoning, structured prediction 3. Memory, short-term/working/episodic memory 4. \nUnsupervised learning that actually works PostProcessor Low-Level Features More Features \nClassifier Page 3 Y LeCun Deep Learning = Learning Hierarchical Representations Traditional \nPattern Recognition: Fixed/Handcrafted Feature Extractor Trainable Classifier Feature \u2026",
      "citation_histogram": [
        [2015, 1],
        [2016, 4],
        [2017, 3],
        [2018, 4],
        [2019, 3],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:JmU_-KX0ghQC",
      "title": "Toward real-time indoor semantic segmentation using depth information",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.431.9275&rep=rep1&type=pdf",
      "year": 2014,
      "cited_by": 16,
      "authors": [
        "Camille Couprie",
        "Cl\u00e9ment Farabet",
        "Laurent Najman",
        "Yann LeCun"
      ],
      "description": "This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on handcrafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. Using a frame by frame labeling, we obtain nearly state-of-the-art performance on the NYU-v2 depth dataset with an accuracy of 64.5%. We then show that the labeling can be further improved by exploiting the temporal consistency in the video sequence of the scene. To that goal, we present a method producing temporally consistent superpixels from a streaming video. Among the different methods producing superpixel segmentations of an image, the graph-based approach of Felzenszwalb and Huttenlocher is broadly employed. One of its interesting properties is that the regions are computed in a greedy manner in quasi-linear time by using a minimum spanning tree. In a framework exploiting minimum spanning trees all along, we propose an efficient video segmentation approach that computes temporally consistent pixels in a causal manner, filling the need for causal and real-time applications. We illustrate the labeling of indoor scenes in video sequences that could be processed in real-time using appropriate hardware such as an FPGA.",
      "citation_histogram": [
        [2015, 1],
        [2016, 4],
        [2017, 3],
        [2018, 1],
        [2019, 2],
        [2020, 2],
        [2021, 2],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:-95Q15plzcUC",
      "title": "On-line recognition of limited-vocabulary Chinese character using multiple convolutional neural networks",
      "link": "https://ieeexplore.ieee.org/abstract/document/394256/",
      "year": 1993,
      "cited_by": 16,
      "authors": ["Q-Z Wu", "YL Cun", "Lawrence D Jackel", "B-S Jeng"],
      "description": "The authors present a new feature extraction method together with neural network recognition for online Chinese characters. A Chinese character can be represented by a three-dimensional 12 /spl times/ 12 /spl times/ 4 array of numbers. Multiple conventional neural networks are used for online small vocabulary Chinese character recognition based on this feature extraction method. One hundred character classes were chosen as an example for recognition. Simulation results show that 98.8% and 94.2% of training examples and test examples were correctly recognized respectively.< >",
      "citation_histogram": [
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 1],
        [1999, 2],
        [2000, 2],
        [2001, 4],
        [2002, 2],
        [2003, 1],
        [2004, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:4UtermoNRQAC",
      "title": "Implicit Rank-Minimizing Autoencoder",
      "link": "https://proceedings.neurips.cc/paper/2020/hash/a9078e8653368c9c291ae2f8b74012e7-Abstract.html",
      "year": 2020,
      "cited_by": 15,
      "authors": ["Li Jing", "Jure Zbontar", "Yann LeCun"],
      "description": "An important component of autoencoder methods is the method by which the information capacity of the latent representation is minimized or limited. In this work, the rank of the covariance matrix of the codes is implicitly minimized by relying on the fact that gradient descent learning in multi-layer linear networks leads to minimum-rank solutions. By inserting a number of extra linear layers between the encoder and the decoder, the system spontaneously learns representations with a low effective dimension. The model, dubbed Implicit Rank-Minimizing Autoencoder (IRMAE), is simple, deterministic, and learns continuous latent space. We demonstrate the validity of the method on several image generation and representation learning tasks.",
      "citation_histogram": [
        [2020, 1],
        [2021, 6],
        [2022, 8]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:aIdbFUkbNIkC",
      "title": "Learning maneuver dictionaries for ground robot planning",
      "link": "https://www.researchgate.net/profile/Yann-Lecun/publication/216792715_Learning_Maneuver_Dictionaries_for_Ground_Robot_Planning/links/00b7d514af9ef3f61b000000/Learning-Maneuver-Dictionaries-for-Ground-Robot-Planning.pdf",
      "year": 2008,
      "cited_by": 15,
      "authors": [
        "Pierre Sermanet",
        "Marco Scoffier",
        "Chris Crudele",
        "Urs Muller",
        "Yann LeCun"
      ],
      "description": "Vehicle dynamics is typically handled by models whose parameters are found through system identification or manually computed from the vehicle\u2019s characteristics. While these methods provide accurate theoretical dynamical models, they may not take into account differences between individual vehicles, lack adaptability to new environments and may not handle sophisticated models, requiring hand-crafted heuristics for backwards motion for example. Similarly to space and aerial maneuver-based planning methods, we demonstrate a simple and computationally fast planning method for ground robots with obstacle avoidance. It bypasses the need for model parameters identification and hand-crafted heuristics, learns the particularities of individual vehicles, allows on-line adaptation and sophisticated models. Human-driven or autonomouslydriven trajectories are recorded and stored into a trajectory bank. While in learning mode, the robot records each traveled trajectory and places it into a bank, indexed by the initial speeds of each left and right wheels and the ending position at a fixed radius. Only the best trajectories are stored in the trajectory bank and then reused during autonomous runs for optimal short-range planning. Pre-computed (but not recorded) trajectories have been used in previous work and provide an important computational advantage over on-line computation methods, which are less practical in real-time applications due to the high-dimensional search space. A collision-free platform was developed without any hand-crafted heuristics or knowledge about the vehicle\u2019s characteristics. This method is demonstrated on the\u00a0\u2026",
      "citation_histogram": [
        [2008, 1],
        [2009, 1],
        [2010, 2],
        [2011, 1],
        [2012, 3],
        [2013, 1],
        [2014, 1],
        [2015, 1],
        [2016, 1],
        [2017, 1],
        [2018, 1],
        [2019, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:TGemctCAZTQC",
      "title": "Une Procedure d\u015bapprentissage pour reseau a seuil assymetrique cog'nitiva 85: A la Frontiere de lqIntelligence Artificielle des Sciences de la Connais' sance des Neurosciences",
      "link": "https://scholar.google.com/scholar?cluster=4516087311233330491&hl=en&oi=scholarr",
      "year": 1985,
      "cited_by": 15,
      "authors": ["Y Lecun"],
      "description": null,
      "citation_histogram": [
        [1988, 2],
        [1989, 1],
        [1990, 1],
        [1991, 1],
        [1992, 1],
        [1993, 1],
        [1994, 1],
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:pOP5Rf-i_loC",
      "title": "System and method for biometric authentication in connection with camera equipped devices",
      "link": "https://patents.google.com/patent/US10135815B2/en",
      "year": 2018,
      "cited_by": 14,
      "authors": ["Yann Lecun", "Adam Perold", "Yang Wang", "Sagar Waghmare"],
      "description": "The present invention relates generally to the use of biometric technology for authentication and identification, and more particularly to non-contact based solutions for authenticating and identifying users, via computers, such as mobile devices, to selectively permit or deny access to various resources. In the present invention authentication and/or identification is performed using an image or a set of images of an individual's palm through a process involving the following key steps:(1) detecting the palm area using local classifiers;(2) extracting features from the region (s) of interest; and (3) computing the matching score against user models stored in a database, which can be augmented dynamically through a learning process.",
      "citation_histogram": [
        [2018, 4],
        [2019, 2],
        [2020, 2],
        [2021, 2],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:UmS_249rOGwC",
      "title": "Real-time adaptive off-road vehicle navigation and terrain classification",
      "link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/8741/87410A/Real-time-adaptive-off-road-vehicle-navigation-and-terrain-classification/10.1117/12.2015533.short",
      "year": 2013,
      "cited_by": 14,
      "authors": [
        "Urs A Muller",
        "Lawrence D Jackel",
        "Yann LeCun",
        "Beat Flepp"
      ],
      "description": "We are developing a complete, self-contained autonomous navigation system for mobile robots that learns quickly, uses commodity components, and has the added benefit of emitting no radiation signature. It builds on the autonomous navigation technology developed by Net-Scale and New York University during the Defense Advanced Research Projects Agency (DARPA) Learning Applied to Ground Robots (LAGR) program and takes advantage of recent scientific advancements achieved during the DARPA Deep Learning program. In this paper we will present our approach and algorithms, show results from our vision system, discuss lessons learned from the past, and present our plans for further advancing vehicle autonomy.",
      "citation_histogram": [
        [2014, 1],
        [2015, 1],
        [2016, 1],
        [2017, 4],
        [2018, 4],
        [2019, 2],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:XzWLPxS1ir4C",
      "title": "Computer Vision\u2013ECCV 2010",
      "link": "https://scholar.google.com/scholar?cluster=10346211466182162356&hl=en&oi=scholarr",
      "year": 2010,
      "cited_by": 14,
      "authors": ["GW Taylor", "R Fergus", "Y LeCun", "C Bregler"],
      "description": null,
      "citation_histogram": [
        [2014, 2],
        [2015, 6],
        [2016, 1],
        [2017, 2],
        [2018, 1],
        [2019, 1],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=200&pagesize=100&citation_for_view=WLN3QrAAAAAJ:yB1At4FlUx8C",
      "title": "Graph transformer networks for image recognition",
      "link": "https://www.isi-web.org/isi.cbs.nl/iamamember/CD6-Sydney2005/ISI2005_Presentations/isi2005.pdf",
      "year": 2005,
      "cited_by": 14,
      "authors": ["L\u00e9on Bottou", "Yann LeCun"],
      "description": "Graph Transformer Networks for Image Recognition Page 1 Graph Transformer Networks for \nImage Recognition L\u00e9on Bottou \u2020 and Yann LeCun \u2021 \u2020 NEC Labs America, Princeton NJ \u2021 The \nCourant Institute, New York University, NY April 5, 2005 Page 2 Objectives Modeling issues in \nComplex Learning Systems. 1. Model Composition in Learning Systems. 2. Generative models \nand their limitations. 3. Discriminative models. 4. Model alphabet. 5. Examples. Page 3 Simple \nLearning Systems Character recognition network. (LeCun et al.) INPUT 32x32 Convolutions \nSubsampling Convolutions Subsampling Full connection Full connection Gaussian \nconnections feature maps 6@28x28 f. maps 6@14x14 f. maps 16@10x10 f. maps 16@5x5 \nlayer 120 OUTPUT 95 layer 84 Gives a score p(y|x) to class y given pixels x. Non linear model \nwith 60000 scalar parameters. Fitted with 500000 examples. Page 4 Complex Learning \u2026",
      "citation_histogram": [
        [2010, 2],
        [2011, 1],
        [2012, 1],
        [2013, 1],
        [2014, 3],
        [2015, 2],
        [2016, 1],
        [2017, 2],
        [2018, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:GnPB-g6toBAC",
      "title": "Method and apparatus for image segmentation",
      "link": "https://patents.google.com/patent/US5253304A/en",
      "year": 1993,
      "cited_by": 14,
      "authors": [
        "Yann A LeCun",
        "Ofer Matan",
        "William D Satterfield",
        "Timothy J Thompson"
      ],
      "description": "Segmentation of characters in a character set (10), made by placing a dark mark against a light background (12), is accomplished by establishing a vertical pixel projection for each pixel column in the image. The vertical pixel projections are filtered with a decay parameter so those pixel columns which contain only background have the highest projection. Thereafter, a set of\" cut-points\"(points of image segmentation) is obtained so that each cut-point coincides with a pixel column whose vertical pixel projection is both a local maxima and exceeds a predetermined threshold. The number of such cut-points is counted and if the number is not significantly greater than a predetermined number, the image is segmented along the cut-points. Otherwise, the vertical projections of those pixel columns coincident with the cut-points are filtered with a decreasing threshold to reduce the number of potential cut-points.",
      "citation_histogram": [
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 1],
        [1999, 1],
        [2000, 1],
        [2001, 1],
        [2002, 1],
        [2003, 2],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:RpHLKABnwqoC",
      "title": "Generalization and network design strategies: technical report CRG-TR-89-4",
      "link": "https://scholar.google.com/scholar?cluster=1856233219444901696&hl=en&oi=scholarr",
      "year": 1989,
      "cited_by": 14,
      "authors": ["Y LeCun"],
      "description": null,
      "citation_histogram": [
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 1],
        [1999, 2],
        [2000, 4],
        [2001, 1],
        [2002, 2],
        [2003, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:VofzgsFG4o0C",
      "title": "Source separation with scattering non-negative matrix factorization",
      "link": "https://ieeexplore.ieee.org/abstract/document/7178296/",
      "year": 2015,
      "cited_by": 13,
      "authors": ["Joan Bruna", "Pablo Sprechmann", "Yann LeCun"],
      "description": "This paper presents a single-channel source separation method that extends the ideas of Nonnegative Matrix Factorization (NMF). We interpret the approach of audio demixing via NMF as a cascade of a pooled analysis operator, given for example by the magnitude spectrogram, and a synthesis operators given by the matrix decomposition. Instead of imposing the temporal consistency of the decomposition through sophisticated structured penalties in the synthesis stage, we propose to change the analysis operator for a deep scattering representation, where signals are represented at several time resolutions. This new signal representation is invariant to smooth changes in the signal, consistent with its temporal dynamics. We evaluate the proposed approach in a speech separation task obtaining promising results.",
      "citation_histogram": [
        [2015, 4],
        [2016, 5],
        [2017, 2],
        [2018, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:iomT83CKXisC",
      "title": "Neural information processing scaled for bioacoustics, from neurons to big data",
      "link": "https://www.researchgate.net/profile/Julie-Elie/publication/307634857_Data_driven_approaches_for_identifying_information_bearing_features_in_communication_calls/links/57ce267208aed67896ffcab1/Data-driven-approaches-for-identifying-information-bearing-features-in-communication-calls.pdf",
      "year": 2013,
      "cited_by": 13,
      "authors": [
        "H Glotin",
        "Y LeCun",
        "T Artieres",
        "S Mallat",
        "O Tchernichovski",
        "X Halkias"
      ],
      "description": "Bioacousticians have traditionally investigated the acoustical nature of information bearing features in communication calls by describing sounds using a small number of acoustical parameters that appear particularly salient (eg the mean frequency, duration, spectral balance). These measures are then used as parameters for linear discriminant analyses (LDA) or other supervised learning approaches to investigate what acoustic parameters drive sound categorization. This classical approach is computationally efficient and yields results that are easily interpretable. However this approach can also be limited by the a priori choice of the putative information bearing features: as long as the sound representation is not complete, one will not be able to determine whether the correct information bearing features are identified and, thus, whether the actual amount of information present in the calls(measured for example as the quality of a discrimination test) is correctly estimated.To address this issue, we have adopted a data driven approach. In the traditional approach, specific acoustical parameters are chosen for two reasons: for dimensionality reduction and for the implementation of a non-\u2010linear transformation that could be required for linear discriminant approaches to effectively discriminate among sound categories. These two steps, however, can be implemented without a priori assumptions or loss of information. In our approach, our non-\u2010linear transformation is an invertible spectrographic representation of the sound. Then before using a classifier, the dimension of this high-\u2010dimensional representation is reduced using principal component\u00a0\u2026",
      "citation_histogram": [
        [2014, 1],
        [2015, 2],
        [2016, 1],
        [2017, 2],
        [2018, 1],
        [2019, 2],
        [2020, 3],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:1qzjygNMrQYC",
      "title": "Learning representations by maximizing compression",
      "link": "https://arxiv.org/abs/1108.1169",
      "year": 2011,
      "cited_by": 13,
      "authors": ["Karol Gregor", "Yann LeCun"],
      "description": "We give an algorithm that learns a representation of data through compression. The algorithm 1) predicts bits sequentially from those previously seen and 2) has a structure and a number of computations similar to an autoencoder. The likelihood under the model can be calculated exactly, and arithmetic coding can be used directly for compression. When training on digits the algorithm learns filters similar to those of restricted boltzman machines and denoising autoencoders. Independent samples can be drawn from the model by a single sweep through the pixels. The algorithm has a good compression performance when compared to other methods that work under random ordering of pixels.",
      "citation_histogram": [
        [2014, 2],
        [2015, 3],
        [2016, 3],
        [2017, 1],
        [2018, 2],
        [2019, 1],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:oC1yQlCKEqoC",
      "title": "The effects of regularization and data augmentation are class dependent",
      "link": "https://arxiv.org/abs/2204.03632",
      "year": 2022,
      "cited_by": 12,
      "authors": ["Randall Balestriero", "Leon Bottou", "Yann LeCun"],
      "description": "Regularization is a fundamental technique to prevent over-fitting and to improve generalization performances by constraining a model's complexity. Current Deep Networks heavily rely on regularizers such as Data-Augmentation (DA) or weight-decay, and employ structural risk minimization, i.e. cross-validation, to select the optimal regularization hyper-parameters. In this study, we demonstrate that techniques such as DA or weight decay produce a model with a reduced complexity that is unfair across classes. The optimal amount of DA or weight decay found from cross-validation leads to disastrous model performances on some classes e.g. on Imagenet with a resnet50, the \"barn spider\" classification test accuracy falls from  to  only by introducing random crop DA during training. Even more surprising, such performance drop also appears when introducing uninformative regularization techniques such as weight decay. Those results demonstrate that our search for ever increasing generalization performance -- averaged over all classes and samples -- has left us with models and regularizers that silently sacrifice performances on some classes. This scenario can become dangerous when deploying a model on downstream tasks e.g. an Imagenet pre-trained resnet50 deployed on INaturalist sees its performances fall from  to  on class \\#8889 when introducing random crop DA during the Imagenet pre-training phase. Those results demonstrate that designing novel regularizers without class-dependent bias remains an open research question.",
      "citation_histogram": [[2022, 12]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:0izwh0c-50kC",
      "title": "Self-supervised learning",
      "link": "https://scholar.google.com/scholar?cluster=11787820473360853806&hl=en&oi=scholarr",
      "year": 2020,
      "cited_by": 12,
      "authors": ["Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [2019, 1],
        [2020, 4],
        [2021, 5],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:bGhaNo82unsC",
      "title": "Deep learning est mort. vive differentiable programming",
      "link": "https://scholar.google.com/scholar?cluster=15646274805982104712&hl=en&oi=scholarr",
      "year": 2018,
      "cited_by": 12,
      "authors": ["Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [2018, 1],
        [2019, 3],
        [2020, 7],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:abCsMXLaarkC",
      "title": "Adversarially regularized autoencoders for generating discrete structures",
      "link": "https://scholar.google.com/scholar?cluster=5025238780002623841&hl=en&oi=scholarr",
      "year": 2017,
      "cited_by": 12,
      "authors": ["Zhao Junbo", "Y Kim", "K Zhang", "AM Rush", "Y LeCun"],
      "description": null,
      "citation_histogram": [
        [2017, 2],
        [2018, 2],
        [2019, 1],
        [2020, 3],
        [2021, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:r90dRYkPj7EC",
      "title": "et HINTON, Geoffrey",
      "link": "https://scholar.google.com/scholar?cluster=3955488421087487875&hl=en&oi=scholarr",
      "year": 2015,
      "cited_by": 12,
      "authors": ["Yann LECUN", "Yoshua BENGIO"],
      "description": null,
      "citation_histogram": [
        [2019, 8],
        [2020, 1],
        [2021, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:hMod-77fHWUC",
      "title": "Method and apparatus for standardization of inputs to word recognition systems",
      "link": "https://patents.google.com/patent/US5774586A/en",
      "year": 1998,
      "cited_by": 12,
      "authors": ["Yann Andre LeCun"],
      "description": "Groups of symbols to be recognized are standardized by fitting four flexible curves to the group of symbols. The curves are fitted by minimizing a cost or energy function that associates a cost with the curvature of the curves, the slant of the curves, the displacement in spacing between the curves and the distance of maxima and minima points from the curves. After the curves are fitted to the group of symbols, the symbols are standardized by transforming coordinates systems so that the fitted curves are placed in a predetermined configuration.",
      "citation_histogram": [
        [2000, 1],
        [2001, 3],
        [2002, 1],
        [2003, 2],
        [2004, 3],
        [2005, 1],
        [2006, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:O3NaXMp0MMsC",
      "title": "Method and apparatus for symbol recognition using multidimensional preprocessing at multiple resolutions",
      "link": "https://patents.google.com/patent/US5337372A/en",
      "year": 1994,
      "cited_by": 12,
      "authors": ["Yann A LeCun", "Quen-Zong Wu"],
      "description": "Data samples describing a plurality of micro-segments that compose a symbol to be recognized are received from a device such as an electronic pad. Preprocessors map the micro-segments into cells of a plurality of feature arrays with different resolutions. Preprocessors assign values to the cells based on the length of a micro-segment associated with the cell, and how well the features of the associated micro-segment correspond to the feature label of the cell. The cell values are used as inputs to comparators that compare the feature arrays with reference arrays. The results of comparisons involving lower resolution feature arrays and reference arrays, are used to limit the number of comparisons involving higher resolution feature arrays and reference arrays. The highest resolution comparison selects the reference array that identifies the symbol to be recognized.",
      "citation_histogram": [
        [1996, 1],
        [1997, 1],
        [1998, 2],
        [1999, 2],
        [2000, 1],
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:c5LcigzBm8MC",
      "title": "D. henderson, RE Howard, W. hubbard, and LJ Jackel",
      "link": "https://scholar.google.com/scholar?cluster=2528211939740507220&hl=en&oi=scholarr",
      "year": 1990,
      "cited_by": 12,
      "authors": ["Y LeCun", "B Boser", "JS Denker"],
      "description": null,
      "citation_histogram": [
        [1995, 1],
        [1996, 2],
        [1997, 1],
        [1998, 1],
        [1999, 1],
        [2000, 2],
        [2001, 1],
        [2002, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:fEOibwPWpKIC",
      "title": "A time delay neural network character recognizer for a touch terminal",
      "link": "https://nyuscholars.nyu.edu/en/publications/a-time-delay-neural-network-character-recognizer-for-a-touch-term",
      "year": 1990,
      "cited_by": 12,
      "authors": [
        "Isabelle Guyon",
        "P Albrecht",
        "Yann LeCun",
        "John S Denker",
        "Wayne Hubbard"
      ],
      "description": "A time delay neural network character recognizer for a touch terminal \u2014 NYU Scholars Skip to \nmain navigation Skip to search Skip to main content NYU Scholars Home NYU Scholars Logo \nHelp & FAQ Home Profiles Research Units Research output Search by expertise, name or \naffiliation A time delay neural network character recognizer for a touch terminal I. Guyon, P. \nAlbrecht, Yann Lecun, JS Denker, W. Hubbard Computer Science Research output: Chapter in \nBook/Report/Conference proceeding \u203a Conference contribution Overview Original language \nEnglish (US) Title of host publication Proceedings of the International Neural Network \nConference, Paris, June 1990 State Published - 1990 Cite this APA Standard Harvard Vancouver \nAuthor BIBTEX RIS Guyon, I., Albrecht, P., Lecun, Y., Denker, JS, & Hubbard, W. (1990). A \ntime delay neural network character recognizer for a touch terminal. In Proceedings of the \u2026",
      "citation_histogram": [
        [1990, 2],
        [1991, 1],
        [1992, 3],
        [1993, 1],
        [1994, 2],
        [1995, 1],
        [1996, 1],
        [1997, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:UTh8PbecGs4C",
      "title": "Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors",
      "link": "https://arxiv.org/abs/2103.15949",
      "year": 2021,
      "cited_by": 11,
      "authors": ["Zeyu Yun", "Yubei Chen", "Bruno A Olshausen", "Yann LeCun"],
      "description": "Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these `black boxes' as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g. word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work.",
      "citation_histogram": [
        [2021, 4],
        [2022, 7]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:3NskZpgvI9IC",
      "title": "Inspirational adversarial image generation",
      "link": "https://ieeexplore.ieee.org/abstract/document/9381602/",
      "year": 2021,
      "cited_by": 11,
      "authors": [
        "Baptiste Rozi\u00e8re",
        "Morgane Riviere",
        "Olivier Teytaud",
        "J\u00e9r\u00e9my Rapin",
        "Yann LeCun",
        "Camille Couprie"
      ],
      "description": "The task of image generation started receiving some attention from artists and designers, providing inspiration for new creations. However, exploiting the results of deep generative models such as Generative Adversarial Networks can be long and tedious given the lack of existing tools. In this work, we propose a simple strategy to inspire creators with new generations learned from a dataset of their choice, while providing some control over the output. We design a simple optimization method to find the optimal latent parameters corresponding to the closest generation to any input inspirational image. Specifically, we allow the generation given an inspirational image of the user\u2019s choosing by performing several optimization steps to recover optimal parameters from the model\u2019s latent space. We tested several exploration methods from classical gradient descents to gradient-free optimizers. Many gradient-free\u00a0\u2026",
      "citation_histogram": [
        [2020, 4],
        [2021, 3],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:TAWzVH7WYHIC",
      "title": "Universal halting times in optimization and machine learning",
      "link": "https://arxiv.org/abs/1511.06444",
      "year": 2015,
      "cited_by": 11,
      "authors": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"],
      "description": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that, after centering and scaling, remains unchanged even when the distribution on the landscape is changed. We observe two qualitative classes: A Gumbel-like distribution that appears in Google searches, human decision times, the QR eigenvalue algorithm and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.",
      "citation_histogram": [
        [2016, 4],
        [2017, 2],
        [2018, 2],
        [2019, 2],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:b0M2c_1WBrUC",
      "title": "Hybrid hessians for flexible optimization of pose graphs",
      "link": "https://ieeexplore.ieee.org/abstract/document/5650091/",
      "year": 2010,
      "cited_by": 11,
      "authors": ["Matthew Koichi Grimes", "Dragomir Anguelov", "Yann LeCun"],
      "description": "We present a novel \u201chybrid Hessian\u201d six-degrees-of-freedom simultaneous localization and mapping (SLAM) algorithm. Our method allows for the smooth trade-off of accuracy for efficiency and for the incorporation of GPS measurements during real-time operation, thereby offering significant advantages over other SLAM solvers. Like other stochastic SLAM methods, such as SGD and TORO, our technique is robust to local minima and eliminates the need for costly relinearizations of the map. Unlike other stochastic methods, but similar to exact solvers, such as iSAM, our technique is able to process position-only constraints, such as GPS measurements, without introducing systematic distortions in the map. We present results from the Google Street View database, and compare our method with results from TORO. We show that our solver is able to achieve higher accuracy while operating within real-time bounds. In\u00a0\u2026",
      "citation_histogram": [
        [2011, 1],
        [2012, 2],
        [2013, 1],
        [2014, 2],
        [2015, 1],
        [2016, 2],
        [2017, 1],
        [2018, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:dfsIfKJdRG4C",
      "title": "Overview of the djvu document compression technology",
      "link": "https://books.google.com/books?hl=en&lr=&id=oONqBYkAMIUC&oi=fnd&pg=PA119&dq=info:q2tgxjWeex8J:scholar.google.com&ots=wlZ4N1cE91&sig=Qp_i4n7YPrYABvThMGNugVUgjg0",
      "year": 2001,
      "cited_by": 11,
      "authors": [
        "Yann LeCun",
        "Leon Bottou",
        "Patrick Haffner",
        "Jeffery Triggs",
        "Bill Riemers",
        "Luc Vincent"
      ],
      "description": "Despite the growing importance of multimedia content, much of the knowledge, culture, and edu-cational material in existence today is still available only in paper form. Bringing this wealth of information into the digital realm in a form that is faithful to the original, easily accessible, and searchable, is an essential step towards making the Internet the World's Universal Library.",
      "citation_histogram": [
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 2],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:J_g5lzvAfSwC",
      "title": "DjVu: a compression method for distributing scanned documents in color over the internet",
      "link": "https://scholar.google.com/scholar?cluster=90618379849476868&hl=en&oi=scholarr",
      "year": 1998,
      "cited_by": 11,
      "authors": [
        "Yann LeCun",
        "L\u00e9on Bottou",
        "Patrick Haffner",
        "Paul G Howard"
      ],
      "description": "We present a newimage compression technique called \u201cDjVu\u201d that is specifically geared towards the compression of scanned documents in color at high revolution. DjVu enable any screen connected to the Internet to access and display images of scanned pages while faithfully reproducing the font, color, drawings, pictures, and paper texture. With DjVu, a typical magazine page in color at 300dpi can be compressed down to between 40 to 60 KB, approximately 5 to 10 times better than JPEGfor a similar level of subjective quality. A real-time, memory efficient version of the decoder is available as a plug-in for popular web browsers.",
      "citation_histogram": [
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 2],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:ns9cj8rnVeAC",
      "title": "Natural Versus",
      "link": "https://www.computer.org/csdl/proceedings-article/phycmp/1992/00615508/12OmNzVXNTi",
      "year": 1992,
      "cited_by": 11,
      "authors": ["JS Denker"],
      "description": "DNA microarray technology has been used to measure expression levels for thousands of genes in a single experiment, across different samples. These samples can be clustered into homogeneous groups corresponding to some particular macroscopic phenotypes. In sample clustering problems, it is common to come up against the challenges of high dimensional data due to small sample volume and high feature (gene) dimensionality. Therefore, it is necessary to conduct dimension reduction on the gene dimension and identify informative genes prior to the clustering on the samples. This paper introduces a method for informative genes selection by utilizing independent component analysis (ICA). The performance of the proposed method on various microarray datasets is reported to illustrate its effectiveness.",
      "citation_histogram": [
        [2002, 1],
        [2003, 1],
        [2004, 3],
        [2005, 1],
        [2006, 1],
        [2007, 2],
        [2008, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:KmkpU35IWjAC",
      "title": "Constrained neural networks for pattern recognition",
      "link": "http://yann.lecun.org/exdb/publis/psgz/solla-lecun-91.ps.gz",
      "year": 1991,
      "cited_by": 11,
      "authors": ["Sara A Solla", "Yann Le Cun"],
      "description": "Layered neural networks are of interest as a tool to implement input-output maps. This work explores the ability of such architectures to perform pattern recognition tasks.",
      "citation_histogram": [
        [1993, 1],
        [1994, 1],
        [1995, 1],
        [1996, 1],
        [1997, 2],
        [1998, 1],
        [1999, 1],
        [2000, 1],
        [2001, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:nrtMV_XWKgEC",
      "title": "VLSI implementations of electronic neural networks: An example in character recognition",
      "link": "https://ieeexplore.ieee.org/abstract/document/142119/",
      "year": 1990,
      "cited_by": 11,
      "authors": [
        "LD Jackel",
        "B Boser",
        "HP Graf",
        "JS Denker",
        "Y Le Cun",
        "D Henderson",
        "O Matan",
        "RE Howard",
        "KS Baird"
      ],
      "description": "A large class of applications where theoretical considerations that promote high-accuracy classification result in constrained network architectures have been identified through a series of experiments in pattern recognition using neural net algorithms. These constrained nets can map onto appropriately designed hardware. The concepts learned from the pattern recognition experiments are discussed, and it is shown how they can be applied to chip design. A neural net chip for machine vision is described. The chip combines analog and digital processing and is reconfigurable.< >",
      "citation_histogram": [
        [1991, 1],
        [1992, 1],
        [1993, 1],
        [1994, 1],
        [1995, 1],
        [1996, 2],
        [1997, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:VETmZaymeusC",
      "title": "Learning world models: The next step towards AI",
      "link": "https://scholar.google.com/scholar?cluster=8991926099508239575&hl=en&oi=scholarr",
      "year": 2018,
      "cited_by": 10,
      "authors": ["Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [2018, 1],
        [2019, 5],
        [2020, 3],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:dVnQPkO1q_gC",
      "title": "My take on Ali Rahimi's \u201cTest of Time\u201d award talk at NIPS",
      "link": "http://www2.isye.gatech.edu/~tzhao80/Yann_Response.pdf",
      "year": 2017,
      "cited_by": 10,
      "authors": ["Yann LeCun"],
      "description": "But another important goal is inventing new methods, new techniques, and yes, new tricks. In the history of science and technology, the engineering artifacts have almost always preceded the theoretical understanding: the lens and the telescope preceded optics theory, the steam engine preceded thermodynamics, the airplane preceded flight aerodynamics, radio and data communication preceded information theory, the computer preceded computer science.",
      "citation_histogram": [
        [2018, 3],
        [2019, 1],
        [2020, 2],
        [2021, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:RGp4dj-Wy-oC",
      "title": "Bengio Y",
      "link": "https://scholar.google.com/scholar?cluster=13732408520082914846&hl=en&oi=scholarr",
      "year": 2015,
      "cited_by": 10,
      "authors": ["Y LeCun", "P Haffner", "L Bottou"],
      "description": null,
      "citation_histogram": [
        [2017, 1],
        [2018, 1],
        [2019, 1],
        [2020, 3],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:70eg2SAEIzsC",
      "title": "Speed-range dilemmas for vision-based navigation in unstructured terrain",
      "link": "https://www.sciencedirect.com/science/article/pii/S1474667016346778",
      "year": 2007,
      "cited_by": 10,
      "authors": [
        "Pierre Sermanet",
        "Raia Hadsell",
        "Jan Ben",
        "Ayse Naz Erkan",
        "Beat Flepp",
        "Urs Muller",
        "Yann LeCun"
      ],
      "description": "The performance of vision-based navigation systems for off-road mobile robots depends crucially on the resolution of the camera, the sophistication of the visual processing, the latency between image and sensor capture to actuator control, and the period of the control loop. One particularly important design question is whether one should increase the resolution of the camera images, and the range of the obstacle detection algorithms, at the expense of latency and control loop period. We first report experimental results on the resolution-period trade-off with a stereo vision-based navigation system implemented on the LAGR mobile robot platform. We propose a multi-agent perception and control architecture that combines a sophisticated long-range path detection method operating at high resolution and low frame rate, with a simple stereo-based obstacle detection method operating at low resolution, high frame\u00a0\u2026",
      "citation_histogram": [
        [2008, 4],
        [2009, 1],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:1DsIQWDZLl8C",
      "title": "Lush reference manual",
      "link": "https://scholar.google.com/scholar?cluster=4707382132922473830&hl=en&oi=scholarr",
      "year": 2002,
      "cited_by": 10,
      "authors": ["L\u00e9on Bottou", "YL Cun"],
      "description": "Lush is an object-oriented programming language with features designed to please researchers, experimenters, and engineers interested in large-scale numerical and graphic applications. Lush is designed to be used in situations where one would want to combine the flexibility of a high-level, loosely-typed interpreted language, with the efficiency of a strongly-typed, natively-compiled language, and with the easy integration of code written in C, C++, or other languages.Lush can be used advantageously for projects where one would otherwise a combination of an interpreted language like Python, Perl, Matlab, S+, or even (gasp!) BASIC, and a compiled language like C. Lush brings the best of both worlds by wrapping three languages into one:(1) a loosely-typed, garbagecollected, dynamically scoped, interpreted language with a simple Lisp-like syntax,(2) a strongly-typed, lexically-scoped compiled language that\u00a0\u2026",
      "citation_histogram": [
        [2001, 1],
        [2002, 1],
        [2003, 2],
        [2004, 2],
        [2005, 1],
        [2006, 2],
        [2007, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Z4TH09HQ3SoC",
      "title": "Guest editorial applications of artificial neural networks to image processing",
      "link": "https://ieeexplore.ieee.org/abstract/document/704303/",
      "year": 1998,
      "cited_by": 10,
      "authors": [
        "Rama Chellappa",
        "Kunihiko Fukushima",
        "Aggelos K Katsaggelos",
        "Sun-Yuan Kung",
        "Yann LeCun",
        "Nasser M Nasrabadi",
        "Tomaso A Poggio"
      ],
      "description": "ARTIFICIAL neural network (NN) architectures have been recognized for a number of years as a powerful technology for solving real-world image processing problems. The primary purpose of this special issue is to demonstrate some recent success in solving image processing problems and hopefully to motivate other image processing researchers to utilize this technology to solve their real-world problems. Finally, it is our hope that this special issue will increase the awareness of image processing researchers to the impact of the neural network-based algorithms. From the response to the initial call for papers, ten manuscripts have been selected for inclusion in this special issue. Eight papers have been offered as full papers and two as correspondence items. These papers covered the following major topics:1) neural network-based algorithms for character recognition; 2) automatic target recognition using\u00a0\u2026",
      "citation_histogram": [
        [2000, 3],
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 2],
        [2005, 1],
        [2006, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:SeFeTyx0c_EC",
      "title": "Special issue on applications of artificial neural networks to image processing",
      "link": "https://ieeexplore.ieee.org/iel4/83/15218/x0098694.pdf",
      "year": 1998,
      "cited_by": 10,
      "authors": [
        "R Chellappa",
        "K Fukushima",
        "AK Katsaggelos",
        "SY Kung",
        "Y LeCun",
        "NM Nasrabadi",
        "T Poggio",
        "L Wang",
        "S Der",
        "S Young",
        "PD Scott",
        "C Bandera",
        "JC Principe",
        "M Kim",
        "JW Fisher III"
      ],
      "description": "The Signal Processing Society is an organization, within the framework of the IEEE, of members with principal professional interest in the technology of transmission, recording, reproduction, processing, and measurement of speech and other signals by digital electronic, electrical, acoustic, mechanical, and optical means, the components and systems to accomplish these and related aims, and the environmental, psychological, and physiological factors concerned therewith. All members of the IEEE are eligible for membership in the Society and will receive this TRANSACTIONS upon payment of the annual Society membership fee of $20.00 plus an annual subscription fee of $43.00. For information on joining, write to the IEEE at the address below. Member copies of Transactions/Journals are for personal use only.",
      "citation_histogram": [
        [1999, 1],
        [2000, 1],
        [2001, 1],
        [2002, 1],
        [2003, 3],
        [2004, 1],
        [2005, 1],
        [2006, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:r-XlWH_wwbwC",
      "title": "Efficient learning and second-older methods, a tutorial",
      "link": "https://ci.nii.ac.jp/naid/10017601335/",
      "year": 1993,
      "cited_by": 10,
      "authors": ["Y Le Cun"],
      "description": "CiNii \u8ad6\u6587 - Efficient learning and second-older methods, a tutorial CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\n\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \n\u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\n\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 [7/12\u66f4\u65b0\n]2022\u5e744\u67081\u65e5\u304b\u3089\u306eCiNii Articles\u306eCiNii Research\u3078\u306e\u7d71\u5408\u306b\u3064\u3044\u3066 Efficient learning and \nsecond-older methods, a tutorial LE CUN Y. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 LE CUN Y. \u53ce\u9332\u520a\u884c\u7269 \nAdvances in neural information processing, Denver, CO Advances in neural information \nprocessing, Denver, CO, 1993 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 An alternative approach for \nneural network evolution with a genetic algorithm : Crossover by combinatorial optimization \nGARCIA-PEDRAJAS Nicolas , ORTIZ-BOYER Domingo , HERVAS-MARTINEZ Cesar Neural \u2026",
      "citation_histogram": [
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 1],
        [1999, 1],
        [2000, 2],
        [2001, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:9Nmd_mFXekcC",
      "title": "Improving generalization performance in character recognition",
      "link": "https://www.academia.edu/download/31000711/drucker-lecun-91a.pdf",
      "year": 1991,
      "cited_by": 10,
      "authors": ["Harris Drucker", "Yann Le Cun"],
      "description": "One test of a new training algorithm is how well the algorithm generalizes from the training data to the test data. A new training algorithm termed double backpropagation improves generalization by minimizing the change in the output due to small changes in the input. This is accomplished by minimizing the normal energy term found in backpropagation and an additional energy term that is a function of the Jacobian.",
      "citation_histogram": [
        [1992, 1],
        [1993, 1],
        [1994, 1],
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 1],
        [1999, 1],
        [2000, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:l0_JBNIuc60C",
      "title": "Modeles connexionnistes de l\u2019apprentissage [PhD thesis]",
      "link": "https://scholar.google.com/scholar?cluster=14478472072077023280&hl=en&oi=scholarr",
      "year": 1987,
      "cited_by": 10,
      "authors": ["Y LeCun"],
      "description": null,
      "citation_histogram": [
        [1988, 1],
        [1989, 1],
        [1990, 1],
        [1991, 1],
        [1992, 1],
        [1993, 2],
        [1994, 2],
        [1995, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:f36TrmluGJsC",
      "title": "Yoshua Bengio i Geoffrey Hinton.\u201cDeep learning \u201c",
      "link": "https://scholar.google.com/scholar?cluster=13456976783716272419&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 10,
      "authors": ["Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [2019, 1],
        [2020, 1],
        [2021, 2],
        [2022, 6]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Ba4ZglSFa04C",
      "title": "& Vapnik, V.(1995). Learning algorithms for classification: A comparison on handwritten digit recognition",
      "link": "https://scholar.google.com/scholar?cluster=15236161386510077848&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 10,
      "authors": [
        "Y LeCun",
        "LD Jackel",
        "L Bottou",
        "C Cortes",
        "JS Denker",
        "H Drucker"
      ],
      "description": null,
      "citation_histogram": [
        [2017, 1],
        [2018, 1],
        [2019, 2],
        [2020, 2],
        [2021, 3],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:_9EdYq_GCQ0C",
      "title": "MNist dataset, 2000",
      "link": "https://scholar.google.com/scholar?cluster=13301866058301380790&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 10,
      "authors": ["Yann LeCun", "Corinna Cortes", "CJC Burges"],
      "description": null,
      "citation_histogram": [
        [2008, 1],
        [2009, 1],
        [2010, 2],
        [2011, 2],
        [2012, 1],
        [2013, 2],
        [2014, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:5buzqZzMjwkC",
      "title": "Deep learning, reinforcement learning, and world models",
      "link": "https://www.sciencedirect.com/science/article/pii/S0893608022001150",
      "year": 2022,
      "cited_by": 9,
      "authors": [
        "Yutaka Matsuo",
        "Yann LeCun",
        "Maneesh Sahani",
        "Doina Precup",
        "David Silver",
        "Masashi Sugiyama",
        "Eiji Uchibe",
        "Jun Morimoto"
      ],
      "description": "Deep learning (DL) and reinforcement learning (RL) methods seem to be a part of indispensable factors to achieve human-level or super-human AI systems. On the other hand, both DL and RL have strong connections with our brain functions and with neuroscientific findings. In this review, we summarize talks and discussions in the \u201cDeep Learning and Reinforcement Learning\u201d session of the symposium, International Symposium on Artificial Intelligence and Brain Science. In this session, we discussed whether we can achieve comprehensive understanding of human intelligence based on the recent advances of deep learning and reinforcement learning algorithms. Speakers contributed to provide talks about their recent studies that can be key technologies to achieve human-level intelligence.",
      "citation_histogram": [[2022, 8]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:mYLs_rVKHI4C",
      "title": "Neural Potts Model",
      "link": "https://www.biorxiv.org/content/10.1101/2021.04.08.439084.abstract",
      "year": 2021,
      "cited_by": 9,
      "authors": [
        "Tom Sercu",
        "Robert Verkuil",
        "Joshua Meier",
        "Brandon Amos",
        "Zeming Lin",
        "Caroline Chen",
        "Jason Liu",
        "Yann LeCun",
        "Alexander Rives"
      ],
      "description": " We propose the Neural Potts Model objective as an amortized optimization problem. The objective enables training a single model with shared parameters to explicitly model energy landscapes across multiple protein families. Given a protein sequence as input, the model is trained to predict a pairwise coupling matrix for a Potts model energy function describing the local evolutionary landscape of the sequence. Couplings can be predicted for novel sequences. A controlled ablation experiment assessing unsupervised contact prediction on sets of related protein families finds a gain from amortization for low-depth multiple sequence alignments; the result is then confirmed on a database with broad coverage of protein sequences.",
      "citation_histogram": [
        [2021, 5],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:UlRcoTO8nVoC",
      "title": "Les enjeux de la recherche en intelligence artificielle",
      "link": "https://dataanalyticspost.com/wp-content/uploads/2017/04/ylecun_college_France.pdf",
      "year": 2015,
      "cited_by": 9,
      "authors": ["Yann Lecun"],
      "description": "Qu\u2019est-ce que l\u2019intelligence? Est-ce la capacit\u00e9 \u00e0 percevoir le monde, \u00e0 pr\u00e9dire le futur imm\u00e9diat ou lointain, ou \u00e0 planifier une s\u00e9rie d\u2019actions pour atteindre un but? Est-ce la capacit\u00e9 d\u2019apprendre, ou celle d\u2019appliquer son savoir \u00e0 bon escient? La d\u00e9finition est difficile \u00e0 cerner.",
      "citation_histogram": [
        [2017, 1],
        [2018, 1],
        [2019, 4],
        [2020, 1],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:bFI3QPDXJZMC",
      "title": "Multimedia Processing for Advanced Communications Services",
      "link": "https://link.springer.com/chapter/10.1007/978-1-4471-0859-7_42",
      "year": 1999,
      "cited_by": 9,
      "authors": ["B Shahraray", "R Cox", "B Haskell", "Y LeCun", "L Rabiner"],
      "description": " The advent of digital multimedia communications has generated a growing need for powerful multimedia processing techniques to enable the generation of useful and intelligent communications services. Multimedia processing techniques play a significant role in creating communications services by; 1) enabling efficient transmission and storage of multimedia information through media compression techniques, 2) creating effective user interfaces through media conversion, understanding, and dialogue systems, and 3) providing intelligent information searching and browsing mechanisms based on media processing and understanding techniques. In this paper we present a brief overview of media compression techniques and standards, touch upon several media processing techniques. Then we give a brief overview of three prototype services based on these techniques.",
      "citation_histogram": [
        [2001, 1],
        [2002, 1],
        [2003, 2],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:iKz1iSBcTNcC",
      "title": "E cient pattern recognition using a new transformation distance, inAdvances in Neural Information Processing Systems'",
      "link": "https://scholar.google.com/scholar?cluster=15017561450649034590&hl=en&oi=scholarr",
      "year": 1993,
      "cited_by": 9,
      "authors": ["PY Simard", "Y LeCun", "J Denker"],
      "description": null,
      "citation_histogram": [
        [1994, 1],
        [1995, 1],
        [1996, 1],
        [1997, 4],
        [1998, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:KIG7iI7jH74C",
      "title": "Backpropagation Applied to Handwritten Zip Code Recognition, 1989",
      "link": "https://scholar.google.com/scholar?cluster=4487485438861710753&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 9,
      "authors": [
        "Yan Lecun",
        "B Boser",
        "JS Denker",
        "D Henderson",
        "RE Howard",
        "W Hubbard",
        "LD Jackel"
      ],
      "description": null,
      "citation_histogram": [
        [2016, 1],
        [2017, 2],
        [2018, 1],
        [2019, 1],
        [2020, 2],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:eb1hsBXB1ukC",
      "title": "High performance computer acoustic data accelerator: A new system for exploring marine mammal acoustics for big data applications",
      "link": "https://arxiv.org/abs/1509.03591",
      "year": 2015,
      "cited_by": 8,
      "authors": [
        "Peter Dugan",
        "John Zollweg",
        "Marian Popescu",
        "Denise Risch",
        "Herve Glotin",
        "Yann LeCun"
      ],
      "description": "This paper presents a new software model designed for distributed sonic signal detection runtime using machine learning algorithms called DeLMA. A new algorithm--Acoustic Data-mining Accelerator (ADA)--is also presented. ADA is a robust yet scalable solution for efficiently processing big sound archives using distributing computing technologies. Together, DeLMA and the ADA algorithm provide a powerful tool currently being used by the Bioacoustics Research Program (BRP) at the Cornell Lab of Ornithology, Cornell University. This paper provides a high level technical overview of the system, and discusses various aspects of the design. Basic runtime performance and project summary are presented. The DeLMA-ADA baseline performance comparing desktop serial configuration to a 64 core distributed HPC system shows as much as a 44 times faster increase in runtime execution. Performance tests using 48 cores on the HPC shows a 9x to 12x efficiency over a 4 core desktop solution. Project summary results for 19 east coast deployments show that the DeLMA-ADA solution has processed over three million channel hours of sound to date.",
      "citation_histogram": [
        [2016, 1],
        [2017, 2],
        [2018, 2],
        [2019, 1],
        [2020, 1],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:pqufTfZDgs0C",
      "title": "High performance computer acoustic data accelerator (HPC-ADA): A new system for exploring marine mammal acoustics for big data applications",
      "link": "https://pure.uhi.ac.uk/en/publications/high-performance-computer-acoustic-data-accelerator-hpc-ada-a-new",
      "year": 2014,
      "cited_by": 8,
      "authors": [
        "Peter J Dugan",
        "John Zollweg",
        "Herve Glotin",
        "Marian Popescu",
        "Denise Risch",
        "Yann LeCun",
        "Christopher W Clark"
      ],
      "description": "With continuing growth of the world's population and rapid economic development, our need to preserve the natural environment, especially our oceans, is becoming an increasing concern. For the past several decades scientists have been monitoring the oceans using a variety of sensors and tools. Passive acoustic monitoring is one of the primary methods used to investigate the behavior patterns of soniferous marine animals. Analyzing the vast amount of collected data poses an enormous challenge. This paper presents a new system designed for high speed acoustic processing called the High Performance Computer Acoustic Data Accelerator (HPC-ADA). Together with an appropriate software suite, the HPC-ADA is a powerful tool currently being used by the Bioacoustics Research Program (BRP) at the Cornell Lab of Ornithology, Cornell University. This paper provides a high level technical overview of the HPC-ADA system\u2019s architecture, software suite, and operation of the HPC-ADA. We also summarize the projects that have successfully used the HPC-ADA system; totaling over one million hours of processed sound to date.",
      "citation_histogram": [
        [2014, 1],
        [2015, 2],
        [2016, 3],
        [2017, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:F1-V36_CjEsC",
      "title": "Bioacoustic challenges in icml4b",
      "link": "https://scholar.google.com/scholar?cluster=15240134037947379115&hl=en&oi=scholarr",
      "year": 2013,
      "cited_by": 8,
      "authors": [
        "H Glotin",
        "C Clark",
        "Y LeCun",
        "P Dugan",
        "X Halkias",
        "J Sueur"
      ],
      "description": null,
      "citation_histogram": [
        [2014, 1],
        [2015, 3],
        [2016, 2],
        [2017, 1],
        [2018, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:K3LRdlH-MEoC",
      "title": "Hardware accelerated visual attention algorithm",
      "link": "https://ieeexplore.ieee.org/abstract/document/5766191/",
      "year": 2011,
      "cited_by": 8,
      "authors": [
        "Polina Akselrod",
        "Faye Zhao",
        "Ifigeneia Derekli",
        "Cl\u00e9ment Farabet",
        "Berin Martini",
        "Yann LeCun",
        "Eugenio Culurciello"
      ],
      "description": "We present a hardware-accelerated implementation of a bottom-up visual attention algorithm. This algorithm generates a multi-scale saliency map from differences in image intensity, color, presence of edges and presence of motion. The visual attention algorithm is computed on a custom-designed FPGA-based dataflow computer for general-purpose state-of-the-art vision algorithms. The vision algorithm is accelerated by our hardware platform and reports \u00d74 speedup when compared to a standard laptop with a 2.26 GHz Intel Dual Core processor and for image sizes of 480 \u00d7 480 pixels. We developed a real time demo application running at >; 12 frames per second with the same size images. We also compared the results of the hardware implementation of the algorithm to the eye fixation points of different subjects on six video sequences. We find that our implementation achieves precisions of fixation predictions\u00a0\u2026",
      "citation_histogram": [
        [2013, 2],
        [2014, 1],
        [2015, 1],
        [2016, 1],
        [2017, 1],
        [2018, 1],
        [2019, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:EUQCXRtRnyEC",
      "title": "Efficient off-road localization using visually corrected odometry",
      "link": "https://ieeexplore.ieee.org/abstract/document/5152880/",
      "year": 2009,
      "cited_by": 8,
      "authors": ["Matthew Grimes", "Yann LeCun"],
      "description": "We describe an efficient, low-cost, low-overhead system for robot localization in complex visual environments. Our system augments wheel odometry with visual orientation tracking to yield localization accuracy comparable with ldquopurerdquo visual odometry at a fraction of the cost. Such a system is well-suited to consumer-level robots, small form-factor robots, extraterrestrial rovers, and other platforms with limited computational resources. Our system also benefits high-end multiprocessor robots by leaving ample processor time on all camera-computer pairs to perform other critical visual tasks, such as obstacle detection. Experimental results are shown for outdoor, off-road loops on the order of 200 meters. Comparisons are made with corresponding results from a state-of-the-art pure visual odometer.",
      "citation_histogram": [
        [2011, 1],
        [2012, 2],
        [2013, 1],
        [2014, 1],
        [2015, 1],
        [2016, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:VLnqNzywnoUC",
      "title": "Reverse time delay neural network for pattern generation",
      "link": "https://patents.google.com/patent/US5412754A/en",
      "year": 1995,
      "cited_by": 8,
      "authors": ["Yann A Le Cun", "Patrice Y Simard"],
      "description": "Trajectories are generated in response to an input label by using a reverse time delay neural network. The reverse time delay neural network comprises an input layer, a plurality of hidden layers, and an output layer, all arranged in succession so that the number of frames per layer increases as the network is traversed from the input layer to the output layer. Additionally, the number of features decreases as the network is traversed from the input layer to the output layer. Features of the trajectory are created from the input label so that a time series of frames can be output by the network. Frames generally relate to particular epochs of time or time units and a frame includes a plurality of features.",
      "citation_histogram": [
        [1997, 1],
        [1998, 2],
        [1999, 1],
        [2000, 1],
        [2001, 1],
        [2002, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:SxVRRePJDnEC",
      "title": "YoshuaBengio.\"",
      "link": "https://scholar.google.com/scholar?cluster=2775982482060405217&hl=en&oi=scholarr",
      "year": 1995,
      "cited_by": 8,
      "authors": ["Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [2018, 1],
        [2019, 3],
        [2020, 2],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:viPVbuMW504C",
      "title": "Improving the convergence of backpropagation learning with second order methods In Proceedings of the 1988 Connectionist Models Summer School pages 29\u201337",
      "link": "https://scholar.google.com/scholar?cluster=5587493827370731835&hl=en&oi=scholarr",
      "year": 1989,
      "cited_by": 8,
      "authors": ["S Becker", "Y LeCun"],
      "description": null,
      "citation_histogram": [
        [1989, 1],
        [1990, 1],
        [1991, 1],
        [1992, 1],
        [1993, 1],
        [1994, 1],
        [1995, 1],
        [1996, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:eeRCOjARQ4cC",
      "title": "Self-supervised learning: The dark matter of intelligence, 2021",
      "link": "https://scholar.google.com/scholar?cluster=6155573012820404391&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 8,
      "authors": ["Yann LeCun", "Ishan Misra"],
      "description": null,
      "citation_histogram": [
        [2021, 4],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:t4WtaE3RIIAC",
      "title": "A data-augmentation is worth a thousand samples: Exact quantification from analytical augmented sample moments",
      "link": "https://arxiv.org/abs/2202.08325",
      "year": 2022,
      "cited_by": 7,
      "authors": ["Randall Balestriero", "Ishan Misra", "Yann LeCun"],
      "description": "Data-Augmentation (DA) is known to improve performance across tasks and datasets. We propose a method to theoretically analyze the effect of DA and study questions such as: how many augmented samples are needed to correctly estimate the information encoded by that DA? How does the augmentation policy impact the final parameters of a model? We derive several quantities in close-form, such as the expectation and variance of an image, loss, and model's output under a given DA distribution. Those derivations open new avenues to quantify the benefits and limitations of DA. For example, we show that common DAs require tens of thousands of samples for the loss at hand to be correctly estimated and for the model training to converge. We show that for a training loss to be stable under DA sampling, the model's saliency map (gradient of the loss with respect to the model's input) must align with the smallest eigenvector of the sample variance under the considered DA augmentation, hinting at a possible explanation on why models tend to shift their focus from edges to textures.",
      "citation_histogram": [[2022, 7]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:0IGl3jR3yJEC",
      "title": "A path towards autonomous machine intelligence",
      "link": "https://cis.temple.edu/tagit/presentations/A%20Path%20Towards%20Autonomous%20Machine%20Intelligence.pdf",
      "year": 2022,
      "cited_by": 7,
      "authors": ["Yann LeCun"],
      "description": "\u2013Using latent variable, the model can present multiple predictions.\u2013A latent variable is an input variable whose value is not observed but inferred.\u2013In a temporal prediction scenario, the latent variable represents what cannot be predicted about y (the future) solely from x and from past observations (the past).",
      "citation_histogram": [[2022, 6]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:9nEbuwLZtFEC",
      "title": "Backpropagation for implicit spectral densities",
      "link": "https://arxiv.org/abs/1806.00499",
      "year": 2018,
      "cited_by": 7,
      "authors": ["Aditya Ramesh", "Yann LeCun"],
      "description": "Most successful machine intelligence systems rely on gradient-based learning, which is made possible by backpropagation. Some systems are designed to aid us in interpreting data when explicit goals cannot be provided. These unsupervised systems are commonly trained by backpropagating through a likelihood function. We introduce a tool that allows us to do this even when the likelihood is not explicitly set, by instead using the implicit likelihood of the model. Explicitly defining the likelihood often entails making heavy-handed assumptions that impede our ability to solve challenging tasks. On the other hand, the implicit likelihood of the model is accessible without the need for such assumptions. Our tool, which we call spectral backpropagation, allows us to optimize it in much greater generality than what has been attempted before. GANs can also be viewed as a technique for optimizing implicit likelihoods. We study them using spectral backpropagation in order to demonstrate robustness for high-dimensional problems, and identify two novel properties of the generator G: (1) there exist aberrant, nonsensical outputs to which G assigns very high likelihood, and (2) the eigenvectors of the metric induced by G over latent space correspond to quasi-disentangled explanatory factors.",
      "citation_histogram": [
        [2019, 4],
        [2020, 2],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:aEyKTaVlRPYC",
      "title": "Qu\u2019est-ce que l\u2019intelligence artificielle",
      "link": "https://scholar.google.com/scholar?cluster=14030963798871692668&hl=en&oi=scholarr",
      "year": 2017,
      "cited_by": 7,
      "authors": ["Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [2018, 1],
        [2019, 2],
        [2020, 4]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:OvCsJ6o9tOQC",
      "title": "Simultaneous learning of trees and representations for extreme classification, with application to language modeling",
      "link": "https://scholar.google.com/scholar?cluster=13334614023021608340&hl=en&oi=scholarr",
      "year": 2016,
      "cited_by": 7,
      "authors": [
        "Yacine Jernite",
        "Anna Choromanska",
        "David Sontag",
        "Yann LeCun"
      ],
      "description": "This paper addresses the problem of multiclass classification with an extremely large number of classes, where the class predictor is learned jointly with the data representation, as is the case in language modeling problems. The predictor admits a hierarchical structure, which allows for efficient handling of settings that deal with a very large number of labels. The predictive power of the model however can heavily depend on the structure of the tree. We address this problem with an algorithm for tree construction and training that is based on a new objective function which favors balanced and easilyseparable node partitions. We describe theoretical properties of this objective function and show that it gives rise to a boosting algorithm for which we provide a bound on classification error, ie we show that if the objective is weakly optimized in the internal nodes of the tree, then our algorithm will amplify this weak advantage to build a tree achieving any desired level of accuracy. We apply the algorithm to the task of language modeling by re-framing conditional density estimation as a variant of the hierarchical classification problem. We empirically demonstrate on text data that the proposed approach leads to high-quality trees in terms of perplexity and computational running time compared to its non-hierarchical counterpart.",
      "citation_histogram": [
        [2018, 4],
        [2019, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:ZtJ3RtM1NaMC",
      "title": "Fast incremental learning for off-road robot navigation",
      "link": "https://arxiv.org/abs/1606.08057",
      "year": 2016,
      "cited_by": 7,
      "authors": [
        "Artem Provodin",
        "Liila Torabi",
        "Beat Flepp",
        "Yann LeCun",
        "Michael Sergio",
        "Lawrence D Jackel",
        "Urs Muller",
        "Jure Zbontar"
      ],
      "description": "A promising approach to autonomous driving is machine learning. In such systems, training datasets are created that capture the sensory input to a vehicle as well as the desired response. A disadvantage of using a learned navigation system is that the learning process itself may require a huge number of training examples and a large amount of computing. To avoid the need to collect a large training set of driving examples, we describe a system that takes advantage of the huge number of training examples provided by ImageNet, but is able to adapt quickly using a small training set for the specific driving environment.",
      "citation_histogram": [
        [2017, 1],
        [2018, 3],
        [2019, 1],
        [2020, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:GL0K47J-u9kC",
      "title": "Phase 4: Dcl system using deep learning approaches for land-based or ship-based real-time recognition and localization of marine mammals-distributed processing and big data\u00a0\u2026",
      "link": "https://arxiv.org/abs/1605.00982",
      "year": 2016,
      "cited_by": 7,
      "authors": [
        "Peter J Dugan",
        "Christopher W Clark",
        "Yann Andr\u00e9 LeCun",
        "Sofie M Van Parijs"
      ],
      "description": "While the animal bioacoustics community at large is collecting huge amounts of acoustic data at an unprecedented pace, processing these data is problematic. Currently in bioacoustics, there is no effective way to achieve high performance computing using commericial off the shelf (COTS) or government off the shelf (GOTS) tools. Although several advances have been made in the open source and commercial software community, these offerings either support specific applications that do not integrate well with data formats in bioacoustics or they are too general. Furthermore, complex algorithms that use deep learning strategies require special considerations, such as very large libraiers of exemplars (whale sounds) readily available for algorithm training and testing. Detection-classification for passive acoustics is a data-mining strategy and our goals are aligned with best practices that appeal to the general data mining and machine learning communities where the problem of processing large data is common. Therefore, the objective of this work is to advance the state-of-the art for data-mining large passive acoustic datasets as they pertain to bioacoustics. With this basic deficiency recognized at the forefront, portions of the grant were dedicated to fostering deep-learning by way of international competitions (kaggle.com) meant to attract deep-learning solutions. The focus of this early work was targeted to make significant progress in addressing big data systems and advanced algorithms over the duration of the grant from 2012 to 2015. This early work provided simulataneous advances in systems-algorithms research while supporting various\u00a0\u2026",
      "citation_histogram": [
        [2017, 2],
        [2018, 2],
        [2019, 1],
        [2020, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:SP6oXDckpogC",
      "title": "Efficient learning of sparse invariant representations",
      "link": "https://arxiv.org/abs/1105.5307",
      "year": 2011,
      "cited_by": 7,
      "authors": ["Karol Gregor", "Yann LeCun"],
      "description": "We propose a simple and efficient algorithm for learning sparse invariant representations from unlabeled data with fast inference. When trained on short movies sequences, the learned features are selective to a range of orientations and spatial frequencies, but robust to a wide range of positions, similar to complex cells in the primary visual cortex. We give a hierarchical version of the algorithm, and give guarantees of fast convergence under certain conditions.",
      "citation_histogram": [
        [2013, 4],
        [2014, 2],
        [2015, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:zdjWy_NXXwUC",
      "title": "Machine Learning and Pattern Recognition",
      "link": "https://cilvr.cs.nyu.edu/diglib/mlpr/lecture04-backprop.pdf",
      "year": 2004,
      "cited_by": 7,
      "authors": ["Yann LeCun"],
      "description": "MACHINE LEARNING AND PATTERN RECOGNITION Fall 2006, Lecture 4.1 Gradient-Based \nLearning: Back-Propagation and Multi-Module Syste Page 1 MACHINE LEARNING AND \nPATTERN RECOGNITION Fall 2006, Lecture 4.1 Gradient-Based Learning: Back-Propagation \nand Multi-Module Systems Yann LeCun The Courant Institute, New York University http://yann.lecun.com \nY. LeCun: Machine Learning and Pattern Recognition \u2013 p. 1/26 Page 2 Non-Linear \nLearning So far, we have seen how to train linear machines, and we have hinted at the fact \nthat we could also train non-linear machines. In non-linear machines, the discriminant \nfunction F(X, W) is allowed to be non linear with respect to W and non linear with respect to \nX. This allows us play with a much larger set of parameterized families of functions with a \nrich repertoire of class boundaries. well-designed non-linear classifiers can learn complex \u2026",
      "citation_histogram": [
        [2010, 1],
        [2011, 1],
        [2012, 2],
        [2013, 1],
        [2014, 1],
        [2015, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:z_wVstp3MssC",
      "title": "DjVu document browsing with on-demand loading and rendering of image components",
      "link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/4311/0000/DjVu-document-browsing-with-on-demand-loading-and-rendering-of/10.1117/12.411887.short",
      "year": 2000,
      "cited_by": 7,
      "authors": [
        "Yann Le Cun",
        "L\u00e9on Bottou",
        "Andrei Erofeev",
        "Patrick Haffner",
        "Bill Riemers"
      ],
      "description": "Image-based digital documents are composed of multiple pages, each of which may be composed of multiple components such as the test, pictures background, and annotations. We describe the image structure and software architecture that allows the DjVu system to load and render the required components on demand while minimizing the bandwidth requirements, and the memory requirements in the client. DjVu document files are merely a list of enriched URLs that point to individual files (or file elements) that contain image components. Image components include :text images, background images, shape dictionaries shared by multiple pages, OCRed text, and several types of annotations. A multithreaded software architecture with smart caching allows individual components to be loaded and pre-decoded and rendered on-demand. Pages are pre-fetched or loaded on demand, allowing users to randomly\u00a0\u2026",
      "citation_histogram": [
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 2],
        [2005, 1],
        [2006, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:_B80troHkn4C",
      "title": "Neural Networks and Gradient-Based Learning in OCR (invited talk)",
      "link": null,
      "year": 1997,
      "cited_by": 7,
      "authors": ["Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [2003, 1],
        [2004, 2],
        [2005, 2],
        [2006, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:4OULZ7Gr8RgC",
      "title": "A neural network approach to handprint character recognition.",
      "link": "https://www.researchgate.net/profile/Jane-Bromley/publication/3507044_A_neural_network_approach_to_handprint_character_recognition/links/54a684b80cf257a63609f3e2/A-neural-network-approach-to-handprint-character-recognition.pdf",
      "year": 1991,
      "cited_by": 7,
      "authors": [
        "Lawrence D Jackel",
        "Charles E Stenard",
        "Henry S Baird",
        "Bernhard E Boser",
        "Jane Bromley",
        "Christopher JC Burges",
        "John S Denker",
        "Hans Peter Graf",
        "Donnie Henderson",
        "Richard E Howard",
        "Wayne E Hubbard",
        "Yann LeCun",
        "Ofer Matan",
        "Edwin PD Pednault",
        "William Satterfield",
        "Eduard S\u00e4ckinger",
        "Timothy J Thompson"
      ],
      "description": "Ned-net methods, which rely on highly interconnected arrays of simple processing elements, have demonstrated state-of-the-art performance (both speed and accuracy) in optical character recognition (OCR). The networks can be emulated in software, or can be implemented in special-purpose hardware for added speed. This paper outlines OCR technology developed at AT&T Bell Laboratories, including a recognition network that learns feature extraction kernels, and a custom Wl chip that is designed for neural-net image processing.",
      "citation_histogram": [
        [1998, 1],
        [1999, 1],
        [2000, 2],
        [2001, 1],
        [2002, 1],
        [2003, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:aDdGf5um_jkC",
      "title": "Contrastive and non-contrastive self-supervised learning recover global and local spectral embedding methods",
      "link": "https://arxiv.org/abs/2205.11508",
      "year": 2022,
      "cited_by": 6,
      "authors": ["Randall Balestriero", "Yann LeCun"],
      "description": "Self-Supervised Learning (SSL) surmises that inputs and pairwise positive relationships are enough to learn meaningful representations. Although SSL has recently reached a milestone: outperforming supervised methods in many modalities... the theoretical foundations are limited, method-specific, and fail to provide principled design guidelines to practitioners. In this paper, we propose a unifying framework under the helm of spectral manifold learning to address those limitations. Through the course of this study, we will rigorously demonstrate that VICReg, SimCLR, BarlowTwins et al. correspond to eponymous spectral methods such as Laplacian Eigenmaps, Multidimensional Scaling et al. This unification will then allow us to obtain (i) the closed-form optimal representation for each method, (ii) the closed-form optimal network parameters in the linear regime for each method, (iii) the impact of the pairwise relations used during training on each of those quantities and on downstream task performances, and most importantly, (iv) the first theoretical bridge between contrastive and non-contrastive methods towards global and local spectral embedding methods respectively, hinting at the benefits and limitations of each. For example, (a) if the pairwise relation is aligned with the downstream task, any SSL method can be employed successfully and will recover the supervised method, but in the low data regime, SimCLR or VICReg with high invariance hyper-parameter should be preferred; (b) if the pairwise relation is misaligned with the downstream task, BarlowTwins or VICReg with small invariance hyper-parameter should be preferred.",
      "citation_histogram": [[2022, 6]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:rPDWb_FAO-YC",
      "title": "Audio source separation with discriminative scattering networks",
      "link": "https://link.springer.com/chapter/10.1007/978-3-319-22482-4_30",
      "year": 2015,
      "cited_by": 6,
      "authors": ["Pablo Sprechmann", "Joan Bruna", "Yann LeCun"],
      "description": " Many monaural signal decomposition techniques proposed in the literature operate on a feature space consisting of a time-frequency representation of the input data. A challenge faced by these approaches is to effectively exploit the temporal dependencies of the signals at scales larger than the duration of a time-frame. In this work we propose to tackle this problem by modeling the signals using a time-frequency representation with multiple temporal resolutions. For this reason we use a signal representation that consists of a pyramid of wavelet scattering operators, which generalizes Constant Q Transforms (CQT) with extra layers of convolution and complex modulus. We first show that learning standard models with this multi-resolution setting improves source separation results over fixed-resolution methods. As study case, we use Non-Negative Matrix Factorizations (NMF) that has been widely\u00a0\u2026",
      "citation_histogram": [
        [2016, 1],
        [2017, 1],
        [2018, 1],
        [2019, 2],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:_xSYboBqXhAC",
      "title": "Bio-inspired vision processor for ultra-fast object categorization",
      "link": "https://www.academia.edu/download/7643363/hpec10.pdf",
      "year": 2010,
      "cited_by": 6,
      "authors": [
        "Cl\u00e9ment Farabet",
        "Berin Martini",
        "Polina Akselrod",
        "Benoit Corda",
        "Sel\u00e7uk Talay",
        "Yann LeCun",
        "Eugenio Culurciello"
      ],
      "description": "We present a scalable hardware architecture to implement large-scale bio-inspired synthetic vision systems. The system is a fully digital implementation of a modular vision engine that can perform real-time detection, recognition and segmentation of mega-pixel images. We present performance comparisons between software versions of the vision system executing on CPU and GPU machines, and show that our FPGA implementation can outperform these systems by a factor of four.",
      "citation_histogram": [
        [2011, 2],
        [2012, 1],
        [2013, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:CtYknXOfbFEC",
      "title": "Advances in neural information processing systems",
      "link": "https://ui.adsabs.harvard.edu/abs/2001anip.book.....J/abstract",
      "year": 2001,
      "cited_by": 6,
      "authors": ["Michael I Jordan", "Yann Lecun", "Sara A Solla"],
      "description": "The annual conference on Neural Information Processing Systems (NIPS) is the flagship conference on neural computation. The conference is interdisciplinary, with contributions in algorithms, learning theory, cognitive science, neuroscience, vision, speech and signal processing, reinforcement learning and control, implementations, and diverse applications. Only about 30 percent of the papers submitted are accepted for presentation at NIPS, so the quality is exceptionally high. This CD-ROM contains the entire proceedings of the twelve Neural Information Processing Systems conferences from 1988 to 1999. The files are available in the DjVu image format developed by Yann LeCun and his group at AT&T Labs. The CD-ROM includes free browsers for all major platforms. Michael I. Jordan is Professor of Computer Science and of Statistics at the University of California, Berkeley. Yann LeCun is Head of the Image\u00a0\u2026",
      "citation_histogram": [
        [2004, 1],
        [2005, 1],
        [2006, 2],
        [2007, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:IsPWOBWtZBwC",
      "title": "Gradient-based learning for object detection, segmentation and recognition",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.5633",
      "year": 1999,
      "cited_by": 6,
      "authors": [
        "Yann Lecun",
        "Patrick Haffner",
        "Yoshua Bengio",
        "Leon Bottou"
      ],
      "description": "Finding an appropriate set of features is an essential problem in the design of visual recognition",
      "citation_histogram": [
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 1],
        [2006, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:2P1L_qKh6hAC",
      "title": "Method and apparatus for symbol recognition using multidimensional preprocessing and symbol sorting",
      "link": "https://patents.google.com/patent/US5647022A/en",
      "year": 1997,
      "cited_by": 6,
      "authors": ["Yann Andre LeCun", "Quen-Zong Wu"],
      "description": "Data samples describing a plurality of micro-segments that compose a symbol to be recognized are received from a device such as an electronic pad. A preprocessor maps the micro-segments into cells of an array that has several feature dimensions. The preprocessor assigns values to the cells based on the length of a micro-segment associated with the cell, and how well the features of the associated micro-segment correspond to the feature label of the cell. The cell values are used as inputs to at least one of a plurality of neural networks, where each neural network is trained to identify symbols from a different group of symbols. A sorter examines the symbol to determine which neural network should be used to recognize the symbol. An output produced by the sorter controls a switching means that communicates the cell values to the proper neural network.",
      "citation_histogram": [
        [2000, 1],
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:ipzZ9siozwsC",
      "title": "Neural network applications in character recognition and document analysis",
      "link": "https://link.springer.com/chapter/10.1007/978-1-4615-2734-3_14",
      "year": 1994,
      "cited_by": 6,
      "authors": [
        "LD Jackel",
        "MY Battista",
        "J Ben",
        "J Bromley",
        "CJC Burges",
        "HS Baird",
        "E Cosatto",
        "JS Denker",
        "HP Graf",
        "HP Katseff",
        "Y LeCun",
        "CR Nohl",
        "E Sackinger",
        "JH Shamilian",
        "T Shoemaker",
        "CE Stenard",
        "BI Strom",
        "R Ting",
        "T Wood",
        "CR Zuraw"
      ],
      "description": " Character Recognition has served as one of the principal proving grounds for neural-net methods and has emerged as one of the most successful applications of this technology. This chapter outlines optical character recognition document analysis systems developed at AT&T Bell Labs that combine the strengths of machine-learning algorithms with high-speed, fine-grained parallel hardware. From our point of view, the most significant aspect of this work has been the efficient integration of diverse methods into end-to-end systems. In this paper we use the task of locating and reading ZIP codes on US mail pieces as an illustration of the character recognition / document analysis process. We will also describe other applications of the technology, including interpretation of faxed forms and bit-mapped text to ASCII conversion.",
      "citation_histogram": [
        [1997, 1],
        [1998, 1],
        [1999, 1],
        [2000, 1],
        [2001, 1],
        [2002, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:q93lAJlLO_MC",
      "title": "Optimal brain damage. Advances of NIPS2",
      "link": "https://scholar.google.com/scholar?cluster=4782107872156483071&hl=en&oi=scholarr",
      "year": 1990,
      "cited_by": 6,
      "authors": ["Y LeCun", "J Denker", "S Solla", "D Touretzky"],
      "description": null,
      "citation_histogram": [
        [2001, 2],
        [2002, 1],
        [2003, 2],
        [2004, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:kRWl-cVF35UC",
      "title": "JS Denker, D",
      "link": "https://scholar.google.com/scholar?cluster=9266238989483758127&hl=en&oi=scholarr",
      "year": 1990,
      "cited_by": 6,
      "authors": ["Y LeCun", "B Boser"],
      "description": null,
      "citation_histogram": [
        [1991, 2],
        [1992, 2],
        [1993, 1],
        [1994, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:FS78WRl2AkQC",
      "title": "SN: Un simulateur pour r\u00e9seaux connexionnistes",
      "link": "https://scholar.google.com/scholar?cluster=783564220583802693&hl=en&oi=scholarr",
      "year": 1988,
      "cited_by": 6,
      "authors": ["L Bottou", "Y Le Cun"],
      "description": null,
      "citation_histogram": [
        [1990, 3],
        [1991, 1],
        [1992, 1],
        [1993, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:INESB4G31EoC",
      "title": "Jackel ea, LD (1990). Handwritten digit recognition with a back-propagation network",
      "link": "https://scholar.google.com/scholar?cluster=1246246538909359918&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 6,
      "authors": [
        "Y LeCun",
        "B Boser",
        "J Denker",
        "D Henderson",
        "R Howard",
        "W Hubbard"
      ],
      "description": null,
      "citation_histogram": [
        [1992, 1],
        [1993, 1],
        [1994, 1],
        [1995, 1],
        [1996, 1],
        [1997, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:nlKf13ul9_IC",
      "title": "The MNIST Database of Handwritten Digits. 1998",
      "link": "https://scholar.google.com/scholar?cluster=6407036748153195205&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 6,
      "authors": ["L Yann", "C Corinna", "J Christopher"],
      "description": null,
      "citation_histogram": [
        [2019, 1],
        [2020, 3],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:PjKh-f16SfUC",
      "title": "Coarse-to-fine vision-language pre-training with fusion in the backbone",
      "link": "https://arxiv.org/abs/2206.07643",
      "year": 2022,
      "cited_by": 5,
      "authors": [
        "Zi-Yi Dou",
        "Aishwarya Kamath",
        "Zhe Gan",
        "Pengchuan Zhang",
        "Jianfeng Wang",
        "Linjie Li",
        "Zicheng Liu",
        "Ce Liu",
        "Yann LeCun",
        "Nanyun Peng",
        "Jianfeng Gao",
        "Lijuan Wang"
      ],
      "description": "Vision-language (VL) pre-training has recently received considerable attention. However, most existing end-to-end pre-training approaches either only aim to tackle VL tasks such as image-text retrieval, visual question answering (VQA) and image captioning that test high-level understanding of images, or only target region-level understanding for tasks such as phrase grounding and object detection. We present FIBER (Fusion-In-the-Backbone-based transformER), a new VL model architecture that can seamlessly handle both these types of tasks. Instead of having dedicated transformer layers for fusion after the uni-modal backbones, FIBER pushes multimodal fusion deep into the model by inserting cross-attention into the image and text backbones, bringing gains in terms of memory and performance. In addition, unlike previous work that is either only pre-trained on image-text data or on fine-grained data with box-level annotations, we present a two-stage pre-training strategy that uses both these kinds of data efficiently: (i) coarse-grained pre-training based on image-text data; followed by (ii) fine-grained pre-training based on image-text-box data. We conduct comprehensive experiments on a wide range of VL tasks, ranging from VQA, image captioning, and retrieval, to phrase grounding, referring expression comprehension, and object detection. Using deep multimodal fusion coupled with the two-stage pre-training, FIBER provides consistent performance improvements over strong baselines across all tasks, often outperforming methods using magnitudes more data. Code is available at https://github.com/microsoft/FIBER.",
      "citation_histogram": [[2022, 5]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:JMEA1obkRKoC",
      "title": "On the duality between contrastive and non-contrastive self-supervised learning",
      "link": "https://arxiv.org/abs/2206.02574",
      "year": 2022,
      "cited_by": 5,
      "authors": [
        "Quentin Garrido",
        "Yubei Chen",
        "Adrien Bardes",
        "Laurent Najman",
        "Yann Lecun"
      ],
      "description": "Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-contrastive approaches. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them. By designing contrastive and non-contrastive criteria that can be related algebraically and shown to be equivalent under limited assumptions, we show how close those families can be. We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show how design choices in the criterion can influence the optimization process and downstream performance. We also challenge the popular assumptions that contrastive and non-contrastive methods, respectively, need large batch sizes and output dimensions. Our theoretical and quantitative results suggest that the numerical gaps between contrastive and noncontrastive methods in certain regimes can be significantly reduced given better network design choice and hyperparameter tuning.",
      "citation_histogram": [[2022, 5]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:7fE6T6CK6bcC",
      "title": "Neural manifold clustering and embedding",
      "link": "https://arxiv.org/abs/2201.10000",
      "year": 2022,
      "cited_by": 5,
      "authors": [
        "Zengyi Li",
        "Yubei Chen",
        "Yann LeCun",
        "Friedrich T Sommer"
      ],
      "description": "Given a union of non-linear manifolds, non-linear subspace clustering or manifold clustering aims to cluster data points based on manifold structures and also learn to parameterize each manifold as a linear subspace in a feature space. Deep neural networks have the potential to achieve this goal under highly non-linear settings given their large capacity and flexibility. We argue that achieving manifold clustering with neural networks requires two essential ingredients: a domain-specific constraint that ensures the identification of the manifolds, and a learning algorithm for embedding each manifold to a linear subspace in the feature space. This work shows that many constraints can be implemented by data augmentation. For subspace feature learning, Maximum Coding Rate Reduction (MCR) objective can be used. Putting them together yields {\\em Neural Manifold Clustering and Embedding} (NMCE), a novel method for general purpose manifold clustering, which significantly outperforms autoencoder-based deep subspace clustering. Further, on more challenging natural image datasets, NMCE can also outperform other algorithms specifically designed for clustering. Qualitatively, we demonstrate that NMCE learns a meaningful and interpretable feature space. As the formulation of NMCE is closely related to several important Self-supervised learning (SSL) methods, we believe this work can help us build a deeper understanding on SSL representation learning.",
      "citation_histogram": [[2022, 5]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:1In3SbHwanAC",
      "title": "Byte-level recursive convolutional auto-encoder for text",
      "link": "https://arxiv.org/abs/1802.01817",
      "year": 2018,
      "cited_by": 5,
      "authors": ["Xiang Zhang", "Yann LeCun"],
      "description": "This article proposes to auto-encode text at byte-level using convolutional networks with a recursive architecture. The motivation is to explore whether it is possible to have scalable and homogeneous text generation at byte-level in a non-sequential fashion through the simple task of auto-encoding. We show that non-sequential text generation from a fixed-length representation is not only possible, but also achieved much better auto-encoding results than recurrent networks. The proposed model is a multi-stage deep convolutional encoder-decoder framework using residual connections, containing up to 160 parameterized layers. Each encoder or decoder contains a shared group of modules that consists of either pooling or upsampling layers, making the network recursive in terms of abstraction levels in representation. Results for 6 large-scale paragraph datasets are reported, in 3 languages including Arabic, Chinese and English. Analyses are conducted to study several properties of the proposed model.",
      "citation_histogram": [
        [2018, 3],
        [2019, 1],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:-4bc1-6Giq0C",
      "title": "L\u2019apprentissage profond",
      "link": "https://scholar.google.com/scholar?cluster=12548637897643796628&hl=en&oi=scholarr",
      "year": 2016,
      "cited_by": 5,
      "authors": ["Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [2018, 1],
        [2019, 1],
        [2020, 1],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:KbeHZ-DlqmcC",
      "title": "Fast training of convolutional networks through FFTS: International Conference on Learning Representations (ICLR2014), CBLS, April 2014",
      "link": "https://nyuscholars.nyu.edu/en/publications/fast-training-of-convolutional-networks-through-ffts-internationa",
      "year": 2014,
      "cited_by": 5,
      "authors": ["Michael Mathieu", "Mikael Henaff", "Yann LeCun"],
      "description": "Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges.",
      "citation_histogram": [
        [2021, 2],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:bnK-pcrLprsC",
      "title": "Fully Adaptive Visual Navigation for Autonomous Vehicles",
      "link": "http://www.net-scale.com/papers/armyscience2010/armyscience2010-paper.pdf",
      "year": 2010,
      "cited_by": 5,
      "authors": [
        "Marco Scoffier",
        "Urs Muller",
        "Yann LeCun",
        "Pierre Sermanet",
        "Benoit Corda",
        "Clement Farabet"
      ],
      "description": "Adaptive systems make for robust autonomous ground vehicle navigation able to react to changing environments and to operate in unknown areas. This paper demonstrates that adaptivity and machine learning techniques can reduce a system\u2019s sensitivity to sensor quality and calibration, enable the use of low cost and low power cameras, reduce the dependency on active sensors such as LIDAR, and simplify the process of reusing the same navigation system on different small and large hardware platforms. In particular, this paper presents an obstacle avoidance example that was solely implemented with end-to-end learning, a robust camera-based long range vision system which was integrated into a working robotics platform and employs a new technique called near-to-far learning as well as the results from countless outdoor field tests in natural environments.",
      "citation_histogram": [
        [2013, 4],
        [2014, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:zA6iFVUQeVQC",
      "title": "DjVu: un syst\u00e8me de compression d\u2019images pour la distribution r\u00e9ticulaire de documents num\u00e9ris\u00e9s",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.7298",
      "year": 2000,
      "cited_by": 5,
      "authors": [
        "L\u00e9on Bottou",
        "Patrick Haffner",
        "Yann LeCun",
        "P Horward",
        "Pascal Vincent",
        "Bill Riemers"
      ],
      "description": "Introduction Avec l'utilisation g'en'eralis' ee de l'Internet, avec les couts d'ecroissants des num'eriseurs et des disques, l'archivage, la transmission et la manipulation des documents se fait de plus en plus sur ordinateur et de moins en moins sur papier. L\" ecran de nos ordinateurs est en train de devenir le moyen privil'egi'e de consultation de documents parce qu'il permet un acces imm'ediata l'information. Les technologies de compression d'images bitonales (noir et blanc) de documents ont une longue histoire (cf.[14] et r'ef'erences). Une industrie florissante utilise des techniques standardis' ees tres bien accept'ees (Group 3, MMR/Group 4), parfois moins connues (JBIG), ou meme franchement obscures (JBIG2). Curieusement, jusqu'a pr'esent, il n'existait pas de standard permettant de traiter efficacement les documents en couleur. Le besoin d'une telle technologie se sont fait plus pressant ces dernieres ann'ees avec la g'en'eralisation de l'Internet.",
      "citation_histogram": [
        [2003, 1],
        [2004, 2],
        [2005, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:bJZ_LSxkz4EC",
      "title": "Neural-net applications in character recognition and document analysis",
      "link": "http://yann.lecun.com/exdb/publis/psgz/jackel-95.ps.gz",
      "year": 1995,
      "cited_by": 5,
      "authors": [
        "L Jackel",
        "M Battista",
        "H Baird",
        "Jan Ben",
        "Jane Bromley",
        "C Burges",
        "Eric Cosatto",
        "J Denker",
        "H Graf",
        "H Katseff",
        "Yann Le-Cun",
        "C Nohl",
        "Eduard Sackinger",
        "J Shamilian",
        "T Shoemaker",
        "C Stenard",
        "I Strom",
        "R Ting",
        "T Wood",
        "C Zuraw"
      ],
      "description": "A proven strength of neural-network methods is their application to character recognition and document analysis. In this paper we describe a neural-net Optical Character Recognizer (OCR), neural-net preprocessing, and neuralnet hardware accelerators that together comprise a high-performance character recognition system. We also describe applications in networkbased fax and bit-mapped text processing.",
      "citation_histogram": [
        [2009, 2],
        [2010, 1],
        [2011, 1],
        [2012, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:0KyAp5RtaNEC",
      "title": "Optical character recognition: A technology driver for neural networks",
      "link": "https://ieeexplore.ieee.org/abstract/document/112502/",
      "year": 1990,
      "cited_by": 5,
      "authors": [
        "RE Howard",
        "B Boser",
        "JS Denker",
        "HP Graf",
        "D Henderson",
        "W Hubbard",
        "LD Jackel",
        "Yann Le Cun",
        "HS Baird"
      ],
      "description": "It is shown that a neural net can perform handwritten digit recognition with state-of-the-art accuracy. The solution required automatic learning and generalization from thousands of training examples and also required designing into the system considerable knowledge about the task-neither engineering nor learning from examples alone would have sufficed. The resulting network is well suited for implementation on workstations or PCs and can take advantage of digital signal processors (DSPs) or custom VLSI.< >",
      "citation_histogram": [
        [1992, 2],
        [1993, 1],
        [1994, 1],
        [1995, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:zVX0Cq83Iy8C",
      "title": "Advances in Neural Information Processing Systems 2,[NIPS Conference, Denver, Colorado, USA, November 27-30, 1989]",
      "link": "https://scholar.google.com/scholar?cluster=14759455464177033981&hl=en&oi=scholarr",
      "year": 1989,
      "cited_by": 5,
      "authors": [
        "Y Lecun",
        "BE Boser",
        "JS Denker",
        "D Henderson",
        "LD Jackel"
      ],
      "description": null,
      "citation_histogram": [
        [2008, 1],
        [2009, 1],
        [2010, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:lM7bPffmjyEC",
      "title": "Using curvature information to improve back-propagation",
      "link": "https://nyuscholars.nyu.edu/en/publications/using-curvature-information-to-improve-back-propagation",
      "year": 1988,
      "cited_by": 5,
      "authors": ["Yann le Cun"],
      "description": "Among all the supervised learning algorithms, back-propagation (BP) is probably the most wi (l) dely used. Classical non-linear programming methods generally use an estimate of the Hessian matrix (matrix of second derivatives) to compute the weight modification at each iteration. They are derived from the well known Newton-Raphson algorithm. We propose a very rough approximation to the Newton method which just uses the diagonal terms of the Hessian matrix. These terms give information about the curvature of the error surface in directions parallel to the weight space axes. This information can be used to scale the learning rates for each weight independently. We show that it is possible to approximate the diagonal terms of the Hessian matrix using a back-propagation procedure very similar to the one used for the first derivatives.",
      "citation_histogram": [
        [1988, 1],
        [1989, 2],
        [1990, 1],
        [1991, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:njwWRc9PkvUC",
      "title": "Une procedure d\u2019apprentissage pour reseau a seuil asymetrique. proceedings of Cognitiva 85",
      "link": "https://scholar.google.com/scholar?cluster=2656783343943842583&hl=en&oi=scholarr",
      "year": 1985,
      "cited_by": 5,
      "authors": ["Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [2018, 2],
        [2019, 2],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:OxQqgzTNpSoC",
      "title": "Sparse Coding with Multi-Layer Decoders using Variance Regularization",
      "link": "https://arxiv.org/abs/2112.09214",
      "year": 2021,
      "cited_by": 4,
      "authors": ["Katrina Evtimova", "Yann LeCun"],
      "description": "Sparse coding with an  penalty and a learned linear dictionary requires regularization of the dictionary to prevent a collapse in the  norms of the codes. Typically, this regularization entails bounding the Euclidean norms of the dictionary's elements. In this work, we propose a novel sparse coding protocol which prevents a collapse in the codes without the need to regularize the decoder. Our method regularizes the codes directly so that each latent code component has variance greater than a fixed threshold over a set of sparse representations for a given set of inputs. Furthermore, we explore ways to effectively train sparse coding systems with multi-layer decoders since they can model more complex relationships than linear dictionaries. In our experiments with MNIST and natural image patches, we show that decoders learned with our approach have interpretable features both in the linear and multi-layer case. Moreover, we show that sparse autoencoders with multi-layer decoders trained using our variance regularization method produce higher quality reconstructions with sparser representations when compared to autoencoders with linear dictionaries. Additionally, sparse representations obtained with our variance regularization approach are useful in the downstream tasks of denoising and classification in the low-data regime.",
      "citation_histogram": [[2022, 4]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:t3sMychFT9UC",
      "title": "Yann LeCun",
      "link": "https://scholar.google.com/scholar?cluster=3867998404434871547&hl=en&oi=scholarr",
      "year": 2018,
      "cited_by": 4,
      "authors": ["Yann LeCun", "Martin Ford"],
      "description": "This article relies on references to primary sources or sources affiliated with the subject, rather than references from independent authors and third-party publications. Please add more appropriate citations from reliable sources.(July 2009)",
      "citation_histogram": [
        [2017, 1],
        [2018, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Cv-mv52rCCkC",
      "title": "Tutorial: Geometric deep learning on graphs and manifolds",
      "link": "https://scholar.google.com/scholar?cluster=1594127423833290748&hl=en&oi=scholarr",
      "year": 2018,
      "cited_by": 4,
      "authors": [
        "Michael Bronstein",
        "X Bresson",
        "A Szlam",
        "J Bruna",
        "Y LeCun"
      ],
      "description": "Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclideanstructured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graphand 3D shape analysis and show that it consistently outperforms previous approaches.",
      "citation_histogram": [
        [2017, 1],
        [2018, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:8Xgff_V0N9gC",
      "title": "DCL System Using Deep Learning Approaches for Land-Based or Ship-Based Real Time Recognition and Localization of Marine Mammals",
      "link": "https://apps.dtic.mil/sti/citations/AD1014344",
      "year": 2015,
      "cited_by": 4,
      "authors": [
        "Peter J Dugan",
        "Christopher W Clark",
        "Yann A LeCun",
        "Sofie M Van Parijs"
      ],
      "description": "The ONR DCL grant focuses on advancing state-of-the-art of data-mining for the bioacoustics community through researching and creating new technologies, algorithms and systems to decipher and understand very large passive acoustic datasets. The long-term goal is to develop advanced computational systems and algorithms that will provide scientists the ability to efficiently harvest animal localizations from large, complex datasets. The newly developed systems provides efficient, high performance processing of acoustic sounds by allowing a state-of-the-art technology to host algorithms for advanced detection and classification and other data-mining strategies.Descriptors:",
      "citation_histogram": [
        [2019, 3],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:yuCoZvLJRl8C",
      "title": "Object Recognition, Computer Vision, and the Caltech 101: A Response to Pinto et al",
      "link": "https://scholar.google.com/scholar?cluster=8390791735259137635&hl=en&oi=scholarr",
      "year": 2008,
      "cited_by": 4,
      "authors": [
        "Yann LeCun",
        "David G Lowe",
        "Jitendra Malik",
        "Jim Mutch",
        "Pietro Perona",
        "Tomaso Poggio"
      ],
      "description": "Readers of the recent paper \u201cWhy is Real-World Visual Object Recognition Hard?\u201d[8] who are unfamiliar with the literature on computer vision are likely to come away with the impression that the problem of making visual recognition invariant with respect to position, scale, and pose has been overlooked. We would therefore like to clarify two main points.(1) The paper criticizes the popular Caltech 101 benchmark dataset for not containing images of objects at a variety of positions, scales, and poses. It is true that Caltech 101 does not test these kinds of variability; however, this omission is intentional. Techniques for addressing these issues were the focus of much work in the 1980s [11]. For example, datasets like that of Murase and Nayar [6] focused on the problem of recognizing specific objects from a variety of 3d poses, but did not address the issue of object categories and the attendant intra-category variation in shape and texture. Pinto et al.\u2019s synthetic dataset is in much the same spirit as Murase and Nayar\u2019s. Caltech 101 was created to test a system [4, 3] that was already position, scale, and pose invariant, with the goal of focusing on the more difficult problem of categorization. Its lack of position, scale, and pose variation is stated explicitly on the Caltech 101 website [2], where the dataset is available for download, and is often explicitly restated in later papers that use the dataset (including three of the five cited in Fig. 1). This is not to say that Caltech 101 is without problems. For example, as the authors state, correlation of object classes and backgrounds is a concern, and the relative success of their \u201ctoy\u201d model does seem to suggest that the\u00a0\u2026",
      "citation_histogram": [
        [2011, 1],
        [2012, 1],
        [2013, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:a0OBvERweLwC",
      "title": "On-line learning of long-range obstacle detection for off-road robots",
      "link": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.1168&rep=rep1&type=pdf",
      "year": 2006,
      "cited_by": 4,
      "authors": [
        "Raia Hadsell",
        "Pierre Sermanet",
        "Jan Ben",
        "Jeff Han",
        "Sumit Chopra",
        "M Ranzato",
        "Yury Sulsky",
        "Beat Flepp",
        "Urs Muller",
        "Yann LeCun"
      ],
      "description": "The method of choice for vision-based driving in off-road mobile robots is to construct a traversibility map of the environment using stereo vision. In the most common approach, a stereo matching algorithm, applied to images from a pair of stereo cameras, produces a \u201cpoint-cloud\u201d, in which the most visible pixels are given an XYZ position relative to the robot. A traversibility map can then be derived using various heuristics, such as counting the number of points that are above the ground plane in a given map cell. Maps from multiple frames are assembled in a global map in which path finding algorithms are run [2, 3, 1]. The performance of such stereo-based methods is limited, because stereo-based distance estimation is often unreliable above 8 or 10 meters (for typical camera configurations and resolutions). This may cause the system drive as if in a self-imposed \u201cfog\u201d, driving into dead-ends, and taking time to discover distant pathways that are obvious to a human observer.",
      "citation_histogram": [
        [2006, 1],
        [2007, 3]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:r5Jnz2hFFzMC",
      "title": "Conversion of digital documents to multilayer raster formats",
      "link": "https://nyuscholars.nyu.edu/en/publications/conversion-of-digital-documents-to-multilayer-raster-formats",
      "year": 2001,
      "cited_by": 4,
      "authors": ["Leon Bottou", "Patrick Haffner", "Yann Lecun"],
      "description": "Conversion of digital documents to multilayer raster formats \u2014 NYU Scholars Skip to main \nnavigation Skip to search Skip to main content NYU Scholars Home NYU Scholars Logo Help \n& FAQ Home Profiles Research Units Research output Search by expertise, name or affiliation \nConversion of digital documents to multilayer raster formats Leon Bottou, Patrick Haffner, Yann \nLecun Computer Science Research output: Chapter in Book/Report/Conference proceeding \u203a \nConference contribution Overview Original language English (US) Title of host publication \nProceedings of the International Conference on Document Analysis and Recognition, \nSeptember 2001 State Published - 2001 Cite this APA Standard Harvard Vancouver Author \nBIBTEX RIS Bottou, L., Haffner, P., & Lecun, Y. (2001). Conversion of digital documents to \nmultilayer raster formats. In Proceedings of the International Conference on Document Analysis \u2026",
      "citation_histogram": [
        [2002, 3],
        [2003, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:0_RpHV3rhqYC",
      "title": "On-line handwriting recognition with neural networks: Spatial representation versus temporal representation",
      "link": "https://nyuscholars.nyu.edu/en/publications/on-line-handwriting-recognition-with-neural-networks-spatial-repr",
      "year": 1993,
      "cited_by": 4,
      "authors": [
        "Yann Lecun",
        "Yoshua Bengio",
        "D Henderson",
        "A Weisbuch",
        "H Weissman",
        "LD Jackel"
      ],
      "description": "On-line handwriting recognition with neural networks: Spatial representation versus \ntemporal representation \u2014 NYU Scholars Skip to main navigation Skip to search Skip to \nmain content NYU Scholars Home NYU Scholars Logo Help & FAQ Home Profiles \nResearch Units Research output Search by expertise, name or affiliation On-line \nhandwriting recognition with neural networks: Spatial representation versus temporal \nrepresentation Yann Lecun, Yoshua Bengio, D. Henderson, A. Weisbuch, H. Weissman, LD \nJackel Computer Science Research output: Chapter in Book/Report/Conference \nproceeding \u203a Conference contribution Overview Original language English (US) Title of host \npublication Proceedings of the International Conference on Handwriting and Drawing \nPublisher Ecole Nationale Superieure des Telecommunications State Published - 1993 Cite \nthis APA Standard Harvard Vancouver Author BIBTEX RIS \u2026",
      "citation_histogram": [
        [1994, 1],
        [1995, 1],
        [1996, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:RzXy5zvoWjEC",
      "title": "L. bottou, Y. bengio, and P. haffner,\u201cGradient-based learning applied to document recognition,\u201d",
      "link": "https://scholar.google.com/scholar?cluster=15474681008967694683&hl=en&oi=scholarr",
      "year": 1990,
      "cited_by": 4,
      "authors": ["Y Lecun"],
      "description": null,
      "citation_histogram": [
        [1999, 1],
        [2000, 1],
        [2001, 1],
        [2002, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:q3SxJD15z-gC",
      "title": "The feasibility of applying numerical optimization techniques to back-propagation",
      "link": "https://scholar.google.com/scholar?cluster=4417937486167702780&hl=en&oi=scholarr",
      "year": 1988,
      "cited_by": 4,
      "authors": ["Sue Becker", "Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [1988, 1],
        [1989, 1],
        [1990, 1],
        [1991, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:5kgRglCLipYC",
      "title": "others.(1989)",
      "link": "https://scholar.google.com/scholar?cluster=2362115380589433968&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 4,
      "authors": ["Yann LeCun"],
      "description": null,
      "citation_histogram": [
        [2015, 1],
        [2016, 2],
        [2017, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=300&pagesize=100&citation_for_view=WLN3QrAAAAAJ:M6kHaddf_34C",
      "title": "HPC and Bioacoustics, Practical Considerations for Detection Classification for Big Data, ICML 2013",
      "link": "https://scholar.google.com/scholar?cluster=15419726889579794068&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 4,
      "authors": [
        "P Dugan",
        "Y LeCun",
        "S Van Parijs",
        "D Ponirakis",
        "M Popescu",
        "M Pourhomayoun",
        "Y Shiu",
        "A Rice",
        "C Clark"
      ],
      "description": null,
      "citation_histogram": [
        [2013, 1],
        [2014, 1],
        [2015, 1],
        [2016, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:A8NefVh_EAoC",
      "title": "Masked siamese convnets",
      "link": "https://arxiv.org/abs/2206.07700",
      "year": 2022,
      "cited_by": 3,
      "authors": ["Li Jing", "Jiachen Zhu", "Yann LeCun"],
      "description": "Self-supervised learning has shown superior performances over supervised methods on various vision benchmarks. The siamese network, which encourages embeddings to be invariant to distortions, is one of the most successful self-supervised visual representation learning approaches. Among all the augmentation methods, masking is the most general and straightforward method that has the potential to be applied to all kinds of input and requires the least amount of domain knowledge. However, masked siamese networks require particular inductive bias and practically only work well with Vision Transformers. This work empirically studies the problems behind masked siamese networks with ConvNets. We propose several empirical designs to overcome these problems gradually. Our method performs competitively on low-shot image classification and outperforms previous methods on object detection benchmarks. We discuss several remaining issues and hope this work can provide useful data points for future general-purpose self-supervised learning.",
      "citation_histogram": [[2022, 3]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:0x7hHEPTrR8C",
      "title": "Deep generative models create new and diverse protein structures",
      "link": "https://www.mlsb.io/papers_2021/MLSB2021_Deep_generative_models_create.pdf",
      "year": 2021,
      "cited_by": 3,
      "authors": ["Zeming Lin", "Tom Sercu", "Yann LeCun", "Alexander Rives"],
      "description": "We explore the use of modern variational autoencoders for generating protein structures. Models are trained across a diverse set of natural protein domains. Threedimensional structures are encoded implicitly in the form of an energy function that expresses constraints on pairwise distances and angles. Atomic coordinates are recovered by optimizing the parameters of a rigid body representation of the protein chain to fit the constraints. The model generates diverse structures across a variety of folds, and exhibits local coherence at the level of secondary structure, generating alpha helices and beta sheets, as well as globally coherent tertiary structure. A number of generated protein sequences have high confidence predictions by AlphaFold that agree with their designs. The majority of these have no significant sequence homology to natural proteins.Most designed proteins are variations on existing proteins. It is of great interest to create de novo proteins that go beyond what has been invented by nature. A line of recent work has explored generative models for protein structures [1, 2, 3, 4, 5, 6]. The main challenge for a generative model is to propose stable structures that can be realized as the minimum energy state for a protein sequence, ie the endpoint of folding. The space of possible three-dimensional conformations of a protein sequence is exponentially large [7], but out of this set of possible conformations, most do not correspond to stable realizable structures.",
      "citation_histogram": [[2022, 3]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:2PBQaVm3t-0C",
      "title": "Methods, systems, and media for detecting spoofing in mobile authentication",
      "link": "https://patents.google.com/patent/US10735959B2/en",
      "year": 2020,
      "cited_by": 3,
      "authors": [
        "Yann LeCun",
        "Fengjun Lv",
        "Dushyant Goyal",
        "Yang Wang",
        "Adam Perold"
      ],
      "description": "Provided herein are devices, systems, and methods for detecting spoofing of a 3D object, using a 2D representation, in a mobile object authentication process, comprising capturing image data of the 3D object by a front-facing camera, to record a current spatial characteristic of the 3D object, while a front-facing screen displays an authentication pattern comprising a plurality of regions, wherein at least one of the regions varies in at least one of: brightness, position, size, shape, and color over time causing a variance of lighting effects which create highlights and shadows on the 3D object over time. The devices, systems, and methods thereby provide an efficient and secure process for determining if spoofing of the 3D object, using a 2D representation, is attempted in a mobile authentication process, by comparing the current spatial characteristic of the 3D object with a stored reference spatial characteristic of the 3D\u00a0\u2026",
      "citation_histogram": [
        [2021, 1],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:SAguW2jnL4UC",
      "title": "The deep learning revolution",
      "link": "https://rse.org.uk/wp-content/uploads/2021/08/Hintons-Presentation_20190718.pdf",
      "year": 2019,
      "cited_by": 3,
      "authors": ["Geoffrey Hinton", "Y LeCun"],
      "description": "UCL Tutorial July 17 2009 Page 1 The Deep Learning Revolution Geoffrey Hinton Google \nBrain Team & Vector Institute Page 2 Two paradigms for Artificial Intelligence The logic-inspired \napproach The essence of intelligence is using symbolic rules to manipulate symbolic \nexpressions. We should focus on reasoning. The biologically-inspired approach The essence \nof intelligence is learning the strengths of the connections in a neural network. We should focus \non learning and perception. Page 3 Two views of internal representations \u2022 Internal \nrepresentations are symbolic expressions. \u2013 A programmer can give them to a computer using \nan unambiguous language. \u2013 New representations can be derived by applying rules to existing \nrepresentations. \u2022 Internal representations are nothing like language. \u2013 They are large vectors \nof neural activity. \u2013 They have direct causal effects on other vectors of neural activity. \u2013 These \u2026",
      "citation_histogram": [
        [2020, 1],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:uh8FjILnQOkC",
      "title": "Geometric deep learning on graphs and manifolds",
      "link": "https://qdata.github.io/deep2Read/talks2019/19sCourse/20190220-Arshdeep-Geometric.pdf",
      "year": 2017,
      "cited_by": 3,
      "authors": ["M Brinstein", "Joan Bruna", "X Bresson", "Y LeCun"],
      "description": "Geometric Deep Learning on Graphs and Manifolds - Michael Bronstein1,2,3Joan \nBruna6Arthur Szlam5,Xavier Bresson4Yann LeCun5,6 Page 1 Geometric Deep Learning on \nGraphs and Manifolds Michael Bronstein1,2,3Joan Bruna6Arthur Szlam5,Xavier Bresson4Yann \nLeCun5,6 https://qdata.github.io/deep2Read Presenter: Arshdeep Sekhon \nhttps://qdata.github.io/deep2Read Geometric Deep Learning on Graphs and M https://qdata.github.io/deep2Read \n1 / 41 Page 2 1 Motivation 2 Basics of Euclidean CNNs 3 Basics Basics: Graph Theory Basics: \nRiemannian manifolds Using Dirichlet Energy 4 Spectral Domain CNNs 5 GNNs: Spatial View 6 \nSpatial Domain Geometric Deep Learning 7 Applications Geometric Deep Learning on Graphs \nand M https://qdata.github.io/deep2Read 2 / 41 Page 3 Motivation What kind of geometric \nstructure found in images/text/etc exploited by CNNs How to use this universal property \u2026",
      "citation_histogram": [
        [2018, 1],
        [2019, 1],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:f8T_-ThkUo0C",
      "title": "Guest editorial: Deep learning",
      "link": "https://nyuscholars.nyu.edu/en/publications/guest-editorial-deep-learning",
      "year": 2015,
      "cited_by": 3,
      "authors": [
        "Ranzato Marc\u2019Aurelio\u2019A",
        "Geoffrey Hinton",
        "Yann LeCun"
      ],
      "description": "Guest Editorial: Deep Learning \u2014 NYU Scholars Skip to main navigation Skip to search \nSkip to main content NYU Scholars Home NYU Scholars Logo Help & FAQ Home Profiles \nResearch Units Research output Search by expertise, name or affiliation Guest Editorial: \nDeep Learning Marc\u2019Aurelio \u2019A Ranzato, Geoffrey Hinton, Yann LeCun Computer Science \nResearch output: Contribution to journal \u203a Editorial \u203a peer-review Overview Fingerprint \nOriginal language English (US) Pages (from-to) 1-2 Number of pages 2 Journal \nInternational Journal of Computer Vision Volume 113 Issue number 1 DOIs https://doi.org/10.1007/s11263-015-0813-1 \nState Published - May 1 2015 ASJC Scopus subject areas Software Computer Vision and \nPattern Recognition Artificial Intelligence Access to Document 10.1007/s11263-015-0813-1 \nOther files and links Link to publication in Scopus Link to citation list in Scopus Cite this APA \u2026",
      "citation_histogram": [
        [2019, 2],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:EsO17nB32j8C",
      "title": "Marc\u2019Aurelio Ranzato",
      "link": "https://scholar.google.com/scholar?cluster=17306265148794597582&hl=en&oi=scholarr",
      "year": 2013,
      "cited_by": 3,
      "authors": ["Yann Lecun"],
      "description": "My research interests are in the area of Machine Learning and Pattern Recognition. In particular, my focus has been on algorithms to learn rich internal representations of data and invariant feature hierarchies. These methods go beyond traditional supervised learning methods, and exploit unsupervised, semi-supervised and multi-task learning algorithms to train large systems with little supervision. I have developed expertise in a wide range of domains and I have been using a variety of tools including deep learning architectures, gradient-based learning methods, energy-based models, graphical models, variational methods, kernel methods, and computational neural science models of vision. I have applied these models to a variety of applications in vision, such as generic object recognition, handwriting recognition, image denoising and image compression. I have also worked on natural language processing for\u00a0\u2026",
      "citation_histogram": [
        [2013, 1],
        [2014, 1],
        [2015, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:qLQjcG-1Y3AC",
      "title": "Discriminative recurrent sparse auto-encoders: 1st International Conference on Learning Representations, ICLR 2013",
      "link": "https://nyuscholars.nyu.edu/en/publications/discriminative-recurrent-sparse-auto-encoders-1st-international-c",
      "year": 2013,
      "cited_by": 3,
      "authors": ["Jason Tyler Rolfe", "Yann LeCun"],
      "description": "We present the discriminative recurrent sparse auto-encoder model, comprising a recurrent encoder of rectified linear units, unrolled for a fixed number of iterations, and connected to two linear decoders that reconstruct the input and predict its supervised classification. Training via backpropagation-through-time initially minimizes an unsupervised sparse reconstruction error; the loss function is then augmented with a discriminative term on the supervised classification. The depth implicit in the temporally-unrolled form allows the system to exhibit far more representational power, while keeping the number of trainable parameters fixed. From an initially unstructured network the hidden units differentiate into categorical-units, each of which represents an input prototype with a well-defined class; and part-units representing deformations of these prototypes. The learned organization of the recurrent encoder is hierarchical: part-units are driven directly by the input, whereas the activity of categorical-units builds up over time through interactions with the part-units. Even using a small number of hidden units per layer, discriminative recurrent sparse auto-encoders achieve excellent performance on MNIST.",
      "citation_histogram": [
        [2020, 1],
        [2021, 1],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:WA5NYHcadZ8C",
      "title": "NeuFlow: A Runtime Reconfigurable Dataflow Architecture for Vision (abstract)",
      "link": null,
      "year": 2011,
      "cited_by": 3,
      "authors": ["Cl\u00e9ment Farabet", "Yann LeCun", "Eugenio Culurciello"],
      "description": null,
      "citation_histogram": [
        [2013, 1],
        [2014, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:ZJWHNt7Cjk4C",
      "title": "Tutorial: learning deep architectures",
      "link": "http://www.cs.toronto.edu/~rsalakhu/deeplearning/yoshua_icml2009.pdf",
      "year": 2009,
      "cited_by": 3,
      "authors": ["Yoshua Bengio", "Y LeCun"],
      "description": "Tutorial: Learning Deep Architectures Page 1 Tutorial: Learning Deep Architectures Yoshua \nBengio, U. Montreal Yann LeCun, NYU ICML Workshop on Learning Feature Hierarchies, June \n18th, 2009, Montreal Page 2 Deep Motivations \u25aa Brains have a deep architecture \u25aa Humans \norganize their ideas hierarchically, through composition of simpler ideas \u25aa Unsufficiently deep \narchitectures can be exponentially inefficient \u25aa Distributed (possibly sparse) representations \nare necessary to achieve non-local generalization \u25aa Intermediate representations allow \nsharing statistical strength Page 3 Deep Architecture in the Brain Retina Area V1 Area V2 \nArea V4 pixels Edge detectors Primitive shape detectors Higher level visual abstractions Page \n4 Deep Architecture in our Mind \u25aa Humans organize their ideas and concepts hierarchically \u25aa \nHumans first learn simpler concepts and then compose them to represent more abstract ones \u25aa \u2026",
      "citation_histogram": [
        [2014, 1],
        [2015, 1],
        [2016, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:dTyEYWd-f8wC",
      "title": "Optical character recogntion for automatic teller machines",
      "link": "https://www.worldscientific.com/doi/abs/10.1142/9789812816955_0044",
      "year": 1998,
      "cited_by": 3,
      "authors": [
        "LD Jackel",
        "Y Lecun",
        "CE STENARD",
        "BI STROM",
        "D Sharman",
        "D Zuckert"
      ],
      "description": " Self-service banking, in the form of automatic teller machines (ATMs), has dramatically altered the way people interact with their banks. Now people can withdraw cash from their accounts far from their banks' office at any time. Now, by adding Optical Character Recognition (OCR), people will also be able to deposit checks, cash checks, and pay bills directly using an on-line system.",
      "citation_histogram": [[2021, 2]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:rO6llkc54NcC",
      "title": "Method and apparatus for symbol recognition using multidimensional preprocessing",
      "link": "https://patents.google.com/patent/US5625708A/en",
      "year": 1997,
      "cited_by": 3,
      "authors": ["Yann A LeCun"],
      "description": "Data samples describing a plurality of micro-segments that compose a symbol to be recognized are received from a device such as an electronic pad. A preprocessor maps the micro-segments into cells of an array that has several feature dimensions. The preprocessor assigns values to the cells based on the length of a micro-segment associated with the cell, and how well the features of the associated micro-segment correspond to the feature label of the cell. The cell values are used as inputs to a neural network that identifies the symbol. In one embodiment, recognizing symbols from large groups of symbols is facilitated by using a plurality of neural networks. Each neural network is trained to recognize symbols from a different subgroup of symbols, and each neural network can determine whether the symbol belongs to its subgroup.",
      "citation_histogram": [
        [2006, 1],
        [2007, 1],
        [2008, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:t6hKUfryX1MC",
      "title": "Hardware Requirements for Neural Network Pattern Classifiers a Case Study and Implementation",
      "link": "https://nyuscholars.nyu.edu/en/publications/hardware-requirements-for-neural-network-pattern-classifiers-a-ca",
      "year": 1992,
      "cited_by": 3,
      "authors": [
        "Eduard Sackinger",
        "Jane Bromley",
        "Yann Lecun",
        "Lawrence D Jackel"
      ],
      "description": "Hardware Requirements for Neural Network Pattern Classifiers a Case Study and \nImplementation \u2014 NYU Scholars Skip to main navigation Skip to search Skip to main \ncontent NYU Scholars Home NYU Scholars Logo Help & FAQ Home Profiles Research \nUnits Research output Search by expertise, name or affiliation Hardware Requirements for \nNeural Network Pattern Classifiers a Case Study and Implementation Eduard Sackinger, \nJane Bromley, Yann Lecun, Lawrence D. Jackel Computer Science Research output: \nContribution to journal \u203a Article \u203a peer-review Overview Fingerprint Original language \nEnglish (US) Pages (from-to) 32-40 Number of pages 9 Journal IEEE Micro Volume 12 \nIssue number 1 DOIs https://doi.org/10.1109/40.124378 State Published - 1992 ASJC \nScopus subject areas Software Hardware and Architecture Electrical and Electronic \nEngineering Access to Document 10.1109/40.124378 Other \u2026",
      "citation_histogram": [
        [1996, 1],
        [1997, 1],
        [1998, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:3DN2I6VP0lQC",
      "title": "Steels (eds)'Connectionism in perspective', Elsevier 1989. Generalization and Network Design Strategies",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.476.479",
      "year": 1989,
      "cited_by": 3,
      "authors": ["Y Le Cun", "Yann Le Cun"],
      "description": "An interestmg property of connectiomst systems is their ability to learn from examples. Although most recent work in the field concentrates on reducing learning times, the most important feature of a learning ma-chine is its generalization performance. It is usually accepted that good generalization performance on real-world problems cannot be achieved unless some a pnon knowledge about the task is butlt Into the system. Back-propagation networks provide a way of specifymg such knowledge by imposing constraints both on the architecture of the network and on its weights. In general, such constramts can be considered as particular transformations of the parameter space Building a constramed network for image recogmtton appears to be a feasible task. We descnbe a small handwritten digit recogmtion problem and show that, even though the problem is linearly separable, single layer networks exhibit poor generalizatton performance. Multtlayer constrained",
      "citation_histogram": [
        [2018, 1],
        [2019, 1],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Ib8FQH8mdS0C",
      "title": "Intra-instance vicreg: Bag of self-supervised image patch embedding",
      "link": "https://arxiv.org/abs/2206.08954",
      "year": 2022,
      "cited_by": 2,
      "authors": ["Yubei Chen", "Adrien Bardes", "Zengyi Li", "Yann LeCun"],
      "description": "Recently, self-supervised learning (SSL) has achieved tremendous empirical advancements in learning image representation. However, our understanding and knowledge of the representation are still limited. This work shows that the success of the SOTA siamese-network-based SSL approaches is primarily based on learning a representation of image patches. Particularly, we show that when we learn a representation only for fixed-scale image patches and aggregate different patch representations linearly for an image (instance), it can achieve on par or even better results than the baseline methods on several benchmarks. Further, we show that the patch representation aggregation can also improve various SOTA baseline methods by a large margin. We also establish a formal connection between the SSL objective and the image patches co-occurrence statistics modeling, which supplements the prevailing invariance perspective. By visualizing the nearest neighbors of different image patches in the embedding space and projection space, we show that while the projection has more invariance, the embedding space tends to preserve more equivariance and locality. Finally, we propose a hypothesis for the future direction based on the discovery of this work.",
      "citation_histogram": [[2022, 2]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:s9piBQ-TX4wC",
      "title": "Learning with Reflective Likelihoods",
      "link": "https://openreview.net/forum?id=SJlh2jR9FX",
      "year": 2018,
      "cited_by": 2,
      "authors": [
        "Adji B Dieng",
        "Kyunghyun Cho",
        "David M Blei",
        "Yann LeCun"
      ],
      "description": "Models parameterized by deep neural networks have achieved state-of-the-art results in many domains. These models are usually trained using the maximum likelihood principle with a finite set of observations. However, training deep probabilistic models with maximum likelihood can lead to the issue we refer to as input forgetting. In deep generative latent-variable models, input forgetting corresponds to posterior collapse---a phenomenon in which the latent variables are driven independent from the observations. However input forgetting can happen even in the absence of latent variables. We attribute input forgetting in deep probabilistic models to the finite sample dilemma of maximum likelihood. We formalize this problem and propose a learning criterion---termed reflective likelihood---that explicitly prevents input forgetting. We empirically observe that the proposed criterion significantly outperforms the maximum likelihood objective when used in classification under a skewed class distribution. Furthermore, the reflective likelihood objective prevents posterior collapse when used to train stochastic auto-encoders with amortized inference. For example in a neural topic modeling experiment, the reflective likelihood objective leads to better quantitative and qualitative results than the variational auto-encoder and the importance-weighted auto-encoder.",
      "citation_histogram": [
        [2019, 1],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:jtusTj6o6osC",
      "title": "Unsupervised learning",
      "link": "https://www.cmi.ac.in/~madhavan/courses/dmml2022/lectures/Lecture13-07mar2022.pdf",
      "year": 2016,
      "cited_by": 2,
      "authors": ["Yann LeCun"],
      "description": "Lecture 13: 7 March, 2022 Page 1 Lecture 13: 7 March, 2022 Madhavan Mukund https://www.cmi.ac.in/~madhavan \nData Mining and Machine Learning January\u2013May 2022 Page 2 Unsupervised learning \u2022 \nSupervised learning requires labelled data \u2022 Vast majority of data is unlabelled \u2022 What \ninsights can you get into unlabelled data? \u201cIf intelligence was a cake, unsupervised learning \nwould be the cake, supervised learning would be the icing on the cake ...\u201d - Yann LeCun ACM \nTuring Award 2018 Page 3 Applications \u2022 Customer segmentation \u2022 Marketing campaigns \u2022 \nAnomaly detection \u2022 Outliers \u2022 Semi-supervised learning \u2022 Propagate limited labels \u2022 Image \nsegmentation \u2022 Object detection Page 4 Clustering \u2022 Find natural groups of data \u2022 Define \na distance measure \u2022 Group together data that is close together \u2022 Top down \u2022 Partition \ndata into clusters \u2022 Bottom up \u2022 Group items into clusters Page 5 Top down clustering K \u2026",
      "citation_histogram": [
        [2019, 1],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:9wUeeRLfbNYC",
      "title": "The bottleneck in human letter recognition: A computational model",
      "link": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.707.9696&rep=rep1&type=pdf",
      "year": 2014,
      "cited_by": 2,
      "authors": ["AJ Ziskind", "O H\u00e9naff", "Y LeCun", "DG Pelli"],
      "description": "We have implemented two machine-learning models of object recognition by human observers. Both models capture three hallmarks of human performance that cannot be accounted for by template matching:(1) spatial frequency channels,(2) crowding,(3) effects of letter complexity. One model is a Convolutional Neural Network (ConvNet), and the other is a texture statistics model followed by a linear classifier. With appropriate hyper-parameters and training, both models account for spatial-frequency channels, crowding, and effects of letter complexity.",
      "citation_histogram": [
        [2015, 1],
        [2016, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:AkQCnCHIR28C",
      "title": "Indoor semantic segmentation using depth information: 1st international conference on learning representations, iclr 2013",
      "link": "https://nyuscholars.nyu.edu/en/publications/indoor-semantic-segmentation-using-depth-information-1st-internat",
      "year": 2013,
      "cited_by": 2,
      "authors": [
        "Camille Couprie",
        "Cl\u00e9ment Farabet",
        "Laurent Najman",
        "Yann LeCun"
      ],
      "description": "This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. We obtain state-of-the-art on the NYU-v2 depth dataset with an accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos sequences that could be processed in real-time using appropriate hardware such as an FPGA.",
      "citation_histogram": [
        [2020, 1],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:h0mLeC6b6wcC",
      "title": "Building heterogeneous platforms for end-to-end online learning based on dataflow computing design. 78",
      "link": "http://videolectures.net/site/normal_dl/tag=81974/nipsworkshops2010_corda_bhp_01.pdf",
      "year": 2010,
      "cited_by": 2,
      "authors": [
        "Benoit Corda",
        "Cl\u00e9ment Farabet",
        "Marco Scoffier",
        "Yann LeCun"
      ],
      "description": "Building Heterogeneous Platforms for End-to-end Online Learning Based on Dataflow \nComputing Design Page 1 Building Heterogeneous Platforms for End-to-end Online Learning \nBased on Dataflow Computing Design Benoit Corda, Cl\u00e9ment Farabet, Marco Scoffier and \nYann Lecun Saturday, December 11, 2010 Page 2 LAGR: a Near-to-far obstacle detection \nOff-line Training Feature Extraction Linear Classifier Band with On-line Training Stereo vision \nFind Obstacles (short range) Stereo input Inference : 1 frame/sec Saturday, December 11, \n2010 Page 3 X + % MUX. X + % MUX. X + % MUX. X + % MUX. X + \u2211\u03c0 % MUX. X + % MUX. \nX + % MUX. X + % MUX. X + % MUX. Control & Config Smart DMA Configurable Route Global \nData Lines Runtime Config Bus Off-chip Memory Mem \u2211\u03c0 Mem \u2211\u03c0 Mem \u2211\u03c0 Mem \u2211\u03c0 Mem \n\u2211\u03c0 Mem \u2211\u03c0 Mem \u2211\u03c0 Mem \u2211\u03c0 Mem A Runtime Reconfigurable Dataflow Architecture PT PT \u2026",
      "citation_histogram": [
        [2012, 1],
        [2013, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:P9oYG7HA76QC",
      "title": "An fpga-based stream processor for embedded real-time vision with convolutional networks",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.526.355",
      "year": 2009,
      "cited_by": 2,
      "authors": ["Cyril Poulet", "Yann Lecun"],
      "description": "Many recent visual recognition systems can be seen as being composed of multiple layers of convolutional filter banks, interspersed with various types of non-linearities. This includes Convolutional Networks, HMAX-type archi-tectures, as well as systems based on dense SIFT features or Histogram of Gradients. This paper describes a highly-compact and low power embedded system that can run such vision systems at very high speed. A custom board built around a Xilinx Virtex-4 FPGA was built and tested. It mea-sures 70\u00d7 80 mm, and the complete system\u2014FPGA, cam-era, memory chips, flash\u2014consumes 15 watts in peak, and is capable of more than 4\u00d7 109 multiply-accumulate oper-ations per second in real vision application. This enables real-time implementations of object detection, object recog-nition, and vision-based navigation algorithms in small-size robots, micro-UAVs, and hand-held devices. Real-time face detection is demonstrated, with speeds of 10 frames per sec-ond at VGA resolution. 1.",
      "citation_histogram": [[2018, 1]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:WIzaTCs-0dQC",
      "title": "End-to-end learning of vision-based obstacle avoidance for off-road robots",
      "link": "https://scholar.google.com/scholar?cluster=1251486220576836697&hl=en&oi=scholarr",
      "year": 2004,
      "cited_by": 2,
      "authors": [
        "Yann LeCun",
        "Eric Cosatto",
        "Jan Ben",
        "Urs Muller",
        "B Flepp"
      ],
      "description": "Recent efforts have attacked the problem by relying on a multiplicity of sensors, including laser range finders, Radars, and stereo cameras with dedicated hardware for depth map extraction. While active sensors make the problem considerably simpler, there seems to be an interest from potential users for purely passive systems that rely exclusively on camera input. Since active sensors tend to be rather bulky and power hungry, using only cameras allows levels of miniaturizations that are not otherwise possible.Avoiding obstacle but relying solely on camera input requires solving a highly complex vision problem: mapping pixels from two (or more) cameras to a steering angle. The conventional approach is to look for vertical edge features, extract a depth map using one of the numerous methods for stereo matching, and make a steering decision based on the depth map thereby extracted. Unfortunately, the extreme\u00a0\u2026",
      "citation_histogram": [
        [2018, 1],
        [2019, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:kmGFBRAO0EoC",
      "title": "Pattern recognition and neural networks",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.6032",
      "year": 1995,
      "cited_by": 2,
      "authors": ["Yoshua Bengio", "Yann Lecun"],
      "description": "INTRODUCTION Pattern Recognition (PR) addresses the problem of classifying objects, often represented as vectors or as strings of symbols, into categories. The difficulty is to synthesize, and then to efficiently compute, the classification function that maps objects to categories, given that objects in a category can have widely varying input representations. In most instances, the task is known to the designer through a set of example patterns whose categories are known, and through general, a priori knowledge about the task, such as:\" the category of an object is not changed when the object is slightly translated or rotated in space\". Historically, the field of PR started with the early efforts in Neural Networks (Perceptrons, Adalines...). While in the past, NNs have somewhat played the role of an outsider in PR, the recent progress in learning algorithms (and the availability of powerful hardware) have made them the method of choice for many PR applications",
      "citation_histogram": [
        [2016, 1],
        [2017, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:f2IySw72cVMC",
      "title": "Usability study of text entry interfaces for wireless personal organizers and communicators",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.32.9742&rep=rep1&type=pdf",
      "year": 1995,
      "cited_by": 2,
      "authors": ["Isabelle Guyon", "Yann LeCun", "Colin Warwick"],
      "description": "We performed comparative tests of several keyboards and handwriting recognizers to determine their usability as text entry interfaces for wireless personal organizers and communicators. For very small devices (eg cellular phones), the keyboard is impractical: alternative user interfaces need to be invented, but handwriting recognition is marginally usable. For medium size devices (eg palmtop personal organizers), small\\hardware\" or\\software\" keyboards are both most e cient and preferred by users. Ease of correction was found to be at least as important as recognition accuracy. The handwriting recognition system designed in our department beats other recognizers; our main contender is therefore the keyboard. Users are biased in favor of handwriting, even if it is less e cient, particularly when the size of the keyboard is so small that typing is frustrating. These ndings have in uenced the design of a new pen based interface under development in our department.",
      "citation_histogram": [[2002, 1]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Zrzg8MEyHc4C",
      "title": "Localizing faces in images",
      "link": "https://scholar.google.com/scholar?cluster=9102926015196216633&hl=en&oi=scholarr",
      "year": 1993,
      "cited_by": 2,
      "authors": ["R Vaillant", "C Monrocq", "Y Le Cun"],
      "description": null,
      "citation_histogram": [
        [1996, 1],
        [1997, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:-1RNHcZo4Y8C",
      "title": "Generalization using back-propagation",
      "link": "https://nyuscholars.nyu.edu/en/publications/generalization-using-back-propagation",
      "year": 1987,
      "cited_by": 2,
      "authors": ["F Fogelman Soulie", "P Gallinari", "Yann Lecun", "S Thiria"],
      "description": "Generalization using back-propagation \u2014 NYU Scholars Skip to main navigation Skip to search \nSkip to main content NYU Scholars Home NYU Scholars Logo Help & FAQ Home Profiles \nResearch Units Research output Search by expertise, name or affiliation Generalization using \nback-propagation F. Fogelman Soulie, P. Gallinari, Yann Lecun, S. Thiria Computer Science \nResearch output: Chapter in Book/Report/Conference proceeding \u203a Conference contribution \nOverview Original language English (US) Title of host publication Proceedings of the First \nInternational Conference on Neural Networks, San Diego, California, June 1987 Publisher IEEE \nState Published - 1987 Cite this APA Standard Harvard Vancouver Author BIBTEX RIS \nFogelman Soulie, F., Gallinari, P., Lecun, Y., & Thiria, S. (1987). Generalization using \nback-propagation. In Proceedings of the First International Conference on Neural Networks, San \u2026",
      "citation_histogram": [[2019, 2]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:bZUHSELkOu4C",
      "title": "L\u00e9on Bo ou, Yoshua Bengio, and Patrick Ha ner. 1998. Gradientbased learning applied to document recognition",
      "link": "https://scholar.google.com/scholar?cluster=8340288257843371023&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 2,
      "authors": ["Yann Lecun"],
      "description": null,
      "citation_histogram": [[2017, 2]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:UamA9ItEL6YC",
      "title": "JSD, and et al. 1990. Handwritten digit recognition with a back-propagation network",
      "link": "https://scholar.google.com/scholar?cluster=17094795622333417311&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 2,
      "authors": ["Y LeCun", "B Boser"],
      "description": null,
      "citation_histogram": [
        [2009, 1],
        [2010, 1]
      ],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:wEOyVsangm4C",
      "title": "What Do We Maximize in Self-Supervised Learning?",
      "link": "https://arxiv.org/abs/2207.10081",
      "year": 2022,
      "cited_by": 1,
      "authors": ["Ravid Shwartz-Ziv", "Randall Balestriero", "Yann LeCun"],
      "description": "In this paper, we examine self-supervised learning methods, particularly VICReg, to provide an information-theoretical understanding of their construction. As a first step, we demonstrate how information-theoretic quantities can be obtained for a deterministic network, offering a possible alternative to prior work that relies on stochastic models. This enables us to demonstrate how VICReg can be (re)discovered from first principles and its assumptions about data distribution. Furthermore, we empirically demonstrate the validity of our assumptions, confirming our novel understanding of VICReg. Finally, we believe that the derivation and insights we obtain can be generalized to many other SSL methods, opening new avenues for theoretical and practical understanding of SSL and transfer learning.",
      "citation_histogram": [[2022, 1]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:UIW7YHcmbUEC",
      "title": "Separating the World and Ego Models for Self-Driving",
      "link": "https://arxiv.org/abs/2204.07184",
      "year": 2022,
      "cited_by": 1,
      "authors": [
        "Vlad Sobal",
        "Alfredo Canziani",
        "Nicolas Carion",
        "Kyunghyun Cho",
        "Yann LeCun"
      ],
      "description": "Training self-driving systems to be robust to the long-tail of driving scenarios is a critical problem. Model-based approaches leverage simulation to emulate a wide range of scenarios without putting users at risk in the real world. One promising path to faithful simulation is to train a forward model of the world to predict the future states of both the environment and the ego-vehicle given past states and a sequence of actions. In this paper, we argue that it is beneficial to model the state of the ego-vehicle, which often has simple, predictable and deterministic behavior, separately from the rest of the environment, which is much more complex and highly multimodal. We propose to model the ego-vehicle using a simple and differentiable kinematic model, while training a stochastic convolutional forward model on raster representations of the state to predict the behavior of the rest of the environment. We explore several configurations of such decoupled models, and evaluate their performance both with Model Predictive Control (MPC) and direct policy learning. We test our methods on the task of highway driving and demonstrate lower crash rates and better stability. The code is available at https://github.com/vladisai/pytorch-PPUU/tree/ICLR2022.",
      "citation_histogram": [[2022, 1]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:YW9K3tL-BTUC",
      "title": "Adversarially-Trained Normalized Noisy-Feature Auto-Encoder for Text Generation",
      "link": "https://arxiv.org/abs/1811.04201",
      "year": 2018,
      "cited_by": 1,
      "authors": ["Xiang Zhang", "Yann LeCun"],
      "description": "This article proposes Adversarially-Trained Normalized Noisy-Feature Auto-Encoder (ATNNFAE) for byte-level text generation. An ATNNFAE consists of an auto-encoder where the internal code is normalized on the unit sphere and corrupted by additive noise. Simultaneously, a replica of the decoder (sharing the same parameters as the AE decoder) is used as the generator and fed with random latent vectors. An adversarial discriminator is trained to distinguish training samples reconstructed from the AE from samples produced through the random-input generator, making the entire generator-discriminator path differentiable for discrete data like text. The combined effect of noise injection in the code and shared weights between the decoder and the generator can prevent the mode collapsing phenomenon commonly observed in GANs. Since perplexity cannot be applied to non-sequential text generation, we propose a new evaluation method using the total variance distance between frequencies of hash-coded byte-level n-grams (NGTVD). NGTVD is a single benchmark that can characterize both the quality and the diversity of the generated texts. Experiments are offered in 6 large-scale datasets in Arabic, Chinese and English, with comparisons against n-gram baselines and recurrent neural networks (RNNs). Ablation study on both the noise level and the discriminator is performed. We find that RNNs have trouble competing with the n-gram baselines, and the ATNNFAE results are generally competitive.",
      "citation_histogram": [[2019, 1]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:qdelZCX8GBYC",
      "title": "What is the Best Feature Learning Procedure in Hierarchical Recognition Architectures?",
      "link": "https://arxiv.org/abs/1606.01535",
      "year": 2016,
      "cited_by": 1,
      "authors": [
        "Kevin Jarrett",
        "Koray Kvukcuoglu",
        "Karol Gregor",
        "Yann LeCun"
      ],
      "description": "(This paper was written in November 2011 and never published. It is posted on arXiv.org in its original form in June 2016). Many recent object recognition systems have proposed using a two phase training procedure to learn sparse convolutional feature hierarchies: unsupervised pre-training followed by supervised fine-tuning. Recent results suggest that these methods provide little improvement over purely supervised systems when the appropriate nonlinearities are included. This paper presents an empirical exploration of the space of learning procedures for sparse convolutional networks to assess which method produces the best performance. In our study, we introduce an augmentation of the Predictive Sparse Decomposition method that includes a discriminative term (DPSD). We also introduce a new single phase supervised learning procedure that places an L1 penalty on the output state of each layer of the network. This forces the network to produce sparse codes without the expensive pre-training phase. Using DPSD with a new, complex predictor that incorporates lateral inhibition, combined with multi-scale feature pooling, and supervised refinement, the system achieves a 70.6\\% recognition rate on Caltech-101. With the addition of convolutional training, a 77\\% recognition was obtained on the CIfAR-10 dataset.",
      "citation_histogram": [[2018, 1]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:7gg4mBK4JdgC",
      "title": "Deep learning with elastic averaging SGD: 3rd International Conference on Learning Representations, ICLR 2015",
      "link": "https://scholar.google.com/scholar?cluster=6870003294143028922&hl=en&oi=scholarr",
      "year": 2015,
      "cited_by": 1,
      "authors": ["Sixin Zhang", "Anna Choromanska", "Yann LeCun"],
      "description": "We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, ie the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very\u00a0\u2026",
      "citation_histogram": [[2021, 1]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:3z7foVzkq2cC",
      "title": "Deep Learning & Convolutional Networks In Vision (part 2)",
      "link": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.393.256&rep=rep1&type=pdf",
      "year": 2013,
      "cited_by": 1,
      "authors": ["Yann LeCun"],
      "description": "Deep Learning & Convolutional Networks In Vision (part 2) Page 1 Y LeCun Deep Learning & \nConvolutional Networks In Vision (part 2) VRML, Paris 2013-07-23 Yann LeCun Center for \nData Science & Courant Institute, NYU yann@cs.nyu.edu http://yann.lecun.com Page 2 Y \nLeCun Energy-Based Unsupervised Learning Page 3 Y LeCun Energy-Based Unsupervised \nLearning Learning an energy function (or contrast function) that takes Low values on the data \nmanifold Higher values everywhere else Y1 Y2 Page 4 Y LeCun Capturing Dependencies \nBetween Variables with an Energy Function The energy surface is a \u201ccontrast function\u201d that \ntakes low values on the data manifold, and higher values everywhere else Special case: \nenergy = negative log density Example: the samples live in the manifold Y1 Y2 Y 2 =(Y 1 ) 2 \nPage 5 Y LeCun Transforming Energies into Probabilities (if necessary) Y P(Y|W) Y E(Y,W) The \u2026",
      "citation_histogram": [[2015, 1]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:uK1dVpBkok0C",
      "title": "Handwritten Digit Recognition: Applications of Neural Net Chips and Automatic",
      "link": "https://books.google.com/books?hl=en&lr=&id=hMaqCAAAQBAJ&oi=fnd&pg=PA303&dq=info:tE3UhSWaxJ0J:scholar.google.com&ots=__3cESfxeL&sig=X-F1DxdpkucCoNPSIk0Zb1e_6RQ",
      "year": 2012,
      "cited_by": 1,
      "authors": ["Y Le Cun", "LD Jackel", "B Boser", "JS Denker"],
      "description": "We describe two neural-net approaches to digit recognition. One method uses a neural-network chip to perform line thinning and local feature extraction. This preprocessing stage was designed by hand and did not involve any learning. However, automatic learning was used in the final classification step. The chip can process about 100 characters/sec, but the interface to the host computer limits the throughput to about 1 character/sec.",
      "citation_histogram": [[2019, 1]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:kRWSkSYxWN8C",
      "title": "High-Accuracy Object Recognition with a New Convolutional Net Architecture and Learning Algorithm (abstract)",
      "link": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.420.981&rep=rep1&type=pdf",
      "year": 2009,
      "cited_by": 1,
      "authors": [
        "Kevin Jarrett",
        "Koray Kavukcuoglu Marc\u2019Aurelio Ranzato",
        "Yann LeCun"
      ],
      "description": "Purely supervised Convolutional Networks yield excellent accuracy on image recognition tasks when data is plentiful [1]. But until now, they have not produced state-of-the-art accuracy on object recognition benchmarks for which few labeled samples per category are available. For example, on the popular Caltech-101 dataset with 30 samples for each of the 101 categories, methods that use hand-designed features, such as SIFT and Geometric Blur combined with a kernel classifier, achieve accuracies of 66.2%[5], and 64.6%[6]. By contrast, a purely supervised convolutional network with standard sigmoid non-linearities yields only 26%. This abstract describes a modified ConvNet architecture with a new unsupervised/supervised training procedure that can reach 67.2% accuracy on Caltech-101. This work explores several architectural designs and training methods and studies their effect on the accuracy for object recognition. The convolutional network under consideration takes a 143x143 grayscale image as input. The preprocessing includes removing mean and performing a local contrast normalization (dividing each pixel by the standard deviation of its neighbors). The first stage has 64 filters of size 9x9, followed by a subsampling layer with 5x5 stride, and 10x10 averaging window. The second stage has 256 feature map, each with 16 filters connected to a random subset of first-layer feature maps. The subsampling layer has a stride of 4x4 and a 6x6 averaging window. Hence, the input to the last layer has 256 feature maps of size 4x4 (4096 dimensions). Figure 1 shows the outline of a convolutional net, and figure 2 shows the best sequence\u00a0\u2026",
      "citation_histogram": [[2011, 1]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:eJXPG6dFmWUC",
      "title": "Loss functions for discriminative training of energy-based graphical models",
      "link": "https://scholar.google.com/scholar?cluster=12131730992226770876&hl=en&oi=scholarr",
      "year": 2005,
      "cited_by": 1,
      "authors": ["Yann LeCun", "J Huangfu"],
      "description": "Probabilistic graphical models associate a probability to each configuration of the relevant variables. Energy-based graphical models (EBGM) associate an energy to those configurations, eliminating the need for proper normalization of probability distributions. Making a decision (an inference) with an EBGM consists in comparing the energies associated with various configurations of the variable to be predicted, and choosing the one with the smallest energy. Such systems must be trained discriminatively to associate low energies to the desired configurations and higher energies to undesired configurations. A wide variety of loss function can be used for this purpose. We give sufficient conditions that a loss function should satisfy so that its minimization will cause the system to approach to desired behavior. We give many specific examples of suitable loss functions, and show an application to object recognition in\u00a0\u2026",
      "citation_histogram": [[2006, 1]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:FgeqF4j-V1wC",
      "title": "DjVu: Un Systeme de Compression d'Images pour la Distribution Reticulaire de Documents Numerises. Resume",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.83.9039",
      "year": 2000,
      "cited_by": 1,
      "authors": [
        "Leon Bottou",
        "Patrick Ha Ner",
        "Yann Lecun",
        "Paul Howard",
        "Pascal Vincent",
        "Bill Riemers"
      ],
      "description": "Nous presentons une technique nouvelle de compression d'images appelee\\DjVu\". Cette technique est specia-lement concue pour la compression de documents en couleurs numerises a haute resolution. Un chier DjVu representant une page typique d'un magazine en couleurs, numerisee a 300 points par pouce (dpi), requiert entre 40 et 80 KB, ce qui est est 5 a 10 fois meilleur qu'un chier JPEG o rant une lisibilite similaire. Le compresseur DjVu commence par classer chaque pixel de l'image numerisee comme pixel d'avant-plan (texte, dessins au trait) ou pixel d'arriere-plan (images, photos, texture dupapier) gr^ ace a une combinaison de Modeles de Markov Caches (HMM) et d'heuristiques fondes sur le principe de Minimum Description Length (MDL). Cette classi cation forme une image bitonale qui est compressee gr^ ace a une technique qui tire parti des similitudes de forme entre les divers caracteres composant l'avant-plan. Les images d'avant-plan et d'arriere-plan sont ensuite compressees a l'aide d'un algorithme de compression par ondelettes avec une resolution reduite. Un algorithme de masquage minimise le nombre de bits utilises pour coder les pixels d'avant-plan ou d'arriere-plan qui ne sont pas visibles dans l'image nale. Des logiciels d'encodage et de decodage sont disponibles pour toutes les plateformes usuelles. Une extension de butineur (\\browser plugin\") permet de visualiser tres e cacement les images DjVu sur le Web. 1",
      "citation_histogram": [[2001, 1]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:qu5FZ_za5owC",
      "title": "Neural-Net Applications in Character Recognition and",
      "link": "http://yann.lecun.org/exdb/publis/psgz/jackel-95.ps.gz",
      "year": 1995,
      "cited_by": 1,
      "authors": [
        "LD Jackel",
        "MY Battista",
        "J Ben J Bromley",
        "CJC Burges",
        "HS Baird",
        "E Cosatto",
        "JS Denker",
        "HP Graf",
        "HP Katseff",
        "Y LeCun",
        "CR Nohl",
        "E Sackinger",
        "JH Shamilian",
        "T Shoemaker",
        "CE Stenard",
        "BI Strom",
        "R Ting",
        "T Wood",
        "CR Zuraw"
      ],
      "description": "A proven strength of neural-network methods is their application to character recognition and document analysis. In this paper we describe a neural-net Optical Character Recognizer (OCR), neural-net preprocessing, and neuralnet hardware accelerators that together comprise a high-performance character recognition system. We also describe applications in networkbased fax and bit-mapped text processing.",
      "citation_histogram": [[2017, 1]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:EkHepimYqZsC",
      "title": "Applying neural network technologies",
      "link": "https://scholar.google.com/scholar?cluster=651689335264335036&hl=en&oi=scholarr",
      "year": 1990,
      "cited_by": 1,
      "authors": ["Yann Le Cun", "David S Touretzky"],
      "description": null,
      "citation_histogram": [[1991, 1]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:uc_IGeMz5qoC",
      "title": "Hardware Considerations for Neural-net Character Recognition Systems",
      "link": "https://scholar.google.com/scholar?cluster=7562167987584691165&hl=en&oi=scholarr",
      "year": 1990,
      "cited_by": 1,
      "authors": [
        "Lawrence D Jackel",
        "HP Graf",
        "\u0412 Boser",
        "John S Denker",
        "Y Le Cun",
        "Richard E Howard",
        "\u041e Matan",
        "Henry S Baird"
      ],
      "description": "Through a series of experiments in optical character recognition, an understanding is beginning to emerge of the general nature of the neural-net hardware appropriate for this task. We find that machine perception tasks require a connectivity pattern that is both sparser and more complex than the fully-connected network envisioned by most early researchers. In particular, it is important to be able to perform convolutions and other operations that involve patterns of repeated weights. One could imagine carrying out all these operations in parallel in dedicated hardware, but this turns out to be both prohibitively costly (with current technology) and unnecessary. Fortunately, the repetitive nature of the convolution operation makes time-division multiplexing of the hardware possible and even efficient. To avoid I/O bottlenecks, the hardware must contain substantial input data registers and shift registers. I/O requirements are further relaxed if several layers of the net are processed in a pipeline without recourse to off-chip storage. This paper will discuss hardware architectures for character recognition and will outline choices for possible circuits. An advanced (and working) reconfigurable neural-net chip, that mixes analog and digital processing, will be described.",
      "citation_histogram": [[1990, 1]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:KUbvn5osdkgC",
      "title": "Advances in artificial intelligence",
      "link": null,
      "year": 1986,
      "cited_by": 1,
      "authors": ["F Fogelman-Soulie", "Y Le Cun"],
      "description": null,
      "citation_histogram": [[2021, 1]],
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Kp2F6cy7E28C",
      "title": "VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment",
      "link": "https://arxiv.org/abs/2210.04135",
      "year": 2022,
      "cited_by": null,
      "authors": [
        "Shraman Pramanick",
        "Li Jing",
        "Sayan Nag",
        "Jiachen Zhu",
        "Hardik Shah",
        "Yann LeCun",
        "Rama Chellappa"
      ],
      "description": "Vision-language pre-training (VLP) has recently proven highly effective for various uni- and multi-modal downstream applications. However, most existing end-to-end VLP methods use high-resolution image-text box data to perform well on fine-grained region-level tasks, such as object detection, segmentation, and referring expression comprehension. Unfortunately, such high-resolution images with accurate bounding box annotations are expensive to collect and use for supervision at scale. In this work, we propose VoLTA (Vision-Language Transformer with weakly-supervised local-feature Alignment), a new VLP paradigm that only utilizes image-caption data but achieves fine-grained region-level image understanding, eliminating the use of expensive box annotations. VoLTA adopts graph optimal transport-based weakly-supervised alignment on local image patches and text tokens to germinate an explicit, self-normalized, and interpretable low-level matching criterion. In addition, VoLTA pushes multi-modal fusion deep into the uni-modal backbones during pre-training and removes fusion-specific transformer layers, further reducing memory requirements. Extensive experiments on a wide range of vision- and vision-language downstream tasks demonstrate the effectiveness of VoLTA on fine-grained applications without compromising the coarse-grained downstream performance, often outperforming methods using significantly more caption and box annotations.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:hNFvT5Kh_mEC",
      "title": "RankMe: Assessing the downstream performance of pretrained self-supervised representations by their rank",
      "link": "https://arxiv.org/abs/2210.02885",
      "year": 2022,
      "cited_by": null,
      "authors": [
        "Quentin Garrido",
        "Randall Balestriero",
        "Laurent Najman",
        "Yann Lecun"
      ],
      "description": "Joint-Embedding Self Supervised Learning (JE-SSL) has seen a rapid development, with the emergence of many method variations and few principled guidelines that would help practitioners to successfully deploy those methods. The main reason for that pitfall actually comes from JE-SSL's core principle of not employing any input reconstruction. Without any visual clue, it becomes extremely cryptic to judge the quality of a learned representation without having access to a labelled dataset. We hope to correct those limitations by providing a single -- theoretically motivated -- criterion that reflects the quality of learned JE-SSL representations: their effective rank. Albeit simple and computationally friendly, this method -- coined RankMe -- allows one to assess the performance of JE-SSL representations, even on different downstream datasets, without requiring any labels, training or parameters to tune. Through thorough empirical experiments involving hundreds of repeated training episodes, we demonstrate how RankMe can be used for hyperparameter selection with nearly no loss in final performance compared to the current selection method that involve dataset labels. We hope that RankMe will facilitate the use of JE-SSL in domains with little or no labeled data.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:vkAjbg6qVbQC",
      "title": "VICRegL: Self-Supervised Learning of Local Visual Features",
      "link": "https://arxiv.org/abs/2210.01571",
      "year": 2022,
      "cited_by": null,
      "authors": ["Adrien Bardes", "Jean Ponce", "Yann LeCun"],
      "description": "Most recent self-supervised methods for learning image representations focus on either producing a global feature with invariance properties, or producing a set of local features. The former works best for classification tasks while the latter is best for detection and segmentation tasks. This paper explores the fundamental trade-off between learning local and global features. A new method called VICRegL is proposed that learns good global and local features simultaneously, yielding excellent performance on detection and segmentation tasks while maintaining good performance on classification tasks. Concretely, two identical branches of a standard convolutional net architecture are fed two differently distorted versions of the same image. The VICReg criterion is applied to pairs of global feature vectors. Simultaneously, the VICReg criterion is applied to pairs of local feature vectors occurring before the last pooling layer. Two local feature vectors are attracted to each other if their l2-distance is below a threshold or if their relative locations are consistent with a known geometric transformation between the two input images. We demonstrate strong performance on linear classification and segmentation transfer tasks. Code and pretrained models are publicly available at: https://github.com/facebookresearch/VICRegL",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:P3HMU8IszGwC",
      "title": "Minimalistic Unsupervised Learning with the Sparse Manifold Transform",
      "link": "https://arxiv.org/abs/2209.15261",
      "year": 2022,
      "cited_by": null,
      "authors": [
        "Yubei Chen",
        "Zeyu Yun",
        "Yi Ma",
        "Bruno Olshausen",
        "Yann LeCun"
      ],
      "description": "We describe a minimalistic and interpretable method for unsupervised learning, without resorting to data augmentation, hyperparameter tuning, or other engineering designs, that achieves performance close to the SOTA SSL methods. Our approach leverages the sparse manifold transform, which unifies sparse coding, manifold learning, and slow feature analysis. With a one-layer deterministic sparse manifold transform, one can achieve 99.3% KNN top-1 accuracy on MNIST, 81.1% KNN top-1 accuracy on CIFAR-10 and 53.2% on CIFAR-100. With a simple gray-scale augmentation, the model gets 83.2% KNN top-1 accuracy on CIFAR-10 and 57% on CIFAR-100. These results significantly close the gap between simplistic ``white-box'' methods and the SOTA methods. Additionally, we provide visualization to explain how an unsupervised representation transform is formed. The proposed method is closely connected to latent-embedding self-supervised methods and can be treated as the simplest form of VICReg. Though there remains a small performance gap between our simple constructive model and SOTA methods, the evidence points to this as a promising direction for achieving a principled and white-box approach to unsupervised learning.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:gJGMWE3-404C",
      "title": "Variance Covariance Regularization Enforces Pairwise Independence in Self-Supervised Representations",
      "link": "https://arxiv.org/abs/2209.14905",
      "year": 2022,
      "cited_by": null,
      "authors": ["Gr\u00e9goire Mialon", "Randall Balestriero", "Yann LeCun"],
      "description": "Self-Supervised Learning (SSL) methods such as VICReg, Barlow Twins or W-MSE avoid collapse of their joint embedding architectures by constraining or regularizing the covariance matrix of their projector's output. This study highlights important properties of such strategy, which we coin Variance-Covariance regularization (VCReg). More precisely, we show that VCReg enforces pairwise independence between the features of the learned representation. This result emerges by bridging VCReg applied on the projector's output to kernel independence criteria applied on the projector's input. This provides the first theoretical motivations and explanations of VCReg. We empirically validate our findings where (i) we observe that SSL methods employing VCReg learn visual representations with greater pairwise independence than other methods, (i) we put in evidence which projector's characteristics favor pairwise independence, and show it to emerge independently from learning the projector, (ii) we use these findings to obtain nontrivial performance gains for VICReg, (iii) we demonstrate that the scope of VCReg goes beyond SSL by using it to solve Independent Component Analysis. We hope that our findings will support the adoption of VCReg in SSL and beyond.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:FlNF6OXwGF0C",
      "title": "Joint Embedding Self-Supervised Learning in the Kernel Regime",
      "link": "https://arxiv.org/abs/2209.14884",
      "year": 2022,
      "cited_by": null,
      "authors": [
        "Bobak T Kiani",
        "Randall Balestriero",
        "Yubei Chen",
        "Seth Lloyd",
        "Yann LeCun"
      ],
      "description": "The fundamental goal of self-supervised learning (SSL) is to produce useful representations of data without access to any labels for classifying the data. Modern methods in SSL, which form representations based on known or constructed relationships between samples, have been particularly effective at this task. Here, we aim to extend this framework to incorporate algorithms based on kernel methods where embeddings are constructed by linear maps acting on the feature space of a kernel. In this kernel regime, we derive methods to find the optimal form of the output representations for contrastive and non-contrastive loss functions. This procedure produces a new representation space with an inner product denoted as the induced kernel which generally correlates points which are related by an augmentation in kernel space and de-correlates points otherwise. We analyze our kernel model on small datasets to identify common features of self-supervised learning algorithms and gain theoretical insights into their performance on downstream tasks.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:cqul0IHDQ48C",
      "title": "Light-weight probing of unsupervised representations for Reinforcement Learning",
      "link": "https://arxiv.org/abs/2208.12345",
      "year": 2022,
      "cited_by": null,
      "authors": [
        "Wancong Zhang",
        "Anthony GX-Chen",
        "Vlad Sobal",
        "Yann LeCun",
        "Nicolas Carion"
      ],
      "description": "Unsupervised visual representation learning offers the opportunity to leverage large corpora of unlabeled trajectories to form useful visual representations, which can benefit the training of reinforcement learning (RL) algorithms. However, evaluating the fitness of such representations requires training RL algorithms which is computationally intensive and has high variance outcomes. To alleviate this issue, we design an evaluation protocol for unsupervised RL representations with lower variance and up to 600x lower computational cost. Inspired by the vision community, we propose two linear probing tasks: predicting the reward observed in a given state, and predicting the action of an expert in a given state. These two tasks are generally applicable to many RL domains, and we show through rigorous experimentation that they correlate strongly with the actual downstream control performance on the Atari100k Benchmark. This provides a better method for exploring the space of pretraining algorithms without the need of running RL evaluations for every setting. Leveraging this framework, we further improve existing self-supervised learning (SSL) recipes for RL, highlighting the importance of the forward model, the size of the visual backbone, and the precise formulation of the unsupervised objective.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:uWFADo6cQ7YC",
      "title": "Methods, systems, and media for detecting spoofing in mobile authentication",
      "link": "https://patents.google.com/patent/US11425562B2/en",
      "year": 2022,
      "cited_by": null,
      "authors": [
        "Yann LeCun",
        "Fengjun Lv",
        "Dushyant Goyal",
        "Yang Wang",
        "Adam Perold"
      ],
      "description": "Provided herein are devices, systems, and methods for detecting spoofing of a 3D object, using a 2D representation, in a mobile object authentication process, comprising capturing image data of the 3D object by a front-facing camera, to record a current spatial characteristic of the 3D object, while a front-facing screen displays an authentication pattern comprising a plurality of regions, wherein at least one of the regions varies in at least one of: brightness, position, size, shape, and color over time causing a variance of lighting effects which create highlights and shadows on the 3D object over time. The devices, systems, and methods thereby provide an efficient and secure process for determining if spoofing of the 3D object, using a 2D representation, is attempted in a mobile authentication process, by comparing the current spatial characteristic of the 3D object with a stored reference spatial characteristic of the 3D\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:LpWf3qrnWeoC",
      "title": "A Path Towards Autonomous Machine Intelligence Version 0.9. 2, 2022-06-27",
      "link": "https://openreview.net/pdf?id=BZ5a1r-kVsf&utm_source=pocket_mylist",
      "year": 2022,
      "cited_by": null,
      "authors": ["Yann LeCun"],
      "description": "How could machines learn as efficiently as humans and animals? How could machines learn to reason and plan? How could machines learn representations of percepts and action plans at multiple levels of abstraction, enabling them to reason, predict, and plan at multiple time horizons? This position paper proposes an architecture and training paradigms with which to construct autonomous intelligent agents. It combines concepts such as configurable predictive world model, behavior driven through intrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:RTQXeVVbsVUC",
      "title": "TiCo: Transformation Invariance and Covariance Contrast for Self-Supervised Visual Representation Learning",
      "link": "https://arxiv.org/abs/2206.10698",
      "year": 2022,
      "cited_by": null,
      "authors": [
        "Jiachen Zhu",
        "Rafael M Moraes",
        "Serkan Karakulak",
        "Vlad Sobol",
        "Alfredo Canziani",
        "Yann LeCun"
      ],
      "description": "We present Transformation Invariance and Covariance Contrast (TiCo) for self-supervised visual representation learning. Similar to other recent self-supervised learning methods, our method is based on maximizing the agreement among embeddings of different distorted versions of the same image, which pushes the encoder to produce transformation invariant representations. To avoid the trivial solution where the encoder generates constant vectors, we regularize the covariance matrix of the embeddings from different images by penalizing low rank solutions. By jointly minimizing the transformation invariance loss and covariance contrast loss, we get an encoder that is able to produce useful representations for downstream tasks. We analyze our method and show that it can be viewed as a variant of MoCo with an implicit memory bank of unlimited size at no extra memory cost. This makes our method perform better than alternative methods when using small batch sizes. TiCo can also be seen as a modification of Barlow Twins. By connecting the contrastive and redundancy-reduction methods together, TiCo gives us new insights into how joint embedding methods work.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:ntg98fmFLVcC",
      "title": "Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors",
      "link": "https://arxiv.org/abs/2205.10279",
      "year": 2022,
      "cited_by": null,
      "authors": [
        "Ravid Shwartz-Ziv",
        "Micah Goldblum",
        "Hossein Souri",
        "Sanyam Kapoor",
        "Chen Zhu",
        "Yann LeCun",
        "Andrew Gordon Wilson"
      ],
      "description": "Deep learning is increasingly moving towards a transfer learning paradigm whereby large foundation models are fine-tuned on downstream tasks, starting from an initialization learned on the source task. But an initialization contains relatively little information about the source task. Instead, we show that we can learn highly informative posteriors from the source task, through supervised or self-supervised approaches, which then serve as the basis for priors that modify the whole loss surface on the downstream task. This simple modular approach enables significant performance gains and more data-efficient learning on a variety of downstream classification and segmentation tasks, serving as a drop-in replacement for standard pre-training strategies. These highly informative priors also can be saved for future use, similar to pre-trained weights, and stand in contrast to the zero-mean isotropic uninformative priors that are typically used in Bayesian deep learning.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Lbh3VFZM3akC",
      "title": "projUNN: efficient method for training deep networks with unitary matrices",
      "link": "https://arxiv.org/abs/2203.05483",
      "year": 2022,
      "cited_by": null,
      "authors": [
        "Bobak Kiani",
        "Randall Balestriero",
        "Yann Lecun",
        "Seth Lloyd"
      ],
      "description": "In learning with recurrent or very deep feed-forward networks, employing unitary matrices in each layer can be very effective at maintaining long-range stability. However, restricting network parameters to be unitary typically comes at the cost of expensive parameterizations or increased training runtime. We propose instead an efficient method based on rank- updates -- or their rank- approximation -- that maintains performance at a nearly optimal training runtime. We introduce two variants of this method, named Direct (projUNN-D) and Tangent (projUNN-T) projected Unitary Neural Networks, that can parameterize full -dimensional unitary or orthogonal matrices with a training runtime scaling as . Our method either projects low-rank gradients onto the closest unitary matrix (projUNN-T) or transports unitary matrices in the direction of the low-rank gradient (projUNN-D). Even in the fastest setting (), projUNN is able to train a model's unitary parameters to reach comparable performances against baseline implementations. By integrating our projUNN algorithm into both recurrent and convolutional neural networks, our models can closely match or exceed benchmarked results from state-of-the-art algorithms.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Xi1bsBfZRoQC",
      "title": "Recurrent Parameter Generators",
      "link": "https://arxiv.org/abs/2107.07110",
      "year": 2021,
      "cited_by": null,
      "authors": [
        "Jiayun Wang",
        "Yubei Chen",
        "Stella X Yu",
        "Brian Cheung",
        "Yann LeCun"
      ],
      "description": "We present a generic method for recurrently using the same parameters for many different convolution layers to build a deep network. Specifically, for a network, we create a recurrent parameter generator (RPG), from which the parameters of each convolution layer are generated. Though using recurrent models to build a deep convolutional neural network (CNN) is not entirely new, our method achieves significant performance gain compared to the existing works. We demonstrate how to build a one-layer neural network to achieve similar performance compared to other traditional CNN models on various applications and datasets. Such a method allows us to build an arbitrarily complex neural network with any amount of parameters. For example, we build a ResNet34 with model parameters reduced by more than  times, which still achieves  ImageNet top-1 accuracy. Furthermore, we demonstrate the RPG can be applied at different scales, such as layers, blocks, or even sub-networks. Specifically, we use the RPG to build a ResNet18 network with the number of weights equivalent to one convolutional layer of a conventional ResNet and show this model can achieve  ImageNet top-1 accuracy. The proposed method can be viewed as an inverse approach to model compression. Rather than removing the unused parameters from a large model, it aims to squeeze more information into a small number of parameters. Extensive experiment results are provided to demonstrate the power of the proposed recurrent parameter generator.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:CY3uIpTmi-gC",
      "title": "System and method for biometric authentication in connection with camera-equipped devices",
      "link": "https://patents.google.com/patent/US10728242B2/en",
      "year": 2020,
      "cited_by": null,
      "authors": ["Yann LeCun", "Adam Perold", "Yang Wang", "Sagar Waghmare"],
      "description": "The present invention relates generally to the use of biometric technology for authentication and identification, and more particularly to non-contact based solutions for authenticating and identifying users, via computers, such as mobile devices, to selectively permit or deny access to various resources. In the present invention authentication and/or identification is performed using an image or a set of images of an individual's palm through a process involving the following key steps:(1) detecting the palm area using local classifiers;(2) extracting features from the region (s) of interest; and (3) computing the matching score against user models stored in a database, which can be augmented dynamically through a learning process.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:_xmTp9Pj6u8C",
      "title": "System and method for biometric authentication in connection with camera-equipped devices",
      "link": "https://patents.google.com/patent/EP3657389A1/da%20US4325121.pdf",
      "year": 2020,
      "cited_by": null,
      "authors": ["Yann Lecun", "Adam Perold", "Yang Wang", "Sagar Waghmare"],
      "description": "Priority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.) 2012-09-05",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:T-Bu49-jKQgC",
      "title": "Congratulations to the 2020 AAAI Fellows!",
      "link": "https://ojs.aaai.org/index.php/aimagazine/article/view/5305/5179",
      "year": 2020,
      "cited_by": null,
      "authors": [
        "Yoshua Bengio",
        "Cynthia Breazeal",
        "Yann LeCun",
        "Radhika Nagpal",
        "Natasha Noy",
        "Martha Palmer",
        "Dragomir Radev",
        "Sylvie Thi\u00e9baux"
      ],
      "description": "Each year a small number of fellows are recognized for their unusual distinction in the profession and for their sustained contributions to the field for a decade or more. An official dinner and ceremony were held in their honor during AAAi-20 in new York, new York.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:yFpZgd9WRDkC",
      "title": "5TH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS, ICLR 2017-CONFERENCE TRACK PROCEEDINGS",
      "link": "https://scholar.google.com/scholar?cluster=16818602808763444944&hl=en&oi=scholarr",
      "year": 2019,
      "cited_by": null,
      "authors": [
        "Y Yang",
        "TM Hospedales",
        "J Park",
        "S Li",
        "PTP Tang",
        "P Dubey",
        "W Wen",
        "H Li",
        "Y Chen",
        "H Samet",
        "A Kadav",
        "I Durdanovic",
        "HP Graf",
        "W Zellinger",
        "E Lughofer",
        "S Saminger-Platz",
        "T Grubinger",
        "T Natschl\u00e4ger",
        "G Berger",
        "R Memisevic",
        "L Bazzani",
        "L Torresani",
        "H Larochelle",
        "P Molchanov",
        "S Tyree",
        "T Karras",
        "T Aila",
        "J Kautz",
        "S Purushotham",
        "W Carvalho",
        "T Nilanon",
        "Y Liu",
        "P Chaudhari",
        "S Soatto",
        "A Choromanska",
        "Y LeCun",
        "L Sagun",
        "C Baldassi",
        "R Zecchina",
        "C Borgs",
        "J Chayes",
        "G Philipp",
        "JG Carbonell",
        "Y Taigman",
        "A Polyak",
        "L Wolf",
        "Y Choi",
        "M El-Khamy",
        "J Lee",
        "A Gupta",
        "C Devin",
        "P Abbeel",
        "S Levine",
        "J Ba",
        "R Grosse",
        "J Martens",
        "L Hou",
        "Q Yao",
        "JT Kwok",
        "AA Alemi",
        "I Fischer",
        "JV Dillon",
        "K Murphy"
      ],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:8dmKnlANe1sC",
      "title": "Informatique et sciences num\u00e9riques",
      "link": "https://journals.openedition.org/annuaire-cdf/13178",
      "year": 2018,
      "cited_by": null,
      "authors": ["Yann LeCun"],
      "description": "Enseignement\u00a0\u2013\u00a0L\u2019apprentissage profond\u00a0: une r\u00e9volution en intelligence artificielle Cours\u00a0\u2013\u00a0L\u2019apprentissage profond 12\u00a0f\u00e9vrier 2016\u00a0: Pourquoi l\u2019apprentissage profond\u00a0? 19\u00a0f\u00e9vrier 2016\u00a0: R\u00e9seaux multi-couches et r\u00e9tropropagation du gradient. 26\u00a0f\u00e9vrier 2016\u00a0: L\u2019apprentissage profond en pratique. 4\u00a0mars 2016\u00a0: R\u00e9seaux convolutifs. 25\u00a0mars 2016\u00a0: R\u00e9seaux convolutifs. Applications \u00e0 la vision. 1er\u00a0avril 2016\u00a0: R\u00e9seaux r\u00e9currents. Applications au traitement du langage naturel. 8\u00a0avril 2016\u00a0: Rai...",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:6g4f2aSzPAQC",
      "title": "Advancing AI: From Academia to Facebook and Back",
      "link": "https://aaas.confex.com/aaas/2018/meetingapp.cgi/Paper/22847",
      "year": 2018,
      "cited_by": null,
      "authors": ["Yann LeCun"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:IEHkmGLyHGEC",
      "title": "Universality in halting time",
      "link": "https://openreview.net/forum?id=HyET6tYex",
      "year": 2016,
      "cited_by": null,
      "authors": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"],
      "description": "The authors present empirical distributions for the halting time (measured by the number of iterations to reach a given accuracy) of optimization algorithms applied to two random systems: spin glasses and deep learning. Given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time follow a distribution that remains unchanged even when the input is changed drastically. We observe two main classes, a Gumbel-like distribution that appears in Google searches, human decision times, QR factorization and spin glasses, and a Gaussian-like distribution that appears in conjugate gradient method, deep network with MNIST input data and deep network with random input data. This empirical evidence suggests presence of a class of distributions for which the halting time is independent of the underlying distribution under some conditions.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:plawwrVfPpoC",
      "title": "ENERGY-BASED GENERATIVE ADVERSARIAL NET",
      "link": "https://scholar.archive.org/work/az3gxhblbvdk7crw6oqtg6enk4/access/wayback/https://openreview.net/pdf?id=ryh9pmcee",
      "year": 2016,
      "cited_by": null,
      "authors": ["Junbo Zhao", "Michael Mathieu", "Yann LeCun"],
      "description": "We introduce the \u201cEnergy-based Generative Adversarial Network\u201d model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:C7HQDqNFSbYC",
      "title": "System and method for biometric authentication in connection with camera-equipped devices",
      "link": "https://patents.google.com/patent/HK1212494A1/cs%20US4325121.pdf",
      "year": 2016,
      "cited_by": null,
      "authors": ["Yann Lecun", "Adam Perold", "Yang Wang", "Sagar Waghmare"],
      "description": "Priority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.) 2012-09-05",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Id9pRtCSqO0C",
      "title": "Phase 1: DCL System Research Using Advanced Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals-HPC System Implementation",
      "link": "https://arxiv.org/abs/1605.00971",
      "year": 2016,
      "cited_by": null,
      "authors": [
        "Peter J Dugan",
        "Christopher W Clark",
        "Yann Andr\u00e9 LeCun",
        "Sofie M Van Parijs"
      ],
      "description": "We aim to investigate advancing the state of the art of detection, classification and localization (DCL) in the field of bioacoustics. The two primary goals are to develop transferable technologies for detection and classification in: (1) the area of advanced algorithms, such as deep learning and other methods; and (2) advanced systems, capable of real-time and archival and processing. This project will focus on long-term, continuous datasets to provide automatic recognition, minimizing human time to annotate the signals. Effort will begin by focusing on several years of multi-channel acoustic data collected in the Stellwagen Bank National Marine Sanctuary (SBNMS) between 2006 and 2010. Our efforts will incorporate existing technologies in the bioacoustics signal processing community, advanced high performance computing (HPC) systems, and new approaches aimed at automatically detecting-classifying and measuring features for species-specific marine mammal sounds within passive acoustic data.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:zBYNw3HCx4kC",
      "title": "AAAI Workshop-Technical Report: Preface",
      "link": "https://nyuscholars.nyu.edu/en/publications/aaai-workshop-technical-report-preface",
      "year": 2013,
      "cited_by": null,
      "authors": [
        "Marc Pickett",
        "Benjamin Kuipers",
        "Yann LeCun",
        "Clayton Morrison"
      ],
      "description": "AAAI Workshop - Technical Report: Preface \u2014 NYU Scholars Skip to main navigation Skip to \nsearch Skip to main content NYU Scholars Home NYU Scholars Logo Help & FAQ Home \nProfiles Research Units Research output Search by expertise, name or affiliation AAAI \nWorkshop - Technical Report: Preface Marc Pickett, Benjamin Kuipers, Yann LeCun, Clayton \nMorrison Computer Science Research output: Contribution to journal \u203a Editorial \u203a peer-review \nOverview Fingerprint Original language English (US) Pages (from-to) vii Journal AAAI Workshop \n- Technical Report Volume WS-13-12 State Published - 2013 Event 2013 AAAI Workshop - \nBellevue, WA, United States Duration: Jul 15 2013 \u2192 Jul 15 2013 ASJC Scopus subject areas \nEngineering(all) Other files and links Link to publication in Scopus Link to citation list in \nScopus Cite this APA Standard Harvard Vancouver Author BIBTEX RIS Pickett, M., Kuipers, B., \u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Xz60mAmATU4C",
      "title": "DCL System Research Using Advanced Approaches for Land-based or Ship-based Real-Time Recognition and Localization of Marine Mammals",
      "link": "https://apps.dtic.mil/sti/citations/ADA572279",
      "year": 2012,
      "cited_by": null,
      "authors": [
        "Peter J Dugan",
        "Christopher W Clark",
        "Yann A LeCun",
        "Sofie M Van Parijs"
      ],
      "description": "We aim to investigate advancing the state of the art of detection, classification and localization DCL in the field of bioacoustics. The two primary goals are to develop transferable technologies for detection and classification in 1 the area of advanced algorithms, such as deep learning and other methods and 2 advanced systems, capable of real-time and archival and processing. This project will focus on long-term, continuous datasets to provide automatic recognition, minimizing human time to annotate the signals. Effort will begin by focusing on several years of multi-channel acoustic data collected in the Stellwagen Bank National Marine Sanctuary SBNMS between 2006 and 2010. Our efforts will incorporate existing technologies in the bioacoustics signal processing community, advanced high performance computing HPC systems, and new approaches aimed at automatically detecting-classifying and measuring features for species-specific marine mammal sounds within passive acoustic data.Descriptors:",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:rHJHxKgnXwkC",
      "title": "A New Computer Science Publishing Model",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.460.4479&rep=rep1&type=pdf",
      "year": 2011,
      "cited_by": null,
      "authors": ["Shirley Zhao", "Yann LeCun"],
      "description": "The current publishing model in computer science emphasizes conference publications, which has made it extremely competitive and difficult to bring new ideas to the field. The review process needs changing to allow papers and ideas to surface without restriction. This paper outlines the functional specifications of a system that was proposed in Yann LeCun\u2019s pamphlet. The overarching goal of this new system is to maximize the Rate of Progress of Science (RPoS) over the long term. The system will rely heavily on the ideas of collaborative filtering and \u201cwisdom of the crowds.\u201d",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:DQQjGlBKAuwC",
      "title": "technical Perspective concerto for Violin and Markov Model",
      "link": "https://wp.nyu.edu/robert_rowe/wp-content/uploads/sites/1076/2015/12/Concerto-for-Violin-and-Markov-Model.pdf",
      "year": 2011,
      "cited_by": null,
      "authors": ["Juan Bello", "Yann LeCun", "Robert Rowe"],
      "description": "Concerto for violin and Markov model: technical perspective Page 1 86 coMMunications of the \nacM | MArch 2011 | vol. 54 | no. 3 in the OPening moments of Jean Sibelius\u2019 Violin Concerto, \nthe young soloist plays delicately, almost languidly. The orchestra responds in kind, muting the \nrepeated string motif to a whisper. As the piece progresses, soloist and orchestra alternatively \nperform the main motifs in increasing measures of power and virtuosity, which inexorably lead \ntoward the movement\u2019s stirring resolution. The soloist looks relieved as she crosses the stage \nto shake the conductor\u2019s hand. This violinist, like most others in music education, can benefit \nenormously from interacting with large ensembles in honing her performing skills. However, \nthe demand far exceeds the number and capabilities of existing orchestras, ensuring \nmost of these students won\u2019t have access to this experience. Our soloist is no exception. \u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:vq25oHwZT-8C",
      "title": "Building Artificial Vision Systems with Machine Learning",
      "link": "https://scholar.google.com/scholar?cluster=16157621738802016552&hl=en&oi=scholarr",
      "year": 2011,
      "cited_by": null,
      "authors": ["Yann LeCun"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:mSXQG6lSlFkC",
      "title": "Concerto for violin and Markov model: technical perspective. Communications of the ACM, 54 (3), 86-86",
      "link": "https://nyuscholars.nyu.edu/en/publications/concerto-for-violin-and-markov-model-technical-perspective-commun",
      "year": 2011,
      "cited_by": null,
      "authors": ["Robert Rowe", "Juan Bello", "Yann LeCun"],
      "description": "Concerto for violin and Markov model: technical perspective. Communications of the ACM, \n54(3), 86-86 \u2014 NYU Scholars Skip to main navigation Skip to search Skip to main content NYU \nScholars Home NYU Scholars Logo Help & FAQ Home Profiles Research Units Research \noutput Search by expertise, name or affiliation Concerto for violin and Markov model: technical \nperspective. Communications of the ACM, 54(3), 86-86 Robert Rowe, Juan Bello, Yann LeCun \nMusic Music and Performing Arts Professions Urban Initiative Computer Science Research \noutput: Chapter in Book/Report/Conference proceeding \u203a Conference contribution Overview \nOriginal language English (US) Title of host publication Concerto for violin and Markov model: \ntechnical perspective. Communications of the ACM, 54(3), 86-86 State Published - 2011 \nCite this APA Standard Harvard Vancouver Author BIBTEX RIS Rowe, R., Bello, J., & LeCun\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:eMMeJKvmdy0C",
      "title": "Convolutional K-SVD (abstract)",
      "link": null,
      "year": 2010,
      "cited_by": null,
      "authors": ["Arthur Szlam", "Koray Kavukcuoglu", "Yann LeCun"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:5Ul4iDaHHb8C",
      "title": "Analysis of Feature Learning and Feature Pooling for Image Recognition (abstract)",
      "link": null,
      "year": 2010,
      "cited_by": null,
      "authors": ["Y-Lan Boureau", "Francis Bach", "Yann LeCun", "Jean Ponce"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:tS2w5q8j5-wC",
      "title": "Workshop summary: Workshop on learning feature hierarchies",
      "link": "https://dl.acm.org/doi/abs/10.1145/1553374.1553543",
      "year": 2009,
      "cited_by": null,
      "authors": [
        "Kay Yu",
        "Ruslan Salakhutdinov",
        "Yann LeCun",
        "Geoff Hinton",
        "Yoshua Bengio"
      ],
      "description": "Workshop summary: Workshop on learning feature hierarchies | Proceedings of the 26th Annual \nInternational Conference on Machine Learning ACM Digital Library home ACM home Google, \nInc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals \nMagazines Proceedings Books SIGs Conferences People More Search ACM Digital Library \nSearchSearch Advanced Search icml Conference Proceedings Upcoming Events Authors \nAffiliations Award Winners More HomeConferencesICMLProceedingsICML '09Workshop \nsummary: Workshop on learning feature hierarchies research-article Share on Workshop \nsummary: Workshop on learning feature hierarchies Authors: Kay Yu NEC Laboratories America \nNEC Laboratories America View Profile , Ruslan Salakhutdinov University of Toronto, Canada \nUniversity of Toronto, Canada View Profile , Yann LeCun New York University New York \u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:n5u26LFhhPsC",
      "title": "International Journal of Computer Vision",
      "link": "https://nyuscholars.nyu.edu/en/publications/international-journal-of-computer-vision-editorial",
      "year": 2008,
      "cited_by": null,
      "authors": ["Andrew Fitzgibbon", "Camillo J Taylor", "Yann LeCun"],
      "description": "International Journal of Computer Vision: Editorial \u2014 NYU Scholars Skip to main navigation \nSkip to search Skip to main content NYU Scholars Home NYU Scholars Logo Help & FAQ \nHome Profiles Research Units Research output Search by expertise, name or affiliation \nInternational Journal of Computer Vision: Editorial Andrew Fitzgibbon, Camillo J. Taylor, \nYann LeCun Computer Science Research output: Contribution to journal \u203a Editorial \u203a peer-review \nOverview Fingerprint Original language English (US) Pages (from-to) 1-2 Number of pages \n2 Journal International Journal of Computer Vision Volume 80 Issue number 1 DOIs https://doi.org/10.1007/s11263-008-0162-4 \nState Published - Oct 2008 ASJC Scopus subject areas Software Computer Vision and \nPattern Recognition Artificial Intelligence Access to Document 10.1007/s11263-008-0162-4 \nOther files and links Link to publication in Scopus Link to citation list in \u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:lRnoeYR1YAAC",
      "title": "Computer Vision and Pattern Recognition (CVPR). Conference",
      "link": "https://pascal-francis.inist.fr/vibad/index.php?action=getRecordDetail&idt=20663189",
      "year": 2008,
      "cited_by": null,
      "authors": ["Andrew FITZGIBBON", "Camillo J TAYLOR", "Yann LECUN"],
      "description": "Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:m1aD9PlKDecC",
      "title": "Online Learning for Offroad Robots: Using Spatial Label Propagation to Learn Long-Range Traversability",
      "link": "http://yann.lecun.com/exdb/publis/psgz/hadsell-lagrtr-07.ps.gz",
      "year": 2008,
      "cited_by": null,
      "authors": ["Urs Muller", "Yann LeCun"],
      "description": "We present a solution to the problem of long-range obstacle/path recognition in autonomous robots. The system uses sparse traversability information from a stereo module to train a classifier online. The trained classifier can then accurately classify the entire scene. A distance-normalized image pyramid makes it possible to efficiently train a learning network on each frame seen by the robot, using large windows that contain contextual information as well as shape, color, and texture. Traversability labels are initially obtained for each target using a stereo module, then propagated to other views of the same target using temporal and spatial concurrences, producing a classifier that is largely view-invariant. A ring buffer simulates short-term memory and ensures that the discriminative learning is balanced and consistent. This long-range obstacle detection system (LROD) sees obstacles and paths at 30-40 meters, far\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:qe6vwMD2xtsC",
      "title": "SEIZURE PREDICTION USING MACHINE LEARNING ON BIVARIATE FEATURES FROM EEG",
      "link": "https://scholar.google.com/scholar?cluster=3019332669669866578&hl=en&oi=scholarr",
      "year": 2008,
      "cited_by": null,
      "authors": ["Piotr Mirowski", "D Madhavan", "Y LeCun", "RI Kuzniecky"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Mojj43d5GZwC",
      "title": "Efficient Learning and Inference of Sparse Overcomplete Representations (abstract)",
      "link": null,
      "year": 2008,
      "cited_by": null,
      "authors": ["Koray Kavukcuoglu", "Y-Lan Boureau", "Yann LeCun"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:CYCckWUYoCcC",
      "title": "Analysis of ictal dynamics using generative neural networks",
      "link": "https://scholar.google.com/scholar?cluster=11654728740879382296&hl=en&oi=scholarr",
      "year": 2007,
      "cited_by": null,
      "authors": ["Piotr Mirowski", "D Madhavan", "Y LeCun", "R Kuzniecky"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:vRqMK49ujn8C",
      "title": "Energy-Based Factor Graphs for Prediction in Relational Data",
      "link": null,
      "year": 2007,
      "cited_by": null,
      "authors": ["Sumit Chopra", "Yann LeCun"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:F2UWTTQJPOcC",
      "title": "Computer Vision and Pattern Recognition",
      "link": "https://ieeexplore.ieee.org/abstract/document/1640903/",
      "year": 2006,
      "cited_by": null,
      "authors": ["Andrew Fitzgibbon", "Camillo J Taylor", "Yann LeCun"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:zDAX0LUT-dsC",
      "title": "III Recognition of Object Categories-Synergistic Face Detection and Pose Estimation with Energy-Based Models",
      "link": "https://scholar.google.com/scholar?cluster=9278181026803688044&hl=en&oi=scholarr",
      "year": 2006,
      "cited_by": null,
      "authors": ["Margarita Osadchy", "Yann Le Cun", "Matthew L Miller"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:L_at8tGC9oEC",
      "title": "Message from the program and general chairs",
      "link": "https://experts.illinois.edu/en/publications/message-from-the-program-and-general-chairs",
      "year": 2006,
      "cited_by": null,
      "authors": [
        "Andrew Fitzgibbon",
        "Camillo J Taylor",
        "Yann LeCun",
        "Dan Huttenlocher",
        "David Forsyth"
      ],
      "description": "Message from the program and general chairs \u2014 University of Illinois Urbana-Champaign Skip to \nmain navigation Skip to search Skip to main content University of Illinois Urbana-Champaign \nHome University of Illinois Urbana-Champaign Logo Help & FAQ Home Profiles Research Units \nResearch & Scholarship Datasets Activities Press / Media Honors Search by expertise, name or \naffiliation Message from the program and general chairs Andrew Fitzgibbon, Camillo J. Taylor, \nYann LeCun, Dan Huttenlocher, David Forsyth Research output: Contribution to journal \u203a Editorial \n\u203a peer-review Overview Fingerprint Original language English (US) Article number 1640907 \nPages (from-to) xxvi-xxvii Journal Proceedings of the IEEE Computer Society Conference on \nComputer Vision and Pattern Recognition Volume 2 DOIs https://doi.org/10.1109/CVPR.2006.184 \nState Published - 2006 Externally published Yes Event 2006 \u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:DyXnQzXoVgIC",
      "title": "Prediction and analysis of ictal dynamics using computational neural networks",
      "link": "https://scholar.google.com/scholar?cluster=12051668417121811037&hl=en&oi=scholarr",
      "year": 2006,
      "cited_by": null,
      "authors": [
        "Deepak Madhavan",
        "Piotr Mirowski",
        "Yann LeCun",
        "Ruben Kuzniecky"
      ],
      "description": "MethodsIctal EEG samples were obtained from the NYU Epilepsy Center database, and converted into numerical matrices representing EEG channels values every 25ms (400Hz sampling). We implemented CNN software that would take as input a slice of 400-800 time samples (1-2 sec) of EEG channels and predict values 1 time step into the future. The training of the CNN consisted of iteratively optimizing the thousands of parameters of the CNN in order to minimize the error between the predicted values and the real measurements. The CNN was trained to correctly predict outputs 1 time step into the future, and to re-use its outputs as successive inputs at the following time steps, predicting 2.5 second EEG waveforms. After adequate training, channels determined to be the seizure onset zone via visual analysis were \u201cdeactivated\u201d by replacing their signals with random low frequency noise, and the output EEG was\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:PELIpwtuRlgC",
      "title": "Proceedings of the 2006 Computer Vision and Pattern Recognition Conference",
      "link": null,
      "year": 2006,
      "cited_by": null,
      "authors": ["Andrew Fitzgibbon", "Camillo J Taylor", "Yann LeCun"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:6G1OFF11MdUC",
      "title": "Technical report: Lush reference manual, code available at http://lush. sourceforge. net",
      "link": "https://nyuscholars.nyu.edu/en/publications/technical-report-lush-reference-manual-code-available-at-httplush",
      "year": 2002,
      "cited_by": null,
      "authors": ["Yann Lecun", "Leon Bottou"],
      "description": "Technical report: Lush reference manual, code available at http://lush.sourceforge.net \u2014 NYU \nScholars Skip to main navigation Skip to search Skip to main content NYU Scholars Home \nNYU Scholars Logo Help & FAQ Home Profiles Research Units Research output Search by \nexpertise, name or affiliation Technical report: Lush reference manual, code available at \nhttp://lush.sourceforge.net Yann Lecun, Leon Bottou Computer Science Research output: Other \ncontribution Overview Original language English (US) Type Technical report State Published - \n2002 Other files and links http://lush.sourceforge.net Cite this APA Standard Harvard \nVancouver Author BIBTEX RIS Lecun, Y., & Bottou, L. (2002). Technical report: Lush reference \nmanual, code available at http://lush.sourceforge.net. Technical report : Lush reference manual, \ncode available at http://lush.sourceforge.net. / Lecun, Yann; Bottou, Leon. 2002, Technical \u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:m1cs02wJCiwC",
      "title": "Advances in neural information processing systems: Proceedings of the first 12 conferences (CDROM)",
      "link": "https://nyuscholars.nyu.edu/en/publications/advances-in-neural-information-processing-systems-proceedings-of-",
      "year": 2001,
      "cited_by": null,
      "authors": ["MI Jordan", "Yann Lecun", "Sara A Solla"],
      "description": "Advances in neural information processing systems: Proceedings of the first 12 conferences \n(CDROM) \u2014 NYU Scholars Skip to main navigation Skip to search Skip to main content NYU \nScholars Home NYU Scholars Logo Help & FAQ Home Profiles Research Units Research \noutput Search by expertise, name or affiliation Advances in neural information processing \nsystems: Proceedings of the first 12 conferences (CDROM) MI Jordan (Other), Yann Lecun \n(Other), Sara A. Solla (Other) Computer Science Research output: Non-textual form \u203a Digital or \nVisual Products Overview Original language English (US) Publisher MIT Press Media of output \nCD State Published - 2001 Cite this APA Standard Harvard Vancouver Author BIBTEX RIS \nJordan, MI (Other), Lecun, Y. (Other), & Solla, SA (Other). (2001). Advances in neural information \nprocessing systems: Proceedings of the first 12 conferences (CDROM). Digital or Visual \u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:tzPJaSocouwC",
      "title": "\" S1202",
      "link": "https://scholar.google.com/scholar?cluster=2293276637371850273&hl=en&oi=scholarr",
      "year": 1999,
      "cited_by": null,
      "authors": [
        "CC Jay Kuo",
        "NS Jayant",
        "T Kanekiyo",
        "G Karlsson",
        "EW Knightly",
        "J Kovacevic",
        "C Kyriakakis",
        "C Lande",
        "Y LeCun",
        "JS Lim",
        "A Lombardo",
        "L Lucchese",
        "M Luglio",
        "P Mahonen",
        "P Marchisio",
        "F Mazzenga",
        "G Mazzini",
        "G Medioni",
        "R Menolascino",
        "K Miyaoku",
        "M Moeneclaey",
        "A Mouchtaris",
        "K Nahrstedt",
        "Nikias CL",
        "Y Ohira",
        "K Okada",
        "G Pacifici",
        "G Parladori",
        "F Pedersini",
        "C Perissinotto",
        "P Piccardo",
        "F Piolini",
        "GC Polyzos",
        "A Poma",
        "J Qiu",
        "L Rabiner",
        "CS Regazzoni",
        "S Ricciardi",
        "R Rinaldo",
        "M Rupi",
        "A Sarti",
        "G Schembra",
        "G Sciani",
        "B Shararay",
        "JR Smith",
        "R Stadler",
        "AJ Stienstra",
        "K Stuhlmuller",
        "CD Taylor",
        "A Teschioni",
        "A Thomasian",
        "P Tiwari",
        "P Tsakalides",
        "S Tubaro",
        "F Vatalaro",
        "G Vernazza",
        "J Wilder",
        "M Willebeek-LeMair",
        "M Yamashina",
        "Z Zhang"
      ],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:HE397vMXCloC",
      "title": "Predicting Learning Curves without the Ground Truth Hypothesis",
      "link": "http://leon.bottou.org/publications/psgz/nogroundtruth-1999.ps.gz",
      "year": 1999,
      "cited_by": null,
      "authors": ["Leon Bottou", "Yann LeCun", "Vladimir Vapnik"],
      "description": "Upper bounds for the deviation between test error and training error of a learning machine are derived in the case where no probability distribution that generates the examples is assumed to exist. The bounds are data-dependent and algorithm dependent. The result justi es the concept of data-dependent and algorithm dependent VC-dimension.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:h6vPvb0CPpsC",
      "title": "Future challenges",
      "link": "https://scholar.google.com/scholar?cluster=5581848193094693441&hl=en&oi=scholarr",
      "year": 1998,
      "cited_by": null,
      "authors": ["D LeCount"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:JoZmwDi-zQgC",
      "title": "Representing and Incorporating Prior Knowledge in Neural Network Training-12-Transformation Invariance in Pattern Recognition-Tangent Distance and Tangent Propagation",
      "link": "https://scholar.google.com/scholar?cluster=16446988086222022502&hl=en&oi=scholarr",
      "year": 1998,
      "cited_by": null,
      "authors": [
        "Patrice Y Simard",
        "Yann A LeCun",
        "John S Denker",
        "Bernard Victorri"
      ],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:q-HalDI95KYC",
      "title": "Invited lecture Neural Networks and Gradient-Based Learning in OCR",
      "link": "https://www.researchgate.net/profile/David-Grier-2/publication/2610282_Feature_Extraction_Approach_to_Blind_Source_Separation/links/0fcfd50c49050634f6000000/Feature-Extraction-Approach-to-Blind-Source-Separation.pdf#page=264",
      "year": 1997,
      "cited_by": null,
      "authors": ["Yann LeCun"],
      "description": "A large proportion of today's commercial Optical Character Recognition systems (OCR) and Handwriting Recognition Systems (HWR) use neural networks at the core of the recognition engine. Comparisons on standard databases show that Neural Networks, particularly multi-layer networks, offer a good combination of speed, generality, simplicity, and flexibility. They are also particularly well-suited for the large input dimension required for shape recognition tasks such as character recognition.Neural Networks and Machine Learning have become indispensable ingredient in the design of OCR/HWR systems. Those systems are generally built as a cascade of independent modules including: line and word locators, character segmenters, feature extractors, character recognizers, and language models. However, in most cases, only the character recognizer is trainable. We describe a new learning paradigm called Graph Transformer Networks that allows all the modules in such a system to be trained simultaneously so as to maximize a global performance measure. Each module, called a Graph Transformer, takes graphs as input and produces graphs as output. The arcs on the graphs carry numerical information (scalars or vectors) such as images, scores, and class labels. A gradient-based learning procedure can be used to train the parameters of the modules so as to maximize a global objective function.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:q3oQSFYPqjQC",
      "title": "Invited lecture",
      "link": null,
      "year": 1997,
      "cited_by": null,
      "authors": ["Yann LeCun"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:5OQWvpknaCIC",
      "title": "DRAFT DOCUMENT",
      "link": "https://leon.bottou.org/publications/pdf/transducer-1996.pdf",
      "year": 1996,
      "cited_by": null,
      "authors": ["L\u00e9on Bottou", "Yoshua Bengio", "Yann Le Cun"],
      "description": "We present here a general architecture for building Automatic Document Analysis Systems. This architecture is composed of a succession of modules transforming graphs describing lower-level hypotheses on the documents into graphs describing higher level hypotheses. This architecture generalizes techniques used in Neural Networks, Optical Character Recognition, Natural Language Processing and Speech Recognition.Section 2 presents the motivation for this work. We discuss in section 3 how we use weighted graphs for describing a document at various levels. Section 4 explains how these modules are built using generalized transducers. Section 5 explains how we can adapt simultaneously the parameters of all modules.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:gdpqVl03G0gC",
      "title": "Signature Veri\ufb01cation using a \u201cSiamese\u201d Time Delay Neural Network",
      "link": "http://yann.lecun.org/exdb/publis/psgz/bromley-94.ps.gz",
      "year": 1994,
      "cited_by": null,
      "authors": [
        "Jane Bromley",
        "Isabelle Guyon",
        "Yann LeCun",
        "Eduard Siickinger",
        "Roopak Shah"
      ],
      "description": "This paper describes an algorithm for veri\ufb01cation of signatures written on a pen-input tablet. The algorithm is based on a novel, arti\ufb01cial neural network, called a \u201cSiamese\u201d neural network. This network consists of two identical sub-networks joined at their outputs. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance between the two feature vectors. Veri\ufb01cation consists of comparing an extracted feature vector with a stored feature vector for the signer. Signatures closer to this stored representation than a chosen thresh-old are accepted, all other signatures are rejected as forgeries.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:JE23Nz_pzJ0C",
      "title": "Predicting transportpath degradation/failure based on recent performance history",
      "link": "https://nyuscholars.nyu.edu/en/publications/predicting-transportpath-degradationfailure-based-on-recent-perfo",
      "year": 1994,
      "cited_by": null,
      "authors": [
        "W Chiang",
        "Corinna Cortes",
        "LD Jackel",
        "Yann Lecun",
        "W Lee",
        "E Pednault",
        "V Vapnik"
      ],
      "description": "Predicting transportpath degradation/failure based on recent performance history \u2014 NYU \nScholars Skip to main navigation Skip to search Skip to main content NYU Scholars Home \nNYU Scholars Logo Help & FAQ Home Profiles Research Units Research output Search by \nexpertise, name or affiliation Predicting transportpath degradation/failure based on recent \nperformance history W. Chiang, Corinna Cortes, LD Jackel, Yann Lecun, W. Lee, E. Pednault, \nV. Vapnik Computer Science Research output: Chapter in Book/Report/Conference proceeding \n\u203a Conference contribution Overview Original language English (US) Title of host publication \nProceedings of the Symposium on Intelligent Systems in Communications and Power (SISCAP \n'94) Publisher IEEE State Published - 1994 Cite this APA Standard Harvard Vancouver Author \nBIBTEX RIS Chiang, W., Cortes, C., Jackel, LD, Lecun, Y., Lee, W., Pednault, E., & Vapnik, V\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:URolC5Kub84C",
      "title": "Pen-based visitor registration system (PENGUIN)",
      "link": "https://ieeexplore.ieee.org/abstract/document/471464/",
      "year": 1994,
      "cited_by": null,
      "authors": ["N Matic", "D Henderson", "Yann Le Cun", "Yoshua Bengio"],
      "description": "We describe a new electronic pen-based visitors registration system (PENGUIN) whose goal is to expand and modernize the visitor sign-in procedure at Bell Laboratories. The system uses a pen-interface (i.e. tablet-display) in what is essentially a form filling application. Our pen-interface is coupled with a powerful and accurate on-line handwriting recognition module. A database of AT&T employees (the visitors' hosts) and country names is used to check the recognition module outputs, in order to find the best match. The system provides assistance to the guard at one of the guard stations in routing visitors to their hosts. All the entered data are stored electronically. Initial testing shows that PENGUIN system performs reliably and with high accuracy. It retrieves the correct host name with 97% accuracy and the correct visitors citizenship with 99% accuracy. The system is robust and easy to use for both visitors and\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:d2hLqSNro9AC",
      "title": "APPLICATIONS IN CHARACTER",
      "link": "https://scholar.google.com/scholar?cluster=12793858956064345518&hl=en&oi=scholarr",
      "year": 1993,
      "cited_by": null,
      "authors": [
        "LD Jackel",
        "MY Battista",
        "J Ben",
        "J Bromley",
        "CJC Burges",
        "HS Baird",
        "E Cosatto",
        "JS Denker",
        "HP Graf",
        "HP Katseff",
        "Y LeCun",
        "CR Nohl",
        "E Sackinger",
        "CE Stenard",
        "BI Strom",
        "R Ting",
        "T Wood"
      ],
      "description": "Character Recognition has served as one of the principal proving grounds for neural-net methods and has emerged as one of the most successful applications of this technology. This chapter outlines optical character recognition I document analysis systems developed at AT&T Bell Labs that combine the strengths of machine-learning algorithms with high-speed, fine-grained parallel hardware. From our point of view, the most significant aspect of this work has been the efficient integration of diverse methods into end-to-end systems. In this paper we use the task of locating and reading ZIP codes on US mail pieces as an illustration of the character recognition/document",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=400&pagesize=100&citation_for_view=WLN3QrAAAAAJ:BwyfMAYsbu0C",
      "title": "Localisation de visages dans des images",
      "link": "https://nyuscholars.nyu.edu/en/publications/location-of-faces-in-images",
      "year": 1993,
      "cited_by": null,
      "authors": ["R Vaillant", "C Monrocq", "Y Le Cun"],
      "description": "In this article, we present on original approach for the localization of objects in an image. Our approach is neuronal and it includes two steps. In the first step, a rough localization is performed by presenting each pixel with its neighbourhood to a neural net which is able to indicate if this pixel and its neighbourhood are the image of the search object. This first filter is not very discriminant for the position. From its result, we can select areas which might contain an image of the object. In the second step, these areas are presented to another neural net which can determine the exact position of the object in each area. We apply this algorithm to the problem of localizing faces in images.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:2v_ZtQDX9iAC",
      "title": "Adams, M, Qian, Yi, Tomaszunas, J, Burtscheidt, J, Kaiser, E and lubasz, C",
      "link": "https://scholar.google.com/scholar?cluster=14453531143303758080&hl=en&oi=scholarr",
      "year": 1992,
      "cited_by": null,
      "authors": [
        "BE Boser",
        "E Sackinger",
        "J Bromley",
        "Y leCun",
        "LD Jackel"
      ],
      "description": "Designers working with the European Community's Conformance Testing Services programme have developed a system for testing VMEbus and Multibus II products. Conformance is necessary to allow and guarantee interoperability between products from different vendors. The EC's test system is mainly automated to reduce costs and ensure impartiality. An overview of the system's components familiarizes readers with its procedures.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:kFzFP8IdBVAC",
      "title": "propagation network",
      "link": "https://scholar.google.com/scholar?cluster=17915909500835017219&hl=en&oi=scholarr",
      "year": 1992,
      "cited_by": null,
      "authors": ["Y Le Cun", "RE Howard"],
      "description": "We present an application of back-propagation networks to handwritten digit recognition. Minimum pre-processing of the data was required, but the architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the US Postal Service.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:YP-lgzILWVMC",
      "title": "Applications of Neural Net Chips and Automatic",
      "link": "https://scholar.google.com/scholar?cluster=12175504512186810101&hl=en&oi=scholarr",
      "year": 1990,
      "cited_by": null,
      "authors": ["Y Le Cun", "LD Jackel", "B Boser", "JS Denker"],
      "description": "We describe two neural-net approaches to digit recognition. One method uses a neural-network chip to perform line thinning and local feature extraction. This preprocessing stage was designed by hand and did not involve any learning. However, automatic learning was used in the final classification step. The chip can process about 100 characters/sec, but the interface to the host computer limits the throughput to about 1 character/sec.The other method uses constrained automatic learning on pixel images with no preprocessing other than segmentation and size-normalization. It appears that good generalization performance cannot be obtained unless some ap riori knowl-edge about the task is built into the system. This paper demonstrates how such",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:9fSugHr6AN8C",
      "title": "D. Henderson, RE Howard, W. Hubbard, O. Matan, and SA Solla",
      "link": "https://scholar.google.com/scholar?cluster=5316904372730195427&hl=en&oi=scholarr",
      "year": 1990,
      "cited_by": null,
      "authors": [
        "LD Jackel",
        "B Boser",
        "JS Denker",
        "HP Graf",
        "Y Le Cun",
        "I Guyon"
      ],
      "description": "Neural-net hardware designers have been plagued by the following dilemma: with currently available electronics technology, it is impossible to design hardware that is flexible enough to encompass all possible network architectures and, at the same time, have enough resources to accommodate the large networks needed for many real-world applications. Designers typically guess that the ultimate users of the hardware will want several fully-connected layers of neurons with as many neurons and connections as will fit in the system.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Z5m8FVwuT1cC",
      "title": "HARDWARE FOR NEURAL-NET OPTICAL CHARACTER RECOGNITION",
      "link": "https://www.sciencedirect.com/science/article/pii/B9780444884008500273",
      "year": 1990,
      "cited_by": null,
      "authors": [
        "LD Jackel",
        "B Boser",
        "JS Denker",
        "HP Graf",
        "Y Le Cun",
        "I Guyon",
        "D Henderson",
        "RE Howard",
        "W Hubbard",
        "O Matan",
        "SA Solla"
      ],
      "description": "Through a series of experiments in optical character recognition, an understanding is beginning to emerge of the general nature of the hardware required. Rather than the fully-connected layered neural nets conceived by most hardware researchers, many machine perception tasks require local connectivity and repeated weight patterns between layers to support computing of convolutions. No current-day hardware is available to evaluate in parallel all the connections in a character recognition system. Fortunately, the repetitive nature of the convolution operation makes time-division multiplexing of the hardware possible and even efficient. To avoid I/O bottlenecks, the hardware must contain substantial input data buffers and shift registers. I/O requirements are further relaxed if several layers of the net are processed in a pipelined fashion without recourse to external storage. This paper will discuss hardware\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:rTD5ala9j4wC",
      "title": "Design Strategies",
      "link": "http://yann.lecun.org/exdb/publis/psgz/lecun-89.ps.gz",
      "year": 1989,
      "cited_by": null,
      "authors": ["Y le Cun"],
      "description": "An interesting property of connectionist systems is their ability to learn from examples. Although most recent work in the \ufb01eld concentrates on reducing learning times, the most important feature of a learning ma~ chine is its generalization performance. It is usually accepted that good generalization performance on real-world problems cannot be achieved unless some a prior: knowledge about the task is built into the system. Backopropagation networks prov1de a way of specifying such knowledge by imposing constraints both on the architecture of the network and on its weights. In general, such constraints can be conSidered as particular transformations of the parameter spaceBuilding a constrained network for image recognition appears to be a feasible task. We describe a small handwritten digit recognition problem and show that, even though the problem is linearly separable, single layer networks exhibit poor\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:oyQIn_cjXD4C",
      "title": "User manual: SN: A simulator for connectionist models",
      "link": "https://nyuscholars.nyu.edu/en/publications/user-manual-sn-a-simulator-for-connectionist-models",
      "year": 1988,
      "cited_by": null,
      "authors": ["Yann Lecun", "Leon Bottou"],
      "description": "User manual: SN: A simulator for connectionist models \u2014 NYU Scholars Skip to main \nnavigation Skip to search Skip to main content NYU Scholars Home NYU Scholars Logo Help \n& FAQ Home Profiles Research Units Research output Search by expertise, name or affiliation \nUser manual: SN: A simulator for connectionist models Yann Lecun, Leon Bottou Computer \nScience Research output: Other contribution Overview Original language English (US) Type \nUser manual State Published - Mar 1988 Cite this APA Standard Harvard Vancouver Author \nBIBTEX RIS Lecun, Y., & Bottou, L. (1988, Mar). User manual: SN: A simulator for connectionist \nmodels. User manual : SN: A simulator for connectionist models. / Lecun, Yann; Bottou, Leon. \n1988, User manual. Research output: Other contribution Lecun, Y & Bottou, L 1988, User \nmanual: SN: A simulator for connectionist models.. Lecun Y, Bottou L. User manual: SN: A \u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:T5V60G5X4B8C",
      "title": "eeekkk kkk kkSkSkkSSS kk",
      "link": "https://www.researchgate.net/profile/Ron-Gaynier/publication/234094403_Improved_Learning_Using_a_Pseudo_Fuzzy_Entropic_Error_Function/links/576a9db008ae5b9a62b380b9/Improved-Learning-Using-a-Pseudo-Fuzzy-Entropic-Error-Function.pdf",
      "year": 1988,
      "cited_by": null,
      "authors": ["T Kohonen", "Y LeCun", "B Boser"],
      "description": "In this paper a new error function te training neu-ral networks is developed using the concept of fuzzy entropy. For certain classes of problem this func-tion ia shown to outperform other widely-used error functions by a considerable margin. For other classes of problem, performance-less spectacular. Possible reasons for this discrepancy are discussed.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:bPZF39XCNPMC",
      "title": "Yann LeCun Yann LeCun",
      "link": "http://community.worldheritage.org/articles/eng/Yann_LeCun",
      "year": 1960,
      "cited_by": null,
      "authors": ["Yann LeCun"],
      "description": "Yann LeCun (born 1960) is a computer science researcher with contributions in machine learning, computer vision, mobile robotics and computational neuroscience. He is well known for his work on optical character recognition and computer vision using convolutional neural networks. He is also one of the main creators of the DjVu image compression technology (together with L\u00e9on Bottou and Patrick Haffner). He co-developed the Lush programming language with L\u00e9on Bottou.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:rUiCm8s56TIC",
      "title": "Mots-cl\u00e9s\u2013apprentissage",
      "link": "https://journals.openedition.org/annuaire-cdf/2467",
      "year": null,
      "cited_by": null,
      "authors": ["St\u00e9phane Mallat", "Yann LeCun", "Stanislas Dehaene"],
      "description": "Mots-cl\u00e9s - apprentissage Navigation \u2013 Plan du site L\u2019annuaire du Coll\u00e8ge de France L'annuaire \ndu Coll\u00e8ge de France Cours et travaux AccueilMots-cl\u00e9sapprentissage Chercher Mots-cl\u00e9s \n\u2013 apprentissage Article St\u00e9phane Mallat Sciences des donn\u00e9es [Texte int\u00e9gral] Paru dans L\u2019annuaire \ndu Coll\u00e8ge de France, 119 | 2022 St\u00e9phane Mallat Sciences des donn\u00e9es [Texte int\u00e9gral] \nParu dans L\u2019annuaire du Coll\u00e8ge de France, 118 | 2020 Yann LeCun Informatique et \nsciences num\u00e9riques [Texte int\u00e9gral] Paru dans L\u2019annuaire du Coll\u00e8ge de France, 116 | \n2018 Stanislas Dehaene Psychologie cognitive exp\u00e9rimentale [Texte int\u00e9gral] Paru dans L\u2019annuaire \ndu Coll\u00e8ge de France, 115 | 2016 Stanislas Dehaene Psychologie cognitive exp\u00e9rimentale [Texte \nint\u00e9gral] Paru dans L\u2019annuaire du Coll\u00e8ge de France, 113 | 2014 Stanislas Dehaene \nPsychologie cognitive exp\u00e9rimentale [Texte int\u00e9gral] Paru dans L\u2019annuaire du \u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:UbXTy9l1WKIC",
      "title": "Mots-cl\u00e9s\u2013r\u00e9seaux",
      "link": "https://journals.openedition.org/annuaire-cdf/12485",
      "year": null,
      "cited_by": null,
      "authors": ["Yann LeCun"],
      "description": "Mots-cl\u00e9s - r\u00e9seaux Navigation \u2013 Plan du site L\u2019annuaire du Coll\u00e8ge de France L'annuaire du \nColl\u00e8ge de France Cours et travaux AccueilMots-cl\u00e9sr\u00e9seaux Logo Coll\u00e8ge de France \nChercher Mots-cl\u00e9s \u2013 r\u00e9seaux Article Yann LeCun Informatique et sciences num\u00e9riques [Texte \nint\u00e9gral] Paru dans L\u2019annuaire du Coll\u00e8ge de France, 116 | 2018 Retour \u00e0 l\u2019index Index Auteurs \nMots-cl\u00e9s Chaires Num\u00e9ros en texte int\u00e9gral 2022119 2020118 2019117 2018116 2016115 \n2015114 2014113 2013112 2012111 2010109 2008108 Tous les num\u00e9ros Pr\u00e9sentation \u00c0 \npropos Le Coll\u00e8ge de France. Quelques donn\u00e9es sur son histoire et son caract\u00e8re propre \nInformations Contact Cr\u00e9dits Politiques de publication Suivez-nous Flux RSS Flux RSS Lettres \nd\u2019information La Lettre d\u2019OpenEdition OpenEdition Journals ISSN \u00e9lectronique 2109-9227 \nVoir la notice dans le catalogue OpenEdition Plan du site \u2013 Contact \u2013 Cr\u00e9dits \u2013 Flux de \u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Cew3JUE5alsC",
      "title": "Mots-cl\u00e9s\u2013informatique",
      "link": "https://journals.openedition.org/annuaire-cdf/1654",
      "year": null,
      "cited_by": null,
      "authors": [
        "Xavier Leroy",
        "G\u00e9rard Berry",
        "Yann LeCun",
        "Marie-Paule Cani",
        "Nicholas Ayache",
        "Bernard Chazelle",
        "Martin Abadi",
        "Serge Abiteboul"
      ],
      "description": "Mots-cl\u00e9s - informatique Navigation \u2013 Plan du site L\u2019annuaire du Coll\u00e8ge de France L'annuaire \ndu Coll\u00e8ge de France Cours et travaux AccueilMots-cl\u00e9sinformatique Chercher Mots-cl\u00e9s \u2013 \ninformatique Article Xavier Leroy Sciences du logiciel [Texte int\u00e9gral] Paru dans L\u2019annuaire du \nColl\u00e8ge de France, 119 | 2022 G\u00e9rard Berry Algorithmes, machines et langages [Texte int\u00e9gral] \nParu dans L\u2019annuaire du Coll\u00e8ge de France, 117 | 2019 G\u00e9rard Berry Algorithmes, machines \net langages [Texte int\u00e9gral] Paru dans L\u2019annuaire du Coll\u00e8ge de France, 116 | 2018 Yann \nLeCun Informatique et sciences num\u00e9riques [Texte int\u00e9gral] Paru dans L\u2019annuaire du Coll\u00e8ge \nde France, 116 | 2018 G\u00e9rard Berry Algorithmes, machines et langages [Texte int\u00e9gral] \nParu dans L\u2019annuaire du Coll\u00e8ge de France, 115 | 2016 Marie-Paule Cani Informatique et \nsciences num\u00e9riques [Texte int\u00e9gral] Paru dans L\u2019annuaire du Coll\u00e8ge de France, 115 | \u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:6sXp-ElUWUkC",
      "title": "Mots-cl\u00e9s\u2013intelligence artificielle",
      "link": "https://journals.openedition.org/annuaire-cdf/13182",
      "year": null,
      "cited_by": null,
      "authors": ["Yann LeCun"],
      "description": "Mots-cl\u00e9s - intelligence articielle Navigation \u2013 Plan du site L\u2019annuaire du Coll\u00e8ge de France \nL'annuaire du Coll\u00e8ge de France Cours et travaux AccueilMots-cl\u00e9sintelligence articielle Logo \nColl\u00e8ge de France Chercher Mots-cl\u00e9s \u2013 intelligence articielle Article Yann LeCun Informatique \net sciences num\u00e9riques [Texte int\u00e9gral] Paru dans L\u2019annuaire du Coll\u00e8ge de France, 116 | \n2018 Retour \u00e0 l\u2019index Index Auteurs Mots-cl\u00e9s Chaires Num\u00e9ros en texte int\u00e9gral 2022119 \n2020118 2019117 2018116 2016115 2015114 2014113 2013112 2012111 2010109 \n2008108 Tous les num\u00e9ros Pr\u00e9sentation \u00c0 propos Le Coll\u00e8ge de France. Quelques donn\u00e9es \nsur son histoire et son caract\u00e8re propre Informations Contact Cr\u00e9dits Politiques de publication \nSuivez-nous Flux RSS Flux RSS Lettres d\u2019information La Lettre d\u2019OpenEdition OpenEdition \nJournals ISSN \u00e9lectronique 2109-9227 Voir la notice dans le catalogue OpenEdition Plan du \u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:1u-ON_Kw9acC",
      "title": "Mots-cl\u00e9s\u2013sciences num\u00e9riques",
      "link": "https://journals.openedition.org/annuaire-cdf/1176",
      "year": null,
      "cited_by": null,
      "authors": [
        "Yann LeCun",
        "Marie-Paule Cani",
        "Nicholas Ayache",
        "Bernard Chazelle",
        "Martin Abadi",
        "Serge Abiteboul"
      ],
      "description": "Mots-cl\u00e9s - sciences num\u00e9riques Navigation \u2013 Plan du site L\u2019annuaire du Coll\u00e8ge de France \nCours et travaux AccueilMots-cl\u00e9ssciences num\u00e9riques Chercher Mots-cl\u00e9s \u2013 sciences \nnum\u00e9riques Article Yann LeCun Informatique et sciences num\u00e9riques [Texte int\u00e9gral] Paru \ndans L\u2019annuaire du Coll\u00e8ge de France, 116 | 2018 Marie-Paule Cani Informatique et sciences \nnum\u00e9riques [Texte int\u00e9gral] Paru dans L\u2019annuaire du Coll\u00e8ge de France, 115 | 2016 Nicholas \nAyache Informatique et sciences num\u00e9riques [Texte int\u00e9gral] Paru dans L\u2019annuaire du Coll\u00e8ge \nde France, 114 | 2015 Bernard Chazelle Informatique et sciences num\u00e9riques [Texte int\u00e9gral] \nParu dans L\u2019annuaire du Coll\u00e8ge de France, 113 | 2014 Martin Abadi Informatique et sciences \nnum\u00e9riques [Texte int\u00e9gral] Paru dans L\u2019annuaire du Coll\u00e8ge de France, 111 | 2012 Serge \nAbiteboul Informatique et sciences num\u00e9riques [Texte int\u00e9gral] Paru dans L\u2019annuaire du \u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:E6AQKB9d7D8C",
      "title": "Chaires\u2013chaire annuelle",
      "link": "https://journals.openedition.org/annuaire-cdf/2830",
      "year": null,
      "cited_by": null,
      "authors": [
        "Alain Wijffels",
        "Jean-Daniel Boissonnat",
        "Didier Roux",
        "Philippe Manoury",
        "Thomas Sterner",
        "Yann LeCun",
        "Jos\u00e9-Alain Sahel",
        "Alain Mabanckou",
        "Georges Calas",
        "Marie-Paule Cani",
        "Bernard Meunier",
        "Gilles Boeuf",
        "Fran\u00e7ois Bourguignon",
        "Nicholas Ayache",
        "Philippe Walter",
        "Tony Cragg",
        "Karol Beffa",
        "Anny Cazenave",
        "Dominique Kerouedan",
        "Bernard Chazelle",
        "Yves Br\u00e9chet",
        "Gilles Cl\u00e9ment",
        "Manuela Carneiro da Cunha",
        "Serge Abiteboul",
        "Jean-Paul Laumond",
        "Anselm Kiefer",
        "Jean-Marie Tarascon",
        "Ismail Serageldin",
        "Martin Abadi",
        "Elias Zerhouni",
        "Paul Colonna",
        "G\u00e9rard Berry",
        "Mathias Fink",
        "Esther Duflo",
        "Henri Leridon",
        "Pierre-Laurent Aimard",
        "Ariane Mnouchkine"
      ],
      "description": "Chaires - chaire annuelle Navigation \u2013 Plan du site L\u2019annuaire du Coll\u00e8ge de France Cours \net travaux AccueilChaireschaire annuelle Chercher Chaires \u2013 chaire annuelle Article Alain \nWijffels Chaire europ\u00e9enne [Texte int\u00e9gral] Paru dans L\u2019annuaire du Coll\u00e8ge de France, 117 | \n2019 Jean-Daniel Boissonnat Informatique et sciences num\u00e9riques [Texte int\u00e9gral] Paru dans \nL\u2019annuaire du Coll\u00e8ge de France, 117 | 2019 Didier Roux Innovation technologique \u2013 Liliane \nBettencourt [Texte int\u00e9gral] Paru dans L\u2019annuaire du Coll\u00e8ge de France, 117 | 2019 Philippe \nManoury Cr\u00e9ation artistique [Texte int\u00e9gral] Paru dans L\u2019annuaire du Coll\u00e8ge de France, 117 \n| 2019 Thomas Sterner D\u00e9veloppement durable : environnement, \u00e9nergie et soci\u00e9t\u00e9 [Texte \nint\u00e9gral] Paru dans L\u2019annuaire du Coll\u00e8ge de France, 116 | 2018 Yann LeCun Informatique \net sciences num\u00e9riques [Texte int\u00e9gral] Paru dans L\u2019annuaire du Coll\u00e8ge de France, 116 \u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:LnJLeQ70pnUC",
      "title": "A Path Towards Autonomous Machine Intelligence Download PDF",
      "link": "https://scholar.google.com/scholar?cluster=8427575232703846565&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": ["Yann LeCun"],
      "description": "How could machines learn as efficiently as humans and animals? How could machines learn to reason and plan? How could machines learn representations of percepts and action plans at multiple levels of abstraction, enabling them to reason, predict, and plan at multiple time horizons? This position paper proposes an architecture and training paradigms with which to construct autonomous intelligent agents. It combines concepts such as configurable predictive world model, behavior driven through intrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:yIkSIh5mphAC",
      "title": "Deep learning with Elastic Averaging SGD Download PDF",
      "link": "https://scholar.google.com/scholar?cluster=15249810814698714591&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": ["Sixin Zhang", "Anna E Choromanska", "Yann LeCun"],
      "description": "We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithms proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, ie the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:8cTaIddhMp4C",
      "title": "Leave your comfort zone!",
      "link": "https://rovo98.github.io/posts/1b531d86/",
      "year": null,
      "cited_by": null,
      "authors": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"],
      "description": "\u672c\u6587\u63d0\u4f9b\u4e86\u4f7f\u7528\u5b57\u7b26\u7ea7\u5377\u79ef\u7f51\u7edc (ConvNets) \u8fdb\u884c\u6587\u672c\u5206\u7c7b\u7684\u5b9e\u8bc1\u7814\u7a76. \u6211\u4eec\u6784\u5efa\u4e86\u51e0\u4e2a\u5927\u578b\u6570\u636e\u96c6, \u4ee5\u8868\u660e\u5b57\u7b26\u7ea7\u5377\u79ef\u7f51\u7edc\u53ef\u4ee5\u8fbe\u5230\u6700\u65b0\u6c34\u5e73\u6216\u7ade\u4e89\u7ed3\u679c. \u53ef\u4ee5\u4e0e\u4f20\u7edf\u6a21\u578b (\u4f8b\u5982\u8bcd\u888b BoW, n-grams \u5176 TFIDF \u53d8\u4f53) \u4ee5\u53ca\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b (\u4f8b\u5982\u57fa\u4e8e\u5355\u8bcd\u7684 ConvNets \u548c\u9012\u5f52\u795e\u7ecf\u7f51\u7edc) \u8fdb\u884c\u6bd4\u8f83.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:4b-LE5UluQcC",
      "title": "Unsupervised learning of invariant feature hierarchies with application to object recognition.\u201d CVPR, 2007. 1 Data Driven HMC Algorithm. DDHMC (motion-based proposals) 1\u00a0\u2026",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.182.2521",
      "year": null,
      "cited_by": null,
      "authors": ["Fu-jie Huang", "Y-lan Boureau", "Yann Lecun"],
      "description": "We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a pointwise sigmoid non-linearity, and a feature-pooling layer that computes the max of each filter output within adjacent windows. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level. Training a supervised classifier on these features yields 0.64% error on MNIST, and 54% average recognition rate on Caltech 101",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:2168PZyDXAcC",
      "title": "A Case Study in Handwritten Digit Recognition",
      "link": "https://leon.bottou.org/publications/pdf/icpr-1994.pdf",
      "year": null,
      "cited_by": null,
      "authors": [
        "L\u00e9on Bottou",
        "Corinna Cortes",
        "John S Denker",
        "Harris Drucker",
        "Isabelle Guyon",
        "LD Jackel",
        "Yann LeCun",
        "Urs A M\u00fcllerf",
        "Eduard S\u00e4ckinger",
        "Patrice Simard",
        "Vladimir Vapnik"
      ],
      "description": "This paper compares the performance of several classifier algorithms on a standard database of handwrit-ten digits. We consider not only raw accuracy, but also training time, recognition time, and memory require-ments. When available, we report measurements of the fraction of patterns that must be rejected so that the remaining patterns have misclassification rates less than a given threshold",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:k6hhvAYhr9EC",
      "title": "Fast Convolutional Nets With fbfft: A GPU Performance Evaluation Download PDF",
      "link": "https://scholar.google.com/scholar?cluster=1007488164264274502&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": [
        "Nicolas Vasilache",
        "Jeff Johnson",
        "Michael Mathieu",
        "Soumith Chintala",
        "Serkan Piantino",
        "Yann LeCun"
      ],
      "description": "We examine the performance profile of Convolutional Neural Network training on the current generation of NVIDIA Graphics Processing Units. We introduce two new Fast Fourier Transform convolution implementations: one based on NVIDIA's cuFFT library, and another based on a Facebook authored FFT implementation, fbfft, that provides significant speedups over cuFFT (over 1.5 x) for whole CNNs. Both of these convolution implementations are available in open source, and are faster than NVIDIA's cuDNN implementation for many common convolutional layers (up to 23.5 x for some synthetic kernel configurations). We discuss different performance regimes of convolutions, comparing areas where straightforward time domain convolutions outperform Fourier frequency domain convolutions. Details on algorithmic applications of NVIDIA GPU hardware specifics in the implementation of fbfft are also provided.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:UuaQgmrAG1sC",
      "title": "Color Documents on the Web with DjVu",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.108.2798",
      "year": null,
      "cited_by": null,
      "authors": [
        "Patrick Ha Ner",
        "Yann LeCun",
        "Leon Bottou",
        "Paul Howard",
        "Pascal Vincent",
        "Bill Riemers"
      ],
      "description": "We present a new image compression technique called\\DjVu \" that is speci cally geared towards the compression of scanned documents in color at high resolution. With DjVu, a magazine page in color at 300dpi typically occupies between 40KB and 80KB, approximately 5 to 10 times better than JPEG for a similar level of readability. Using a combination of Hidden Markov Model techniques and MDL-driven heursitics, DjVu rst classi es each pixel in the image as either foreground (text, drawings) or background (pictures, photos, paper texture). The pixel categories form a bitonal image which is compressed usingapattern matching technique that takes advantage of the similarities between character shapes. A progessive, wavelet-based compression technique, combined with a masking algorithm, is then used to compress the foreground and background images at lower resolution while minimizing the number of bits spent on the pixels that are not visible in the foreground and background planes. Encoders, decoders, and real-time, memory efcient plug-ins for various web browsers are available for all the major platforms. 1",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:kMrClmKSQGwC",
      "title": "Koray Kavukcuoglu",
      "link": "https://scholar.google.com/scholar?cluster=14663323041725008251&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": ["Marc'Aurelio Ranzato", "Yann Lecun"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:c3oc_9pK2TEC",
      "title": "Unsupervised Learning of Sparse and Invariant Features Hierarchies",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.105.5380&rep=rep1&type=pdf",
      "year": null,
      "cited_by": null,
      "authors": [
        "Marc\u2019Aurelio Ranzato",
        "Y-Lan Boureau",
        "Fu Jie Huang",
        "Yann LeCun"
      ],
      "description": "Unsupervised learning methods are commonly used to produce feature extractors in image analysis systems. A challenging question is whether these methods can learn invariant hierarchies of features. This would make much easier the problem of extracting useful information from very high dimensional datasets with few labeled samples, as it is often the case in many object recognition tasks in computer vision.The feed-forward, multi-stage Hubel and Wiesel architecture [1, 2, 3, 4, 5] stacks multiple levels of alternating convolutional feature detectors, and local pooling of feature maps using some weighted average of units within a neighborhood. These models have been successfully applied to handwriting recognition [1, 2], and generic object recognition [4, 5]. Learning features in existing models consists in handcrafting the first layers and training the upper layers by recording templates from the training set, which leads to inefficient representations [4, 5], or in training the entire architecture supervised, which requires large training sets [2, 3]. In all these models, invariance is never taken into account while learning the features, but might be achieved after training by using the pooling layers [6].",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:qxGx9raSUrEC",
      "title": "FAST INCREMENTAL LEARNING FOR AUTONOMOUS GROUND NAVIGATION",
      "link": "http://gvsets.ndia-mich.org/documents/AAIR/2015/Fast%20Incremental%20Learning%20for%20Autonomous%20Ground%20Navigation.pdf",
      "year": null,
      "cited_by": null,
      "authors": [
        "Artem Provodin",
        "Liila Torabi",
        "Urs Muller",
        "Beat Flepp",
        "Michael Sergio",
        "Jure \u017dbontar",
        "Yann LeCun",
        "LD Jackel"
      ],
      "description": "A promising approach to autonomous driving is machine learning. In machine learning systems, training datasets are created that capture the sensory input to a vehicle as well as the desired response. One disadvantage of using a learned navigation system is that the learning process itself may require both a huge number of training examples and a large amount of computing. To avoid the need to collect a large training set of driving examples, we describe a system that takes advantage of the immense number of training examples provided by ImageNet, but at the same time is able to adapt quickly using a small training set for the driving environment.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:9PbDelcLwNgC",
      "title": "Hastie, T., Tibshirani, R. & Buja, A.(1992b), Flexible discriminant analysis by optimal scoring, To be published. Hertz, J., Krogh, A. & Palmer, R.(1991), Introduction to the\u00a0\u2026",
      "link": "ftp://ftp.fhg.de/archive/gmd/div/documents/neuroprose/mda.ps.gz",
      "year": null,
      "cited_by": null,
      "authors": [
        "Y Le Cun",
        "B Boser",
        "JS Denker",
        "D Henderson",
        "R Howard",
        "W Hubbard",
        "S Leurgans",
        "R Moyeed",
        "B Silverman"
      ],
      "description": "9 DiscussionClassi cation by Gaussian mixtures is not a new idea. In this paper we have added to the functionality of this approach:",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:gFcjaLVeiroC",
      "title": "Orthogonal RNNs and Long-Memory Tasks",
      "link": "https://scholar.google.com/scholar?cluster=16192439922264816739&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": ["Yann LeCun", "NYU EDU"],
      "description": "Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter & Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:fiDIZbHD5NAC",
      "title": "HS Baird, AT&T Bell Laboratories, Murray Hill NJ 07974",
      "link": "https://www.researchgate.net/profile/Larry-Jackel/publication/3519696_VLSI_implementations_of_electronic_neural_networks_An_example_in_character_recognition/links/548adfbe0cf2d1800d7c348e/VLSI-implementations-of-electronic-neural-networks-An-example-in-character-recognition.pdf",
      "year": null,
      "cited_by": null,
      "authors": [
        "VLSI IMPLEMENTATIONS OF ELECTRONIC NEURAL NETyyOpK",
        "LD Jackel",
        "B Boser",
        "HP Graf",
        "JS Denker",
        "Y Le Cun",
        "D Henderson",
        "O Matan",
        "RE Howard"
      ],
      "description": "We have, however, found a more economical approach that is far more promising. Through a series of experiments in pattern recognition using neural-net algorithms, we identified a large class of applications where theoretical considerations that promote high-accuracy classification result in constrained network architectures. These constrained nets can map nicely onto appropriately designed hardware. Here, we discuss the concepts we learned from our pattern recognition experiments, we show how they can be applied to chip design and we describe a new neural-net chip.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Ecsxi449JjsC",
      "title": "Predicting Deeper into the Future of Semantic Segmentation\u2014Supplementary Material\u2014",
      "link": "http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Luc_Predicting_Deeper_Into_ICCV_2017_supplemental.pdf",
      "year": null,
      "cited_by": null,
      "authors": [
        "Pauline Luc",
        "Natalia Neverova",
        "Camille Couprie",
        "Jakob Verbeek",
        "Yann LeCun"
      ],
      "description": "We propose an improvement over an initial baseline that has previously been used for next frame prediction in the space of RGB intensities and relying on optical flow employed in [4] and [5]. In both the initial and our improved flow baselines, the approach is based on using the optical flow field Ft\u2192 t\u2212 1 computed from Xt, the RGB frame at time t, to Xt\u2212 1, the RGB frame at time t\u2212 1.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:hcF2OqvMasEC",
      "title": "Convolutional Nets",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.8310&rep=rep1&type=pdf",
      "year": null,
      "cited_by": null,
      "authors": ["Yann Le Cun"],
      "description": "Defeatism: since no good parameterization of the \u201cAIset\u201d is available, let's parameterize a much smaller set for each specific task through careful engineering (preprocessing, kernel....). Denial: kernel machines can approximate anything we want, and the VC bounds guarantee generalization. Why would we need anything else? unfortunately, kernel machines with common kernels can only represent a tiny subset of functions efficiently",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:1xqo9R7SDZkC",
      "title": "Deep Learning Made Easier",
      "link": "https://users.ics.aalto.fi/praiko/papers/mlp_trans_slides.pdf",
      "year": null,
      "cited_by": null,
      "authors": ["Tapani Raiko", "Harri Valpola", "Yann LeCun"],
      "description": "Background\u2022 Learning deep networks (many hidden layers) used to be difficult\u2022 Layerwise pretraining by RBMs or denoising autoencoders helps",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:LimhlhUO2s4C",
      "title": "Fast approximate prediction of sparse codes",
      "link": "https://scholar.google.com/scholar?cluster=14524535623667536819&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": ["Karol Gregor", "Yann LeCun"],
      "description": "Sparse coding is the problem of reconstructing input vectors using a linear combination of basis vectors with sparse coefficients. Sparse coding has become a popular method for extracting features representations from raw data. Finding the sparse code for a given input involves minimizing a quadratic reconstruction error with an L1 penalty term. Consequently, a large amount of research has been devoted to efficiently solving this optimization problem 1\u20133, 6\u20138. Even so, these algorithms are often too slow for such applications as visual object recognition. Here we propose two versions of a very fast algorithm that produces approximate estimates of the optimal sparse code. While our method only produces approximate solutions, it can be used to compute good visual features, and can be used to initialize exact iterative algorithms. The main idea is to train a non-linear, feed-forward predictor with a particular architecture and a fixed depth to produce the best possible approximation of the sparse code. A version of the method, which can be seen as a trainable version of Li and Osher\u2019s coordinate descent method, is shown to produce approximate solutions with 10 times less computation than Li and Osher\u2019s for the same approximation error. Unlike previous proposals for sparse code predictors4, 5, the system allows a kind of approximate \u201cexplaining away\u201d to take place during inference. The resulting approximator is differentiable and can be included into globally-trainable recognition systems.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:RL3Vt95MvH0C",
      "title": "Efficient BackProp Efficient BackProp",
      "link": "https://scholar.google.com/scholar?cluster=1516191438983406366&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": ["Yann Le Cun"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:i_acAUMCj1cC",
      "title": "ALGORITHMS FOR HANDWRITTEN DIGIT",
      "link": "https://www.researchgate.net/profile/Harris-Drucker/publication/2333952_Comparison_Of_Learning_Algorithms_For_Handwritten_Digit_Recognition/links/540757d70cf2c48563b2ace0/Comparison-Of-Learning-Algorithms-For-Handwritten-Digit-Recognition.pdf",
      "year": null,
      "cited_by": null,
      "authors": [
        "Y LeCun",
        "L Jackel",
        "L Bottou",
        "A Brunot",
        "C Cortes",
        "J Denker",
        "H Drucker",
        "I Guyon",
        "U M\u00fcller",
        "E S\u00e4ckinger",
        "P Simard",
        "V Vapnik"
      ],
      "description": "This paper compares the performance of several classifier algorithms on a standard database of handwritten digits. We consider not only raw accuracy, but also rejection, training time, recognition time, and memory requirernets.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:HoaxrAv1AIcC",
      "title": "Loss Functions for Energy-Based Models With Applications to Object Recognition.",
      "link": "https://scholar.google.com/scholar?cluster=14285976687485572327&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": ["Yann LeCun", "Fu Jie Huang"],
      "description": "What\u2019s bad about probabilistic models? with generative models, normalization over X is useless, difficult, and restrictive. we should never, ever, EVER have to normalize anything over X (normalization in high dimensional spaces is silly). only a tiny number of models are pre-normalized (eg Gaussians). only a very small number of models have tractable partition functions (easily normalizable). many models have intractable partition functions. most models are not even normalizable. If we only care about making good decisions (picking the best Y), why should we have to estimate the correct P (Y| X) over the full range of Y? We merely need P (Y| X) to have maxima at the right places. We have to come up with embarassing justifications for fudge factors that make things work, but break the nornalization. For example P \u03b1 appearance\u00d7 Pshape in image recognition, or P \u03b1 transition\u00d7 Pemission in speech recognition\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:jenZ_en7QtoC",
      "title": "KAROL GREGOR",
      "link": "https://scholar.google.com/scholar?cluster=13809165225433449167&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": ["Yann LeCun", "Shivaji Sondhi", "David Huse"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:seU1ZbiIO-YC",
      "title": "Emergence of complex like cells in a temporal product network and its smooth generalization",
      "link": "https://scholar.google.com/scholar?cluster=6011857757145851447&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": ["Karol Gregor", "Yann LeCun"],
      "description": "One of the important ingredients in deep learning is the creation of invariant representations at every level in the hierarchy in an unsupervised fashion. In the cortex, at a low level there are complex cells that respond to edges of specific orientation and frequency but are invariant to a range of positions. At higher levels there are neurons that respond to specific objects and are invariant to object position, lighting, orientation, specific shape or even type. At the same time the algorithms that we use need to be able to work with real world data such as large enough sized natural videos obtaining good performance for example on classification. In this work we introduce two concepts. The first is an algorithm for producing invariant representations based on breaking down an input into two complementary representations, one of which is an invariant representation learned using a new implementation of temporal consistency. Training on patches of video produces cells with complex cell like properties. The second is a network that applies to entire images, not just image patches and has smooth geometry in a sense that it has no artificial cuts such as the ones produced in sub-sampling. We have a reason to believe that its representation is more efficient than that of the convolutional neural network, which is a limiting case of this one. Approximate properties of simple cells can be obtained using for example the simple sparse coding algorithm1. Invariance properties or properties of simple cells are harder to obtain. Ideas include topographically organizing simple cells and pooling the neighbors3, making the evolution of hidden states slow4 when training\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:oea97a5D_h0C",
      "title": "Ask the locals: multi-way local pooling for image recognition Supplemental material.",
      "link": "https://y-lanb.org/files/publi/boureau-iccv-11-supplemental.pdf",
      "year": null,
      "cited_by": null,
      "authors": [
        "Y-Lan Boureau",
        "Nicolas Le Roux",
        "Francis Bach",
        "Jean Ponce",
        "Yann LeCun"
      ],
      "description": "This supplemental material contains numerical results of experiments plotted in the main paper, and results from the Caltech-256 and Scenes datasets that were omitted due to space constraints.Caltech 30 tr. Scenes K= 256, Pre, P= 1 70. 5\u00b10. 8 79. 2\u00b10. 7 P= 4 72. 6\u00b11. 0 81. 7\u00b10. 5 P= 16 74. 0\u00b11. 0 82. 0\u00b10. 7 P= 64 75. 0\u00b10. 8 81. 4\u00b10. 4 P= 128 75. 5\u00b10. 8 81. 0\u00b10. 3 P= 256 75. 1\u00b11. 0\u2212 P= 512 74. 5\u00b10. 7\u2212 P= 1024 73. 8\u00b10. 8\u2212 P= 1+ 16 74. 2\u00b11. 1 81. 5\u00b10. 8 P= 1+ 64 75. 6\u00b10. 6 81. 9\u00b10. 7 K= 256, Post, P= 4 72. 4\u00b11. 2 79. 6\u00b10. 8 P= 16 75. 1\u00b10. 8 80. 9\u00b10. 6 P= 64 76. 4\u00b10. 8 81. 1\u00b10. 6 P= 128 76. 7\u00b10. 8 81. 1\u00b10. 5 P= 256 75. 9\u00b10. 8\u2212 P= 512 75. 2\u00b10. 8\u2212 P= 1024 74. 2\u00b10. 6\u2212 K= 1024, Pre, P= 1 75. 6\u00b10. 9 82. 7\u00b10. 7 P= 4 76. 0\u00b11. 2 83. 5\u00b10. 8 P= 16 76. 3\u00b11. 1 82. 8\u00b10. 8 P= 64 76. 2\u00b10. 8 81. 8\u00b10. 7 P= 128\u2212 80. 9\u00b10. 7 P= 1+ 16 76. 9\u00b11. 0 83. 3\u00b11. 0 P= 1+ 64 77. 3\u00b10. 6 83. 1\u00b10. 7 K= 1024, Post, P= 4 75. 8\u00b11. 5. 82. 9\u00b10. 6 P= 16 77. 0\u00b10. 8 82. 9\u00b10. 5 P= 64 77. 1\u00b10. 7 82. 4\u00b10. 7 P= 128 76. 9\u00b10. 5 82. 0\u00b10. 7 P= 256 75. 7\u00b10. 8\u2212 Table 1. Accuracy as a function of whether clustering is performed before (Pre) or after (Post) the encoding, K: dictionary size, and P: number of configuration space bins.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:ObAD8Md4PD8C",
      "title": "Pedestrian Detection with Unsupervised Multi-Stage Feature Learning (Supplementary Material)",
      "link": "https://scholar.google.com/scholar?cluster=15458823367247357048&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": [
        "Pierre Sermanet",
        "Koray Kavukcuoglu",
        "Soumith Chintala",
        "Yann LeCun"
      ],
      "description": "Figure 1: Differences between continuous (proposed) and discrete Area Under Curve (AUC) on the INRIA medium scale experiment. In 1 (a), we compute the continuous AUC as opposed to a discrete AUC 1 (b) based on a few points in the standard benchmarking software. 1 (b) clearly shows the shortcomings of the discrete AUC which wrongly attributes a 0% AUC to ConvNet-F instead of 10.36%. Additionally, several models are re-ranked when using the continuous AUC.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:J1AsQIbV7j8C",
      "title": "Learning a Deep Hierarchy of Sparse and Invariant Features",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.44&rep=rep1&type=pdf",
      "year": null,
      "cited_by": null,
      "authors": [
        "Yann LeCun",
        "Marc'Aurelio Ranzato",
        "YLan Boureau",
        "FuJie Huang",
        "Sumit Chopra"
      ],
      "description": "Is there a universal learning algorithm/architecture which, given a small amount of appropriate prior structure, can produce an intelligent vision system? Or do we have to keep accumulating a large repertoire of pre-engineered \u201cmodules\u201d to solve every specific problem an intelligent vision system must solve?",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:NKe3Q23MlFgC",
      "title": "An Analog Neural Network Processor and its Application to High-speed Character Recognition",
      "link": "https://www.academia.edu/download/35246088/Boser-91.pdf",
      "year": null,
      "cited_by": null,
      "authors": ["Yann LeCun", "Richard E Howard", "Lawrence D Jackel"],
      "description": "A high-speed programmable neural network chip and its application to character recognition are described. A network with over 130,000 connections has been implemented on a single chip and operates at a rate of over 1000 classifications per second. The chip performs up to 2000 multiplications and additions simultaneously. Its datapath is suitable particularly for the convolutional architectures that are typical in pattern classification networks, but can also be configured for fully connected or feedback topologies. Computations are performed with 6Bits accuracy for the weights and 3Bits for the states. The chip uses analog processing internally for higher density and reduced power dissipation, but all input/output is digital to simplify system integration.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:EKd_EOGQzkMC",
      "title": "http://yann. lecun. com",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.8259&rep=rep1&type=pdf",
      "year": null,
      "cited_by": null,
      "authors": ["Yann LeCun", "Fu Jie Huang", "Leon Bottou"],
      "description": "Generic Object Recognition is the problem of detecting and classifying objects into generic categories such as \u201ccars\u201d,\u201ctrucks\u201d,\u201cairplanes\u201d,\u201canimals\u201d, or \u201chuman figures\u201d Appearances are highly variable within a category because of shape variation, position in the visual field, scale, viewpoint, illumination, albedo, texture, background clutter, and occlusions.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:gXFvZ3BI3UoC",
      "title": "Deep Learning for Generic Object Recognition",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.220.5451&rep=rep1&type=pdf",
      "year": null,
      "cited_by": null,
      "authors": ["Yann LeCun"],
      "description": "What happened?(1) Machine Learning,(2) Moore's Law. Generic Object Recognition is the problem of detecting and classifying objects into generic categories such as \u201ccars\u201d,\u201ctrucks\u201d,\u201cairplanes\u201d,\u201canimals\u201d, or \u201chuman figures\u201d Appearances are highly variable within a category because of shape variation, position in the visual field, scale, viewpoint, illumination, albedo, texture, background clutter, and occlusions.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:cvMPO0XfNn8C",
      "title": "EnergyBased Models: The Cure Against Bayesian Fundamentalism",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.448.580&rep=rep1&type=pdf",
      "year": null,
      "cited_by": null,
      "authors": [
        "Yann LeCun",
        "Marc'Aurelio Ranzato",
        "FuJie Huang",
        "YLan Boureau",
        "Sumit Chopra"
      ],
      "description": "2. Pick an inference algorithm for Y: MAP or conditional distribution, belief prop, min cut, variational methods, gradient descent, MCMC, HMC..... 3. Pick a loss function: in such a way that minimizing it with respect to W over a training set will make the inference algorithm find the correct Y for a given X.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:d9ydBXnamCkC",
      "title": "LeRec: A NN/HMM Hybrid for On-Line Handwriting Recognition",
      "link": "https://scholar.google.com/scholar?cluster=1393328231935414751&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": ["Yann LeCun", "Craig Noh", "Chris Burges"],
      "description": "We introduce a new approach for on-line recognition of handwritten words written in unconstrained mixed style. The preprocessor performs a word-level normalization by fitting a model of the word structure using the EM algorithm. Words are then coded into low resolution\" annotated images\" where each pixel contains information about trajectory direction and curvature. The recognizer is a convolution network that can be spatially replicated. From the network output, a hidden Markov model produces word scores. The entire system is globally trained to minimize word-level errors.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:QXXbHxWZe5oC",
      "title": "Supervised and Unsupervised Methods for Learning Invariant Feature Hierarchies",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.439.2608&rep=rep1&type=pdf",
      "year": null,
      "cited_by": null,
      "authors": [
        "Yann LeCun",
        "Marc'Aurelio Ranzato",
        "FuJie Huang",
        "YLan Boureau",
        "Sumit Chopra"
      ],
      "description": "Is there a general principle, or should we just resort to clever engineering (or to a large collection of tricks)? Is there a universal learning algorithm/architecture which, given a small amount of appropriate prior structure, can produce an intelligent vision system? Or do we need to accumulate a large repertoire of \u201cmodules\u201d to solve every specific problem an intelligent vision system must solve. How would we assemble those modules?",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:MwHAKEtqQGUC",
      "title": "Datasets of the Unsupervised and Transfer Learning Challenge",
      "link": "http://proceedings.mlr.press/v27/supplemental/datasetsutl12a.pdf",
      "year": null,
      "cited_by": null,
      "authors": ["Yann LeCun", "Corinna Costes"],
      "description": "To that end, the evaluation data (validation set or final evaluation set) are partitioned randomly into a training set and a test set. The parameters of the linear classifier are adjusted using the training set. Then, predictions are made on test data using the trained model. The Area Under the ROC curve (AUC) is computed to assess the performance of the linear classifier. The results are averaged over all tasks and over several random splits into a training set and a complementary test set. The number of training examples is varied and the AUC is plotted against the number of training examples in a log scale (to emphasize the results on small numbers of training examples). The area under the learning curve (ALC) is used as scoring metric to synthesize the results.The participants are ranked by ALC for each individual dataset. The participants having submitted a complete experiment (results on all 5 datasets of the challenge) enter the final ranking. The winner is determined by the best average rank over all datasets for the results of their last complete experiment.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:D_tqNUsBuKoC",
      "title": "an\u00a5 \u00a7 ritten igit Recognition \u00a7 it1 a Bac57ProBagation Fet \u00a7 or5",
      "link": "https://scholar.google.com/scholar?cluster=2229214037380198359&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": [
        "Y Le Cun",
        "B Boser",
        "JS Denker",
        "D Henderson",
        "RE Howard",
        "W Hubbard",
        "LD Jackel"
      ],
      "description": "We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the US Postal Service.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:L_l9e5I586QC",
      "title": "Report from the NSF Workshop on Integrating Approaches to Computational Cognition",
      "link": "http://cognitive-science.info/wp-content/uploads/2014/07/report.pdf",
      "year": null,
      "cited_by": null,
      "authors": [
        "Matt Jones",
        "Richard M Shiffrin",
        "Alan L Yuille",
        "Jun Zhang",
        "Xiaojin Zhu",
        "Thomas L Griffiths",
        "Charles Kemp",
        "Yann LeCun",
        "Hongjing Lu",
        "David A McAllester",
        "Ruslan Salakhutdinov",
        "Bernhard Sch\u00f6lkopf",
        "Satinder Singh",
        "Robin D Thomas"
      ],
      "description": "At a workshop sponsored by the National Science Foundation, 18 distinguished researchers from the fields of Cognitive Science(CS) and Machine Learning(ML) met in Arlington, VA in May 2013 to discuss computational approaches to cognition in human and artificial systems. The purpose of the workshop was to identify frontiers for collaborative research integrating(a) mathematical and computational modeling of human cognition with (b) machine learning and machine intelligence. The researchers discussed opportunities and challenges for how the two fields can advance each other and what sort of joint efforts are likely to be most fruitful.There are several reasons to believe that theories of human cognition and of machine intelligence are currently in position to greatly benefit from each other. The mathematical and computational tools developed for designing artificial systems are beginning to make an impact on theoretical and empirical work in CS, and conversely CS offers a range of complex problems that challenge and test ML",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:5bg8sr1QxYwC",
      "title": "Name and affiliation of the authors",
      "link": "https://scholar.google.com/scholar?cluster=3496479653097976503&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": ["Christophe Monrocq", "Yann Le Cun"],
      "description": "In this article, we present an original approach for the localization of objects in an image. Our approach is neuronal and it includes two steps. In the first step, a rough localization is performed by presenting each pixel with its neighbourhood to a neural net which is able to indicate if this pixel and its neighbourhood are the image of the search object. This first filter is not very discriminant for the position. From its result, we can select areas which might contain an image of the object. In the second step, these areas are presented to another neural net which can determine the exact position of the object in each area. We apply this algorithm to the problem of localizing faces in images.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:raTqNPD5sRQC",
      "title": "A MULTI-RANGE VISION STRATEGY FOR AUTONOMOUS OFFROAD NAVIGATION Raia Hadsell1 Ayse Erkan1",
      "link": "https://www.actapress.com/Abstract.aspx?paperId=31340",
      "year": null,
      "cited_by": null,
      "authors": [
        "Pierre Sermanet",
        "Jan Ben",
        "Koray Kavukcuoglu",
        "Urs Muller",
        "Yann LeCun"
      ],
      "description": "road is seen to the right, so the dead end is avoided. Bottom row: At the end of the test, the robot clearly sees the road far beyond stereo range.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:-jrNzM816MMC",
      "title": "THROUGH MATRIX INVERSION AFTER NOISE INJECTION",
      "link": "http://yann.lecun.org/exdb/publis/pdf/lecun-89b.pdf",
      "year": null,
      "cited_by": null,
      "authors": ["Yann Le Cun", "Conrad C Galland", "Geoffrey E Hinton"],
      "description": "Learning procedures that measure how random perturbations of unit activities correlate with changes in reinforcement are inefficient but simple to implement in hardware. Procedures like back-propagation (Rumelhart, Hinton and Williams, 1986) which compute how changes in activities affect the output error are much more efficient, but require more complex hardware. GEMINI is a hybrid procedure for multilayer networks, which shares many of the implementation advantages of correlational reinforcement procedures but is more efficient. GEMINI injects noise only at the \ufb01rst hidden layer and measures the resultant effect on the output error. A linear network associated with each hidden layer iteratively inverts the matrix which relates the noise to the error change, thereby obtaining the error-derivatives. No back-propagation is involved, thus allowing un-known non-linearities in the system. Two simulations\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:HGTzPopzzJcC",
      "title": "Handwritten Digit Recognition: Applications of Neural Network Chips",
      "link": "http://yann.lecun.org/exdb/publis/psgz/lecun-89c.ps.gz",
      "year": null,
      "cited_by": null,
      "authors": [
        "Y Le Cun",
        "LD Jackel",
        "B Boser",
        "JS Denker",
        "HP Graf",
        "I GuyOI"
      ],
      "description": "HIS ARTICLE DESCRIBES TWO NEW METHODS for achieving handwritten digit recognition. The task of handwritten digit recognition was chosen for investigation not only because it has considerable practical interest. but because it is relatively well-de\ufb01ned and is sufficiently complex to con-stitute a meaningful test of connectionist methods. Simple classi\ufb01cation techniques applied to pixel images do not provide high recognition rates because systems based on these techniques contain little prior knowledge about the to-pology of the task. Knowledge can be built into the system by changing the representation of a digit from a pixel image to a prede\ufb01ned feature description. The \ufb01rst of our methods implements this idea by performing feature extraction with a neural network chip The feature representation can then be used by a relatively simple classi\ufb01er. consisting of a two-layer network trained with back-propagation\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:MAUkC_7iAq8C",
      "title": "Greedy structured dictionaries for fast sparse coding.",
      "link": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.475.4065&rep=rep1&type=pdf",
      "year": null,
      "cited_by": null,
      "authors": ["Y-Lan Boureau", "Jean Ponce", "Yann LeCun"],
      "description": "Many feature extraction techniques for image recognition in recent years implement some variant of sparse coding [6] within a processing pipeline that alternates coding and pooling operations (eg,[1, 2, 4, 7, 8]). The resulting feature vectors can then be fed to a linear classifier such as a support vector machine.Despite its popularity, sparse coding suffers from several shortcomings. When increasing dictionary size, the computational cost of sparse coding often becomes impractical before performance saturates for realistic datasets, with dictionaries of tens of thousands of atoms performing better than smaller ones. Restricting the set of atoms that can be activated to encode a given input reduces the size of the effective dictionary over which the optimization is performed. This general idea underlies several recent proposals such as locally linear coding (LLC)[7], which selects the active set among the k nearest neighbors of the input to be encoded, or work by Yang et al.[9], which first clusters the data, then learns a separate smaller dictionary within each cluster, with the full underlying dictionary having more than 250,000 atoms.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Dem6FJhTUoYC",
      "title": "DECOUPLING THE DATA GEOMETRY FROM THE PARAMETER GEOMETRY FOR STOCHASTIC GRADIENTS",
      "link": "https://people.idsia.ch/~tom/publications/snowbird.pdf",
      "year": null,
      "cited_by": null,
      "authors": ["TOM SCHAUL", "SIXIN ZHANG", "YANN LECUN"],
      "description": "Besides its fast convergence, SGD has been observed to sometimes lead to significantly better generalization performance than batch gradient descent. SGD is also quicker than batch methods in adapting to non-stationary data distributions. Its Achilles heel are the inherently sequential updates, making it very difficult to parallelize across many machines; which is clashing with the goals of large-scale learning. Our goals here are twofold. On the theoretical level, we want to gain a fuller understanding of how the dynamics of stochastic updates contrast with those of batch updates, and how they are affected by the conditioning of energy surfaces, the presence of local optima, and the properties of the data distribution. On the practical level, we want to use this knowledge to design more efficient mini-batch SGD variants (which are parallelizable), together with robust settings for their hyper-parameters. The study of stochastic gradient methods dates back over six decades [1, 2, 3, 4, 5, 6, 7], but to our knowledge, the present questions remain understudied. The most similar viewpoint is found in approaches based on natural gradients and information geometry [8, 9, 10].",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:jE2MZjpN3IcC",
      "title": "Convolutional Networks Applied to House Numbers Digit Classification",
      "link": "https://scholar.google.com/scholar?cluster=8601689137336708790&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": ["Pierre Sermanet", "Soumith Chintala", "Yann LeCun"],
      "description": "Characters recognition in documents can be considered a solved task for computer vision whether handwritten or typed. It is however a harder problem in the context of complex scenes like photographs where best current methods lag behind human performance.[1] recently introduced a new digit classification dataset built from house numbers extracted from street level images. It is similar to the popular MNIST dataset (10 digits, 32x32 inputs), but an order of magnitude bigger (600,000 labeled digits) and contains color information.[1] demonstrate the superiority of features learned all the way through the architecture as opposed to hand-designed features. Such superiority was also previously shown among others in a traffic sign classification challenge [2] where two independent teams obtained all top scores against various other approaches using convolutional neural networks (ConvNet) with learned features [3, 4].[1] also argue about the superiority of unsupervised learning on this task. We obtain a 2.7 points accuracy improvement (93.3% accuracy) over the current state-of-the-art of 90.6% using a fully supervised ConvNet. This ConvNet departs from the traditional ConvNet architecture by using L2 pooling layers rather than average subsampling, and skipping connections between stages to the classifier [3]. It is implemented using the EBLearn C++ open-source framework 1 [5].",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:-7ulzOJl1JYC",
      "title": "Artificial neural networks are computationally powerful and exhibit brain-like dynamics. However, it is generally believed that the backpropagation algorithm, commonly used to\u00a0\u2026",
      "link": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.368.2387&rep=rep1&type=pdf",
      "year": null,
      "cited_by": null,
      "authors": ["Jason Tyler Rolfe", "Matthew Cook", "Yann LeCun"],
      "description": "The set of intrinsic gradient networks is large and diverse; an example will demonstrate the power of this technique. We can construct a hierarchical sparse coding intrinsic gradient network with consecutive layers of non-negative realvalued units x1, x2,\u00b7\u00b7\u00b7, xn connected by parameter matrices Wi. The dynamics minimize1 the potential function2",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:Ade32sEp0pkC",
      "title": "Deep feature learning for real-time recognition and detection",
      "link": "http://brain2grid.com/wp-content/uploads/2011/11/Boureau.pdf",
      "year": null,
      "cited_by": null,
      "authors": ["Pierre Sermanet", "Y-Lan Boureau", "Yann LeCun"],
      "description": "The deep hierarchical architecture of the mammalian brain is exquisitely suited to solving challenging problems such as vision and speech recognition. Deep architectures may in fact be required for sophisticated artificial intelligence tasks [1], however training them is hard [2], and has long limited the use of deep architectures, with the notable exception of convolutional networks [7]. In recent years, new initialization methods relying on greedy layer-by-layer pretraining [4, 8, 5, 9] have delivered promising results in image and speech recognition, object detection, handwritten digit recognition. However, many issues still need to be addressed. Our recent work has focused on three broad directions: 1) switching from patch-based to convolutional unsupervised pre-training, 2) investigating the benefits of multi-way local pooling, 3) exploring ways to combine information at multiple scales and complexity levels.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:WC9gN4BGCRcC",
      "title": "Strategies for Learning Hierarchies of Invariant Features",
      "link": "https://www.academia.edu/download/30788401/LeCun.pdf",
      "year": null,
      "cited_by": null,
      "authors": [
        "Yann LeCun",
        "Pierre Sermanet",
        "Y-Lan Boureau",
        "Clement Farabet",
        "Jason Rolfe",
        "Karol Gregor",
        "Arthur Szlam"
      ],
      "description": "Intelligent perceptual tasks such as vision and audition require the construction of good internal representations. The next big challenge for Computational Neuroscience, Computer Vision and Machine Learning is to devise learning procedures (biologically plausible and otherwise) that can learn features and internal representations automatically.Theoretical and empirical evidence suggest that the perceptual world is best represented by a multi-stage hierarchy in which features in successive stages are increasingly global, invariant, and abstract. An important question is to devise deep learning methods for multi-stage architecture than can automatically learn feature hierarchies from labeled and unlabeled data.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:9vf0nzSNQJEC",
      "title": "Invariant Feature Learning on a Mobile Robot",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.184.4745",
      "year": null,
      "cited_by": null,
      "authors": ["Raia Hadsell", "Yann LeCun"],
      "description": "Teaching a robot to perceive and navigate in an unstructured natural world is a difficult task. Without learning, navigation systems are short-range and extremely limited. With learning, the robot can be taught to classify terrain at longer distances, but these classifiers can be fragile as well, leading to extremely conservative planning. A robust, high-level learning-based perception system for a mobile robot needs to continually learn and adapt as it explores new environments. To do this, a strong feature representation is necessary that can encode meaningful, discriminative patterns as well as invariance to irrelevant transformations. A simple realtime classifier can then be trained on those features to predict the traversability of the current terrain. Much research has been done on feature extraction, and many different approaches are possible. We focus on learned representations with the belief that learning from real world data is preferable to hand-tuned features. Color histograms are fast to compute, but they are too limited. From a practical viewpoint, learned features are fast to compute, lower dimensional, and can be densely computed across an image. Learned features are efficient and precise, because they can capture patterns directly from the data. One such method for learning a feature representation is dimensionality reduction by learning an",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:l7t_Zn2s7bgC",
      "title": "Autonomous Learning for Long Range Vision in Mobile Robots",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.387.7330&rep=rep1&type=pdf",
      "year": null,
      "cited_by": null,
      "authors": [
        "Raia Hadsell",
        "Pierre Sermanet",
        "Ayse Erkan",
        "Koray Kavukcuoglu",
        "Urs Muller",
        "Yann LeCun"
      ],
      "description": "Designing a vision-based autonomous robot that can navigate through complicated outdoor environments is an extremely challenging problem that is far from solved. However, through use of a self-supervised realtime learning strategy and a trained deep belief net for compact feature representations, we have developed a long-range vision system that succeeds in accurately classifying obstacles and traversable areas that are 200 meters or more distant, bringing us closer to the goal of human-level autonomous driving.Motivation: Shortcomings of Stereo Vision The existing paradigm for vision-based mobile robots relies on hand-tuned heuristics: a stereo algorithm produces a (x, y, z) point cloud and traversability costs are assigned to points based on their proximity to a ground plane [2, 1]. However, stereo algorithms that run in realtime often produce costmaps that are short-range, sparse, and noisy. Our learning strategy uses these stereo labels for supervision to train a realtime classifier. The classifier then predicts the traversability of all visible areas, from close-range to the horizon. For accurate recognition of ground and obstacle categories, it is best to train on large, discriminative windows from the image, since larger windows give contextual information that is lacking in color and texture features. Other research has explored the use of online learning for mobile robots, but their methods have been largely restricted to simple color/texture correspondences [3, 4, 6].",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:u9iWguZQMMsC",
      "title": "End-to-End Learning of Object Categorization with Invariance Pose, Illumination, and Clutter.",
      "link": "https://scholar.google.com/scholar?cluster=3526274426012379533&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": ["Yann LeCun", "Fu Jie Huang"],
      "description": "We describe an end-to-end learning approaches to recognizing generic object categories with full invariance to pose, illumination, and clutter. The End-to-end learning approach consists in training the entire recognition system, from raw pixels to object categories, so as to minimize an overall discriminative performance measure. Many approaches to image recognition rely on expert knowledge or unsupervised learning to produce appropriate segmentors and feature extractors, while relying on supervised learning for the classifier. End-to-end approaches rely on expert knowledge to define the general architecture of the machine, but rely on learning at all levels in the system to produce suitable segmentors, feature extractors and classifiers [2].A large dataset comprising stereo image pairs of 50 uniform-coloredtoys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:OU6Ihb5iCvQC",
      "title": "A Study of Parallel Computing for Machine Learning: which Platform for which Application?",
      "link": "https://www.academia.edu/download/30788400/nyas2010.pdf",
      "year": null,
      "cited_by": null,
      "authors": ["Benoit Corda", "Clement Farabet", "Yann LeCun"],
      "description": "In recent years, advances in silicon technology has brought little improvement in the maximum clock frequency of processors, but has caused a doubling of the number of processor cores on a single chip every 18 months. Many applications of machine learning, particularly in computer vision, are often easily parallelizable and can take advantage of this recent trend. While the learning phase of a typical learning algorithm requires high-precision floating point arithmetics, and a highly flexible software environment, the utilization phase (after training) can often make use of considerably simpler hardware. In this study, we compare implementations of convolutional networks on three different computing platforms: traditional CPUs, Graphical Processing Units (GPUs), and Field-Programmable Gate Arrays (FPGA). Convolutional networks are trainable models specifically designed for detection and recognition in topologicallystructured signals such as images, audio, sonar, and other modalities. They are composed of multiple layers of linear local filter banks (convolutions), point-wise non-linear operations, and spatial pooling. Computations in convolutional networks are dominated by convolutions and dot-product operations. For general purpose CPUs, several machine learning libraries have been developed, which all provide efficient multi-dimensional arrays and modular description frameworks, this includes Lush [1], Torch [2] and Theano [3]. Essentially, all these libraries, and more generally all scientific packages (eg NumPy, Matlab,...) rely on compiled code to handle the most-repetitive operations, and provide a high-level, dynamically-typed\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:UxriW0iASnsC",
      "title": "Energy-Based methods for Simultaneous Localization and Mapping",
      "link": "https://scholar.google.com/scholar?cluster=2796243874513375540&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": ["Matthew Grimes", "Yann LeCun"],
      "description": "We present two energy-based approaches to robot localization, designed for use outdoors in a land-based autonomous robot. The first is a least-squares method for estimating the actual path of a robot given an erroneous path estimate with multiple discontinuities, as is typical in GPS output. The second method performs simultaneous localization and mapping (SLAM) by minimizing the sum of a few modular energy functions, each of which represents a competing concern such as fidelity to odometry or fidelity to the observations.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "url": "view_op=view_citation&hl=en&user=WLN3QrAAAAAJ&cstart=500&pagesize=100&citation_for_view=WLN3QrAAAAAJ:NhqRSupF_l8C",
      "title": "SPECIAL ISSUE ON MULTIMEDIA SIGNAL PROCESSING, PART",
      "link": "https://ieeexplore.ieee.org/iel3/5/14574/x0019656.pdf",
      "year": null,
      "cited_by": null,
      "authors": [
        "AM Tekalp",
        "RV Cox",
        "BG Haskell",
        "Y LeCun",
        "B Shahraray",
        "L Rabiner",
        "RR Rao",
        "VI Pavlovic",
        "TS Huang"
      ],
      "description": "Issue Image no(s) - Proceedings of the IEEE Page 1 SPECIAL ISSUE ON MULTIMEDIA \nSIGNAL PROCESSING, PART 1 Edited by Tsuhan Chen, KJ Ray Liu, and A. Murat Tekalp 751 \nScanning the Special Issue on Multimedia Signal Processing, Part I, T. Chen, KJR Liu, and AM \nTekalp SCANNING THE TECHNOLOGY 755 On the Applications of Multimedia Processing to \nCommunications (Invited Paper), RV Cox, BG Haskell, Y. LeCun, B. Shahraray, and L. Rabiner \nPAPERS 825 Toward the Creation of a New Medium for the Multimedia Era (Invited Paper), \nR. Nakatsu 837 Audio-Visual Integration in Multimodal Communication (Invited Paper), T. \nChen and RR Rao 853 Toward Multimodal Human\u2013Computer Interface (Invited Paper), R. \nSharma, VI Pavlovic, and TS Huang 870 Face to Virtual Face (Invited Paper), NM Thalmann, \nP. Kalra, and M. Escher 884 Next-Generation Content Representation, Creation, and \u2026",
      "citation_histogram": null,
      "detail_extracted": true
    }
  ],
  "all_publications_retrieved": true,
  "all_publications_extracted": true,
  "cstart": 600,
  "pagesize": 100
}
