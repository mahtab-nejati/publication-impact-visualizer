{
  "authorID": "JicYPdAAAAAJ",
  "name": "Geoffrey Hinton",
  "image_link": null,
  "interests": [
    "machine learning",
    "psychology",
    "artificial intelligence",
    "cognitive science",
    "computer science"
  ],
  "citations": 615335,
  "hindex": 172,
  "i10index": 411,
  "citation_histogram": [
    ["1988", "1747"],
    ["1989", "2314"],
    ["1990", "3050"],
    ["1991", "3312"],
    ["1992", "3666"],
    ["1993", "4147"],
    ["1994", "4041"],
    ["1995", "3804"],
    ["1996", "3634"],
    ["1997", "3487"],
    ["1998", "3530"],
    ["1999", "3351"],
    ["2000", "3083"],
    ["2001", "3133"],
    ["2002", "3489"],
    ["2003", "3536"],
    ["2004", "3327"],
    ["2005", "3604"],
    ["2006", "4002"],
    ["2007", "4342"],
    ["2008", "4141"],
    ["2009", "4511"],
    ["2010", "4955"],
    ["2011", "5447"],
    ["2012", "6371"],
    ["2013", "9202"],
    ["2014", "12237"],
    ["2015", "18556"],
    ["2016", "29727"],
    ["2017", "43110"],
    ["2018", "62780"],
    ["2019", "78899"],
    ["2020", "87422"],
    ["2021", "97659"],
    ["2022", "72486"]
  ],
  "coauthors": [
    ["m1qAiOUAAAAJ", "Terrence Sejnowski"],
    ["RnoIxUwAAAAJ", "Vinod Nair"],
    ["ghbWy", "George E. Dahl"],
    ["tJ_PrzgAAAAJ", "Abdelrahman Mohamed"],
    ["rr8pZoUAAAAJ", "Radford Neal"],
    ["bf7ilxMAAAAJ", "Sidney Fels"],
    ["rvKJDbIAAAAJ", "Chris Williams"],
    ["Rr847PIAAAAJ", "David C. Plaut"],
    ["ZpG_cJwAAAAJ", "Robert Tibshirani"],
    ["pKuBFaQAAAAJ", "Demetri Terzopoulos"],
    ["vO2T2c0AAAAJ", "Steven L. Small"]
  ],
  "publications": [
    {
      "title": "Imagenet classification with deep convolutional neural networks",
      "link": "https://dl.acm.org/doi/abs/10.1145/3065386",
      "year": 2017,
      "cited_by": 117783,
      "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"],
      "description": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and\u00a0\u2026",
      "citation_histogram": [
        [2013, 344],
        [2014, 1009],
        [2015, 2817],
        [2016, 6177],
        [2017, 10649],
        [2018, 16151],
        [2019, 20306],
        [2020, 21573],
        [2021, 22399],
        [2022, 14783]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep learning",
      "link": "https://www.nature.com/articles/nature14539",
      "year": 2015,
      "cited_by": 56586,
      "authors": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton"],
      "description": "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.",
      "citation_histogram": [
        [2015, 218],
        [2016, 1671],
        [2017, 4176],
        [2018, 7049],
        [2019, 9725],
        [2020, 11270],
        [2021, 12410],
        [2022, 9477]
      ],
      "detail_extracted": true
    },
    {
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "link": "https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer,",
      "year": 2014,
      "cited_by": 39148,
      "authors": [
        "Nitish Srivastava",
        "Geoffrey Hinton",
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Ruslan Salakhutdinov"
      ],
      "description": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \u201cthinned\u201d networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",
      "citation_histogram": [
        [2015, 476],
        [2016, 1609],
        [2017, 3332],
        [2018, 5633],
        [2019, 7077],
        [2020, 7371],
        [2021, 7672],
        [2022, 5569]
      ],
      "detail_extracted": true
    },
    {
      "title": "Visualizing data using t-SNE",
      "link": "https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf?fbcl",
      "year": 2008,
      "cited_by": 30971,
      "authors": ["Laurens van der Maaten", "Geoffrey Hinton"],
      "description": "We present a new technique called \u201ct-SNE\u201d that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.",
      "citation_histogram": [
        [2011, 94],
        [2012, 118],
        [2013, 152],
        [2014, 286],
        [2015, 559],
        [2016, 1055],
        [2017, 1862],
        [2018, 3110],
        [2019, 4487],
        [2020, 5540],
        [2021, 7204],
        [2022, 6143]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning representations by back-propagating errors",
      "link": "https://www.nature.com/articles/323533a0",
      "year": 1986,
      "cited_by": 30846,
      "authors": [
        "David E Rumelhart",
        "Geoffrey E Hinton",
        "Ronald J Williams"
      ],
      "description": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.",
      "citation_histogram": [
        [1989, 109],
        [1990, 178],
        [1991, 221],
        [1992, 263],
        [1993, 286],
        [1994, 325],
        [1995, 360],
        [1996, 318],
        [1997, 296],
        [1998, 317],
        [1999, 320],
        [2000, 345],
        [2001, 340],
        [2002, 345],
        [2003, 359],
        [2004, 415],
        [2005, 429],
        [2006, 496],
        [2007, 517],
        [2008, 516],
        [2009, 538],
        [2010, 566],
        [2011, 564],
        [2012, 612],
        [2013, 811],
        [2014, 889],
        [2015, 1148],
        [2016, 1498],
        [2017, 1816],
        [2018, 2403],
        [2019, 2966],
        [2020, 3445],
        [2021, 3706],
        [2022, 2738]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning internal representations by error-propagation",
      "link": "https://apps.dtic.mil/sti/citations/ADA164453",
      "year": 1986,
      "cited_by": 30350,
      "authors": [
        "David E Rumelhart",
        "Geoffrey E Hinton",
        "Ronald J Williams"
      ],
      "description": "This paper presents a generalization of the perception learning procedure for learning the correct sets of connections for arbitrary networks. The rule, falled the generalized delta rule, is a simple scheme for implementing a gradient descent method for finding weights that minimize the sum squared error of the sytems performance. The major theoretical contribution of the work is the procedure called error propagation, whereby the gradient can be determined by individual units of the network based only on locally available information. The major empirical contribution of the work is to show that the problem of local minima not serious in this application of gradient descent. Keywords Learning networks Perceptrons Adaptive systems Learning machines and Back propagation.Descriptors:",
      "citation_histogram": [
        [1987, 85],
        [1988, 213],
        [1989, 474],
        [1990, 821],
        [1991, 902],
        [1992, 1094],
        [1993, 1199],
        [1994, 1208],
        [1995, 1123],
        [1996, 985],
        [1997, 911],
        [1998, 862],
        [1999, 828],
        [2000, 745],
        [2001, 701],
        [2002, 756],
        [2003, 775],
        [2004, 724],
        [2005, 766],
        [2006, 798],
        [2007, 805],
        [2008, 755],
        [2009, 806],
        [2010, 681],
        [2011, 769],
        [2012, 769],
        [2013, 731],
        [2014, 662],
        [2015, 756],
        [2016, 844],
        [2017, 835],
        [2018, 991],
        [2019, 1131],
        [2020, 1143],
        [2021, 1369],
        [2022, 874]
      ],
      "detail_extracted": true
    },
    {
      "title": "Schemata and sequential thought processes in PDP models.",
      "link": null,
      "year": 1986,
      "cited_by": 26719,
      "authors": [
        "David Rumelhart",
        "P Smolenksy",
        "James McClelland",
        "Geoffrey Hinton"
      ],
      "description": null,
      "citation_histogram": [
        [1987, 179],
        [1988, 423],
        [1989, 582],
        [1990, 683],
        [1991, 910],
        [1992, 981],
        [1993, 1166],
        [1994, 1049],
        [1995, 937],
        [1996, 927],
        [1997, 855],
        [1998, 871],
        [1999, 766],
        [2000, 683],
        [2001, 696],
        [2002, 820],
        [2003, 723],
        [2004, 658],
        [2005, 693],
        [2006, 713],
        [2007, 793],
        [2008, 747],
        [2009, 733],
        [2010, 710],
        [2011, 674],
        [2012, 769],
        [2013, 907],
        [2014, 734],
        [2015, 582],
        [2016, 691],
        [2017, 670],
        [2018, 689],
        [2019, 688],
        [2020, 626],
        [2021, 501],
        [2022, 326]
      ],
      "detail_extracted": true
    },
    {
      "title": "Reducing the dimensionality of data with neural networks",
      "link": "https://www.science.org/doi/abs/10.1126/science.1127647",
      "year": 2006,
      "cited_by": 18806,
      "authors": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"],
      "description": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \u201cautoencoder\u201d networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.",
      "citation_histogram": [
        [2007, 82],
        [2008, 84],
        [2009, 120],
        [2010, 216],
        [2011, 206],
        [2012, 267],
        [2013, 483],
        [2014, 733],
        [2015, 1197],
        [2016, 1599],
        [2017, 1844],
        [2018, 2267],
        [2019, 2562],
        [2020, 2571],
        [2021, 2543],
        [2022, 1839]
      ],
      "detail_extracted": true
    },
    {
      "title": "Rectified linear units improve restricted boltzmann machines",
      "link": "https://openreview.net/forum?id=rkb15iZdZB",
      "year": 2010,
      "cited_by": 18424,
      "authors": ["Vinod Nair", "Geoffrey E Hinton"],
      "description": "Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these\" Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.",
      "citation_histogram": [
        [2013, 76],
        [2014, 150],
        [2015, 365],
        [2016, 767],
        [2017, 1505],
        [2018, 2461],
        [2019, 3274],
        [2020, 3551],
        [2021, 3629],
        [2022, 2452]
      ],
      "detail_extracted": true
    },
    {
      "title": "A fast learning algorithm for deep belief nets",
      "link": "https://direct.mit.edu/neco/article-abstract/18/7/1527/7065",
      "year": 2006,
      "cited_by": 18279,
      "authors": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"],
      "description": " We show how to use \u201ccomplementary priors\u201d to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level\u00a0\u2026",
      "citation_histogram": [
        [2007, 65],
        [2008, 78],
        [2009, 122],
        [2010, 207],
        [2011, 251],
        [2012, 308],
        [2013, 601],
        [2014, 986],
        [2015, 1458],
        [2016, 1910],
        [2017, 2012],
        [2018, 2321],
        [2019, 2223],
        [2020, 2186],
        [2021, 2015],
        [2022, 1310]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning multiple layers of features from tiny images",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.222.9220&rep=rep1&type=pdf",
      "year": 2009,
      "cited_by": 18052,
      "authors": ["Alex Krizhevsky", "Geoffrey Hinton"],
      "description": "In this work we describe how to train a multi-layer generative model of natural images. We use a dataset of millions of tiny colour images, described in the next section. This has been attempted by several groups but without success [3, 7]. The models on which we focus are RBMs (Restricted Boltzmann Machines) and DBNs (Deep Belief Networks). These models learn interesting-looking filters, which we show are more useful to a classifier than the raw pixels. We train the classifier on a labeled subset that we have collected and call the CIFAR-10 dataset.",
      "citation_histogram": [
        [2011, 45],
        [2012, 66],
        [2013, 107],
        [2014, 172],
        [2015, 315],
        [2016, 488],
        [2017, 786],
        [2018, 1528],
        [2019, 2760],
        [2020, 3522],
        [2021, 4501],
        [2022, 3609]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "link": "https://ieeexplore.ieee.org/abstract/document/6296526/",
      "year": 2012,
      "cited_by": 11712,
      "authors": [
        "Geoffrey Hinton",
        "Li Deng",
        "Dong Yu",
        "George E Dahl",
        "Abdel-rahman Mohamed",
        "Navdeep Jaitly",
        "Andrew Senior",
        "Vincent Vanhoucke",
        "Patrick Nguyen",
        "Tara N Sainath",
        "Brian Kingsbury"
      ],
      "description": "Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.",
      "citation_histogram": [
        [2013, 203],
        [2014, 405],
        [2015, 756],
        [2016, 1113],
        [2017, 1345],
        [2018, 1776],
        [2019, 1925],
        [2020, 1730],
        [2021, 1452],
        [2022, 885]
      ],
      "detail_extracted": true
    },
    {
      "title": "Distilling the knowledge in a neural network",
      "link": "https://openaccess.thecvf.com/content/WACV2022/supplemental/Jeong_BiHPF_Bilateral_High-Pass_WACV_2022_supplemental.zip",
      "year": 2015,
      "cited_by": 11400,
      "authors": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"],
      "description": "@article{temperaturesoftmax, title={Distilling the knowledge in a neural network}, author={Hinton, \nGeoffrey and Vinyals, Oriol and Dean, Jeff}, journal={arXiv}, year={2015} } @inproceedings{srgan, \ntitle={Photo-realistic single image super-resolution using a generative adversarial network}, \nauthor={Ledig, Christian and Theis, Lucas and Husz{\\'a}r, Ferenc and Caballero, Jose and \nCunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and \nTotz, Johannes and Wang, Zehan and others}, booktitle={CVPR}, year={2017} } @inproceedings{stargan2, \ntitle={StarGAN v2: Diverse Image Synthesis for Multiple Domains}, author={Yunjey Choi and \nYoungjung Uh and Jaejun Yoo and Jung-Woo Ha}, booktitle={CVPR}, year={2020} } @inproceedings{stylegan, \ntitle={A style-based generator architecture for generative adversarial networks}, author={Karras, \nTero and Laine, Samuli and Aila, Timo}, \u2026",
      "citation_histogram": [
        [2015, 65],
        [2016, 197],
        [2017, 363],
        [2018, 850],
        [2019, 1390],
        [2020, 2337],
        [2021, 3326],
        [2022, 2795]
      ],
      "detail_extracted": true
    },
    {
      "title": "Speech recognition with deep recurrent neural networks",
      "link": "https://ieeexplore.ieee.org/abstract/document/6638947/",
      "year": 2013,
      "cited_by": 9320,
      "authors": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"],
      "description": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve\u00a0\u2026",
      "citation_histogram": [
        [2013, 25],
        [2014, 110],
        [2015, 268],
        [2016, 600],
        [2017, 978],
        [2018, 1434],
        [2019, 1741],
        [2020, 1646],
        [2021, 1544],
        [2022, 878]
      ],
      "detail_extracted": true
    },
    {
      "title": "Improving neural networks by preventing co-adaptation of feature detectors",
      "link": "https://arxiv.org/abs/1207.0580",
      "year": 2012,
      "cited_by": 8277,
      "authors": [
        "Geoffrey E Hinton",
        "Nitish Srivastava",
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Ruslan R Salakhutdinov"
      ],
      "description": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",
      "citation_histogram": [
        [2013, 142],
        [2014, 342],
        [2015, 591],
        [2016, 724],
        [2017, 867],
        [2018, 1115],
        [2019, 1211],
        [2020, 1185],
        [2021, 1189],
        [2022, 790]
      ],
      "detail_extracted": true
    },
    {
      "title": "A simple framework for contrastive learning of visual representations",
      "link": "http://proceedings.mlr.press/v119/chen20j.html",
      "year": 2020,
      "cited_by": 6547,
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "description": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks,(2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
      "citation_histogram": [
        [2020, 529],
        [2021, 2540],
        [2022, 3425]
      ],
      "detail_extracted": true
    },
    {
      "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude",
      "link": "https://scholar.google.com/scholar?cluster=14955450492433009149&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 6394,
      "authors": ["Tijmen Tieleman", "Geoffrey Hinton"],
      "description": null,
      "citation_histogram": [
        [2014, 18],
        [2015, 91],
        [2016, 343],
        [2017, 703],
        [2018, 1033],
        [2019, 1234],
        [2020, 1215],
        [2021, 1028],
        [2022, 663]
      ],
      "detail_extracted": true
    },
    {
      "title": "Layer normalization",
      "link": "https://arxiv.org/abs/1607.06450",
      "year": 2016,
      "cited_by": 6252,
      "authors": ["Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton"],
      "description": "Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",
      "citation_histogram": [
        [2016, 31],
        [2017, 185],
        [2018, 439],
        [2019, 803],
        [2020, 1199],
        [2021, 1773],
        [2022, 1765]
      ],
      "detail_extracted": true
    },
    {
      "title": "Training products of experts by minimizing contrastive divergence",
      "link": "https://ieeexplore.ieee.org/abstract/document/6789337/",
      "year": 2002,
      "cited_by": 5654,
      "authors": ["Geoffrey E Hinton"],
      "description": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual \u201cexpert\u201d models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called \u201ccontrastive divergence\u201d whose derivatives with regard to the\u00a0\u2026",
      "citation_histogram": [
        [2001, 14],
        [2002, 16],
        [2003, 34],
        [2004, 28],
        [2005, 39],
        [2006, 46],
        [2007, 74],
        [2008, 75],
        [2009, 117],
        [2010, 145],
        [2011, 159],
        [2012, 192],
        [2013, 301],
        [2014, 402],
        [2015, 516],
        [2016, 591],
        [2017, 528],
        [2018, 544],
        [2019, 540],
        [2020, 495],
        [2021, 455],
        [2022, 286]
      ],
      "detail_extracted": true
    },
    {
      "title": "Adaptive mixtures of local experts",
      "link": "https://ieeexplore.ieee.org/abstract/document/6797059/",
      "year": 1991,
      "cited_by": 5111,
      "authors": [
        "Robert A Jacobs",
        "Michael I Jordan",
        "Steven J Nowlan",
        "Geoffrey E Hinton"
      ],
      "description": "We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.",
      "citation_histogram": [
        [1991, 17],
        [1992, 22],
        [1993, 52],
        [1994, 84],
        [1995, 113],
        [1996, 121],
        [1997, 158],
        [1998, 139],
        [1999, 147],
        [2000, 138],
        [2001, 143],
        [2002, 161],
        [2003, 175],
        [2004, 137],
        [2005, 168],
        [2006, 185],
        [2007, 191],
        [2008, 176],
        [2009, 186],
        [2010, 179],
        [2011, 179],
        [2012, 154],
        [2013, 163],
        [2014, 153],
        [2015, 120],
        [2016, 173],
        [2017, 152],
        [2018, 202],
        [2019, 247],
        [2020, 240],
        [2021, 329],
        [2022, 256]
      ],
      "detail_extracted": true
    },
    {
      "title": "A learning algorithm for Boltzmann machines",
      "link": "https://www.sciencedirect.com/science/article/pii/S0364021385800124",
      "year": 1985,
      "cited_by": 4994,
      "authors": [
        "David H Ackley",
        "Geoffrey E Hinton",
        "Terrence J Sejnowski"
      ],
      "description": "The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general\u00a0\u2026",
      "citation_histogram": [
        [1985, 32],
        [1986, 75],
        [1987, 111],
        [1988, 143],
        [1989, 110],
        [1990, 158],
        [1991, 146],
        [1992, 151],
        [1993, 124],
        [1994, 111],
        [1995, 115],
        [1996, 75],
        [1997, 83],
        [1998, 77],
        [1999, 80],
        [2000, 62],
        [2001, 66],
        [2002, 66],
        [2003, 90],
        [2004, 54],
        [2005, 65],
        [2006, 57],
        [2007, 79],
        [2008, 82],
        [2009, 89],
        [2010, 98],
        [2011, 108],
        [2012, 152],
        [2013, 161],
        [2014, 164],
        [2015, 162],
        [2016, 215],
        [2017, 220],
        [2018, 260],
        [2019, 290],
        [2020, 281],
        [2021, 298],
        [2022, 215]
      ],
      "detail_extracted": true
    },
    {
      "title": "On the importance of initialization and momentum in deep learning",
      "link": "http://proceedings.mlr.press/v28/sutskever13.html",
      "year": 2013,
      "cited_by": 4821,
      "authors": [
        "Ilya Sutskever",
        "James Martens",
        "George Dahl",
        "Geoffrey Hinton"
      ],
      "description": "Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.",
      "citation_histogram": [
        [2014, 58],
        [2015, 172],
        [2016, 325],
        [2017, 411],
        [2018, 634],
        [2019, 798],
        [2020, 837],
        [2021, 912],
        [2022, 625]
      ],
      "detail_extracted": true
    },
    {
      "title": "Dynamic routing between capsules",
      "link": "https://proceedings.neurips.cc/paper/2017/hash/2cad8fa47bbef282badbb8de5374b894-Abstract.html",
      "year": 2017,
      "cited_by": 4124,
      "authors": ["Sara Sabour", "Nicholas Frosst", "Geoffrey E Hinton"],
      "description": "A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.",
      "citation_histogram": [
        [2017, 19],
        [2018, 444],
        [2019, 774],
        [2020, 883],
        [2021, 1136],
        [2022, 842]
      ],
      "detail_extracted": true
    },
    {
      "title": "Phoneme recognition using time-delay neural networks",
      "link": "https://ieeexplore.ieee.org/abstract/document/21701/",
      "year": 1989,
      "cited_by": 3762,
      "authors": [
        "Alexander Waibel",
        "Toshiyuki Hanazawa",
        "Geoffrey Hinton",
        "Kiyohiro Shikano",
        "Kevin J Lang"
      ],
      "description": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the\u00a0\u2026",
      "citation_histogram": [
        [1988, 11],
        [1989, 29],
        [1990, 95],
        [1991, 118],
        [1992, 126],
        [1993, 122],
        [1994, 148],
        [1995, 113],
        [1996, 129],
        [1997, 89],
        [1998, 103],
        [1999, 82],
        [2000, 70],
        [2001, 59],
        [2002, 85],
        [2003, 68],
        [2004, 55],
        [2005, 83],
        [2006, 71],
        [2007, 87],
        [2008, 64],
        [2009, 73],
        [2010, 66],
        [2011, 72],
        [2012, 71],
        [2013, 76],
        [2014, 100],
        [2015, 97],
        [2016, 117],
        [2017, 134],
        [2018, 210],
        [2019, 234],
        [2020, 240],
        [2021, 241],
        [2022, 175]
      ],
      "detail_extracted": true
    },
    {
      "title": "A practical guide to training restricted Boltzmann machines",
      "link": "https://link.springer.com/chapter/10.1007/978-3-642-35289-8_32",
      "year": 2010,
      "cited_by": 3731,
      "authors": ["Geoffrey Hinton"],
      "description": " Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters. Over the last few years, the machine learning group at the University of Toronto has acquired considerable expertise at training RBMs and this guide is an attempt to share this expertise with other machine learning researchers.",
      "citation_histogram": [
        [2011, 38],
        [2012, 72],
        [2013, 138],
        [2014, 248],
        [2015, 416],
        [2016, 448],
        [2017, 462],
        [2018, 458],
        [2019, 415],
        [2020, 433],
        [2021, 350],
        [2022, 193]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep Boltzmann machines",
      "link": null,
      "year": 2009,
      "cited_by": 3487,
      "authors": ["Ruslan Salakhutdinov", "Geoffrey Hinton"],
      "description": null,
      "citation_histogram": [
        [2010, 31],
        [2011, 50],
        [2012, 60],
        [2013, 144],
        [2014, 176],
        [2015, 294],
        [2016, 372],
        [2017, 383],
        [2018, 465],
        [2019, 429],
        [2020, 402],
        [2021, 394],
        [2022, 227]
      ],
      "detail_extracted": true
    },
    {
      "title": "A view of the EM algorithm that justifies incremental, sparse, and other variants",
      "link": "https://link.springer.com/chapter/10.1007/978-94-011-5014-9_12",
      "year": 1998,
      "cited_by": 3406,
      "authors": ["Radford M Neal", "Geoffrey E Hinton"],
      "description": " The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.",
      "citation_histogram": [
        [1997, 28],
        [1998, 53],
        [1999, 51],
        [2000, 43],
        [2001, 71],
        [2002, 63],
        [2003, 102],
        [2004, 120],
        [2005, 118],
        [2006, 141],
        [2007, 157],
        [2008, 141],
        [2009, 156],
        [2010, 158],
        [2011, 141],
        [2012, 166],
        [2013, 189],
        [2014, 177],
        [2015, 185],
        [2016, 187],
        [2017, 168],
        [2018, 145],
        [2019, 141],
        [2020, 149],
        [2021, 148],
        [2022, 122]
      ],
      "detail_extracted": true
    },
    {
      "title": "Connectionist learning procedures",
      "link": "https://www.sciencedirect.com/science/article/pii/B9780080510552500298",
      "year": 1990,
      "cited_by": 2345,
      "authors": ["Geoffrey E Hinton"],
      "description": "A major goal of research on networks of neuronlike processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units that are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks, and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can\u00a0\u2026",
      "citation_histogram": [
        [1989, 39],
        [1990, 110],
        [1991, 84],
        [1992, 99],
        [1993, 118],
        [1994, 113],
        [1995, 79],
        [1996, 82],
        [1997, 70],
        [1998, 58],
        [1999, 54],
        [2000, 63],
        [2001, 39],
        [2002, 64],
        [2003, 44],
        [2004, 39],
        [2005, 39],
        [2006, 46],
        [2007, 45],
        [2008, 43],
        [2009, 39],
        [2010, 33],
        [2011, 50],
        [2012, 54],
        [2013, 63],
        [2014, 58],
        [2015, 47],
        [2016, 53],
        [2017, 60],
        [2018, 65],
        [2019, 90],
        [2020, 119],
        [2021, 123],
        [2022, 102]
      ],
      "detail_extracted": true
    },
    {
      "title": "Neighbourhood components analysis",
      "link": "https://proceedings.neurips.cc/paper/2004/hash/42fe880812925e520249e808937738d2-Abstract.html",
      "year": 2004,
      "cited_by": 2344,
      "authors": [
        "Jacob Goldberger",
        "Geoffrey E Hinton",
        "Sam Roweis",
        "Russ R Salakhutdinov"
      ],
      "description": "In this paper we propose a novel method for learning a Mahalanobis distance measure to be used in the KNN classification algorithm. The algorithm directly maximizes a stochastic variant of the leave-one-out KNN score on the training set. It can also learn a low-dimensional lin-ear embedding of labeled data that can be used for data visualization and fast classification. Unlike other methods, our classification model is non-parametric, making no assumptions about the shape of the class distributions or the boundaries between them. The performance of the method is demonstrated on several data sets, both for metric learning and linear dimensionality reduction.",
      "citation_histogram": [
        [2005, 10],
        [2006, 23],
        [2007, 60],
        [2008, 79],
        [2009, 82],
        [2010, 112],
        [2011, 94],
        [2012, 138],
        [2013, 126],
        [2014, 128],
        [2015, 169],
        [2016, 137],
        [2017, 137],
        [2018, 146],
        [2019, 163],
        [2020, 229],
        [2021, 263],
        [2022, 221]
      ],
      "detail_extracted": true
    },
    {
      "title": "Restricted Boltzmann machines for collaborative filtering",
      "link": "https://dl.acm.org/doi/abs/10.1145/1273496.1273596",
      "year": 2007,
      "cited_by": 2314,
      "authors": ["Ruslan Salakhutdinov", "Andriy Mnih", "Geoffrey Hinton"],
      "description": "Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.",
      "citation_histogram": [
        [2007, 16],
        [2008, 36],
        [2009, 38],
        [2010, 62],
        [2011, 47],
        [2012, 72],
        [2013, 75],
        [2014, 122],
        [2015, 152],
        [2016, 207],
        [2017, 272],
        [2018, 279],
        [2019, 294],
        [2020, 258],
        [2021, 223],
        [2022, 139]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning and relearning in Boltzmann machines",
      "link": "http://www.cs.utoronto.ca/~hinton/absps/pdp7.pdf",
      "year": 1986,
      "cited_by": 2190,
      "authors": ["Geoffrey E Hinton", "Terrance J Sejnowski"],
      "description": "Many of the chapters in this volume make use of the ability of a parallel network to perform cooperative searches for good solutions to problems. The basic idea is simple: The weights on the connections between processing units encode knowledge about how things normally fit together in some domain and the initial states or external inputs to a subset of the units encode some fragments of a structure within the domain. These fragments constitute a problem: What is the whole structure from which they probably came? The network computes a\" good solution\" to the problem by repeatedly updating the states of units that represent possible other parts of the structure until the network eventually settles into a stable state of activity that represents the solution.-.. One field in which this style of computation seems particularly appropriate is vision (Ballard, Hinton, & Sejnowski, 1983). A visual system must be able to solve\u00a0\u2026",
      "citation_histogram": [
        [1987, 15],
        [1988, 40],
        [1989, 89],
        [1990, 82],
        [1991, 84],
        [1992, 98],
        [1993, 96],
        [1994, 73],
        [1995, 56],
        [1996, 52],
        [1997, 50],
        [1998, 51],
        [1999, 50],
        [2000, 45],
        [2001, 38],
        [2002, 56],
        [2003, 64],
        [2004, 43],
        [2005, 70],
        [2006, 46],
        [2007, 58],
        [2008, 39],
        [2009, 41],
        [2010, 33],
        [2011, 26],
        [2012, 32],
        [2013, 52],
        [2014, 57],
        [2015, 61],
        [2016, 111],
        [2017, 75],
        [2018, 81],
        [2019, 81],
        [2020, 77],
        [2021, 77],
        [2022, 42]
      ],
      "detail_extracted": true
    },
    {
      "title": "Distributed representations",
      "link": "https://kilthub.cmu.edu/articles/journal_contribution/Distributed_representations/6604925/files/12095348.pdf",
      "year": 1986,
      "cited_by": 2168,
      "authors": [
        "Geoffrey E Hinton",
        "James L McClelland",
        "David E Rumelhart"
      ],
      "description": "Given a^ network of simple computing elements and some entities to be represented, the most straightforward scheme is to use one computing element for each entity. This is called a local representation. It is easy to understand and easy to implement because the structure of the physical network mirrors the structure of the knowledge it contains. This report describes a different type of representation that is less familiar and harder to think about than local representations. Each entity is represented by a pattern of activity distributed over many computing elements, and each computing element is involved in representing many different entities. The strength of this more complicated kind of representation does not lie in its notational convenience or its ease of implementation in a conventional computer, but rather in the efficiency with which it makes use of the processing abilities of networks of simple, neuron-like computing elements.Every representational scheme has its good and bad points. Distributed representations are no exception. Some desirable properties like content-addressable memory and automatic generalization arise very naturally from the use of patterns of activity as representations. Other properties, like the ability to temporarily store a large set of arbitrary associations, are much harder to achieve. The best psychological evidence for distributed representations is the degree to which their strengths and weaknesses match those of the human mind.",
      "citation_histogram": [
        [1985, 6],
        [1986, 8],
        [1987, 57],
        [1988, 31],
        [1989, 70],
        [1990, 110],
        [1991, 96],
        [1992, 56],
        [1993, 73],
        [1994, 92],
        [1995, 48],
        [1996, 45],
        [1997, 60],
        [1998, 60],
        [1999, 42],
        [2000, 45],
        [2001, 43],
        [2002, 49],
        [2003, 78],
        [2004, 29],
        [2005, 41],
        [2006, 40],
        [2007, 35],
        [2008, 59],
        [2009, 31],
        [2010, 27],
        [2011, 33],
        [2012, 48],
        [2013, 59],
        [2014, 60],
        [2015, 56],
        [2016, 70],
        [2017, 74],
        [2018, 98],
        [2019, 94],
        [2020, 91],
        [2021, 76],
        [2022, 45]
      ],
      "detail_extracted": true
    },
    {
      "title": "Acoustic modeling using deep belief networks",
      "link": "https://ieeexplore.ieee.org/abstract/document/5704567/",
      "year": 2012,
      "cited_by": 2042,
      "authors": ["Abdel-rahman Mohamed", "G Dahl", "Geoffrey Hinton"],
      "description": "Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features, we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models.",
      "citation_histogram": [
        [2011, 15],
        [2012, 58],
        [2013, 130],
        [2014, 208],
        [2015, 253],
        [2016, 241],
        [2017, 235],
        [2018, 262],
        [2019, 224],
        [2020, 179],
        [2021, 149],
        [2022, 78]
      ],
      "detail_extracted": true
    },
    {
      "title": "Generating text with recurrent neural networks",
      "link": "https://openreview.net/forum?id=SyEoB2-dZH",
      "year": 2011,
      "cited_by": 1712,
      "authors": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"],
      "description": "Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modeling tasks. The standard RNN architecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or\" gated\") connections which allow the current input character to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character-level language modeling\u2013a hierarchical non-parametric sequence model. To our knowledge this represents the largest recurrent neural network application to date.",
      "citation_histogram": [
        [2012, 17],
        [2013, 32],
        [2014, 50],
        [2015, 97],
        [2016, 185],
        [2017, 223],
        [2018, 237],
        [2019, 234],
        [2020, 267],
        [2021, 213],
        [2022, 138]
      ],
      "detail_extracted": true
    },
    {
      "title": "Improving deep neural networks for LVCSR using rectified linear units and dropout",
      "link": "https://ieeexplore.ieee.org/abstract/document/6639346/",
      "year": 2013,
      "cited_by": 1649,
      "authors": ["George E Dahl", "Tara N Sainath", "Geoffrey E Hinton"],
      "description": "Recently, pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random \u201cdropout\u201d procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid over-fitting, it has also been successful on a small-scale phone recognition task using larger neural nets. However, training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectified linear unit (ReLU) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units, sometimes\u00a0\u2026",
      "citation_histogram": [
        [2013, 14],
        [2014, 77],
        [2015, 126],
        [2016, 163],
        [2017, 168],
        [2018, 208],
        [2019, 238],
        [2020, 262],
        [2021, 246],
        [2022, 135]
      ],
      "detail_extracted": true
    },
    {
      "title": "How learning can guide evolution",
      "link": "https://icts.res.in/sites/default/files/How%20Learning%20Can%20Guide%20Evolution%20-1987.pdf",
      "year": 1987,
      "cited_by": 1633,
      "authors": ["Geoffrey E Hinton", "Steven J Nowlan"],
      "description": "The assumption that acquired characteristics are not in-herited is often taken to imply that the adaptations that an organism learns during its lifetime cannot guide the course of evolution. This inference is incorrect [2]. Learning alters the shape of the search space in which evolution operates and thereby provides good evolutionary paths towards sets of co-adapted alleles. We demonstrate that this effect allows learning organisms to evolve much faster than their 000-learning equivalents, even though the characteristics acquired by the phenotype are not communicated to the genotype.",
      "citation_histogram": [
        [1990, 65],
        [1991, 7],
        [1992, 9],
        [1993, 20],
        [1994, 29],
        [1995, 30],
        [1996, 32],
        [1997, 41],
        [1998, 38],
        [1999, 62],
        [2000, 52],
        [2001, 89],
        [2002, 56],
        [2003, 73],
        [2004, 63],
        [2005, 72],
        [2006, 94],
        [2007, 88],
        [2008, 46],
        [2009, 65],
        [2010, 45],
        [2011, 45],
        [2012, 51],
        [2013, 53],
        [2014, 60],
        [2015, 39],
        [2016, 35],
        [2017, 49],
        [2018, 41],
        [2019, 61],
        [2020, 40],
        [2021, 33],
        [2022, 21]
      ],
      "detail_extracted": true
    },
    {
      "title": "Semantic hashing",
      "link": "https://www.sciencedirect.com/science/article/pii/S0888613X08001813",
      "year": 2007,
      "cited_by": 1603,
      "authors": ["Ruslan Salakhutdinov", "Geoffrey Hinton"],
      "description": "We show how to learn a deep graphical model of the word-count vectors obtained from a large set of documents. The values of the latent variables in the deepest layer are easy to infer and give a much better representation of each document than Latent Semantic Analysis. When the deepest layer is forced to use a small number of binary variables (e.g. 32), the graphical model performs \u201csemantic hashing\u201d: Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing, which is the fastest current method. By using semantic hashing to filter the documents given to\u00a0\u2026",
      "citation_histogram": [
        [2008, 13],
        [2009, 29],
        [2010, 36],
        [2011, 57],
        [2012, 75],
        [2013, 99],
        [2014, 134],
        [2015, 161],
        [2016, 156],
        [2017, 181],
        [2018, 173],
        [2019, 144],
        [2020, 140],
        [2021, 103],
        [2022, 79]
      ],
      "detail_extracted": true
    },
    {
      "title": "The CIFAR-10 dataset",
      "link": "https://scholar.google.com/scholar?cluster=17210469098535632353&hl=en&oi=scholarr",
      "year": 2014,
      "cited_by": 1576,
      "authors": ["Alex Krizhevsky", "Vinod Nair", "Geoffrey Hinton"],
      "description": null,
      "citation_histogram": [
        [2014, 5],
        [2015, 6],
        [2016, 13],
        [2017, 41],
        [2018, 145],
        [2019, 288],
        [2020, 377],
        [2021, 414],
        [2022, 283]
      ],
      "detail_extracted": true
    },
    {
      "title": "A general framework for parallel distributed processing",
      "link": "http://web.stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap2_PDP86.pdf",
      "year": 1986,
      "cited_by": 1564,
      "authors": [
        "David E Rumelhart",
        "Geoffrey E Hinton",
        "James L McClelland"
      ],
      "description": "In Chapter 1 and throughout this book, we describe a large number of models, each different in detail\u2013each a variation on the parallel dis-tributed processing (PDP) idea. These various models, and indeed many in the literature, clearly have many features in common, but they are just as clearly distinct models. How can we characterize the general model of which these specific models are instances? In this chapter we propose a framework sufficiently general so that all of the various models discussed in the book and many models in the literature are special cases. We will proceed by first sketching the general framework and then by showing properties of certain specific realizations of the general model.\"",
      "citation_histogram": [
        [1987, 9],
        [1988, 19],
        [1989, 49],
        [1990, 57],
        [1991, 59],
        [1992, 62],
        [1993, 60],
        [1994, 42],
        [1995, 49],
        [1996, 44],
        [1997, 42],
        [1998, 37],
        [1999, 39],
        [2000, 29],
        [2001, 25],
        [2002, 36],
        [2003, 32],
        [2004, 37],
        [2005, 55],
        [2006, 29],
        [2007, 43],
        [2008, 28],
        [2009, 34],
        [2010, 40],
        [2011, 28],
        [2012, 48],
        [2013, 46],
        [2014, 41],
        [2015, 40],
        [2016, 35],
        [2017, 52],
        [2018, 64],
        [2019, 59],
        [2020, 59],
        [2021, 60],
        [2022, 46]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parallel models of associative memory",
      "link": null,
      "year": 1981,
      "cited_by": 1527,
      "authors": ["Geoffrey E Hinton", "James A. Anderson"],
      "description": null,
      "citation_histogram": [
        [1983, 13],
        [1984, 77],
        [1985, 76],
        [1986, 77],
        [1987, 140],
        [1988, 129],
        [1989, 92],
        [1990, 82],
        [1991, 63],
        [1992, 46],
        [1993, 82],
        [1994, 23],
        [1995, 22],
        [1996, 60],
        [1997, 26],
        [1998, 23],
        [1999, 23],
        [2000, 25],
        [2001, 19],
        [2002, 17],
        [2003, 22],
        [2004, 12],
        [2005, 14],
        [2006, 21],
        [2007, 18],
        [2008, 21],
        [2009, 14],
        [2010, 18],
        [2011, 24],
        [2012, 22],
        [2013, 40],
        [2014, 24],
        [2015, 10],
        [2016, 19],
        [2017, 15],
        [2018, 12],
        [2019, 16],
        [2020, 22],
        [2021, 13],
        [2022, 10]
      ],
      "detail_extracted": true
    },
    {
      "title": "Stochastic neighbor embedding",
      "link": "https://proceedings.neurips.cc/paper/2002/hash/6150ccc6069bea6b5716254057a194ef-Abstract.html",
      "year": 2002,
      "cited_by": 1524,
      "authors": ["Geoffrey E Hinton", "Sam Roweis"],
      "description": "We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional \u201cimages\u201d of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word \u201cbank\u201d, to have versions close to the images of both \u201criver\u201d and \u201cfinance\u201d without forcing the images of outdoor concepts to be located close to those of corporate concepts.",
      "citation_histogram": [
        [2003, 5],
        [2004, 18],
        [2005, 22],
        [2006, 22],
        [2007, 22],
        [2008, 24],
        [2009, 45],
        [2010, 32],
        [2011, 51],
        [2012, 49],
        [2013, 72],
        [2014, 75],
        [2015, 91],
        [2016, 111],
        [2017, 119],
        [2018, 104],
        [2019, 95],
        [2020, 96],
        [2021, 229],
        [2022, 232]
      ],
      "detail_extracted": true
    },
    {
      "title": "The helmholtz machine",
      "link": "https://direct.mit.edu/neco/article-abstract/7/5/889/5898",
      "year": 1995,
      "cited_by": 1484,
      "authors": [
        "Peter Dayan",
        "Geoffrey E Hinton",
        "Radford M Neal",
        "Richard S Zemel"
      ],
      "description": " Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.",
      "citation_histogram": [
        [1995, 13],
        [1996, 28],
        [1997, 29],
        [1998, 36],
        [1999, 38],
        [2000, 17],
        [2001, 25],
        [2002, 24],
        [2003, 34],
        [2004, 24],
        [2005, 23],
        [2006, 21],
        [2007, 25],
        [2008, 21],
        [2009, 24],
        [2010, 26],
        [2011, 18],
        [2012, 45],
        [2013, 78],
        [2014, 72],
        [2015, 69],
        [2016, 86],
        [2017, 98],
        [2018, 115],
        [2019, 108],
        [2020, 143],
        [2021, 121],
        [2022, 98]
      ],
      "detail_extracted": true
    },
    {
      "title": "Autoencoders, minimum description length and Helmholtz free energy",
      "link": "https://proceedings.neurips.cc/paper/1993/hash/9e3cfc48eccf81a0d57663e129aef3cb-Abstract.html",
      "year": 1993,
      "cited_by": 1435,
      "authors": ["Geoffrey E Hinton", "Richard Zemel"],
      "description": "An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distri (cid: 173) bution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this ap (cid: 173) proximation gives an upper bound on the description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn factorial codes.",
      "citation_histogram": [
        [1994, 4],
        [1995, 11],
        [1996, 17],
        [1997, 20],
        [1998, 11],
        [1999, 13],
        [2000, 5],
        [2001, 10],
        [2002, 11],
        [2003, 8],
        [2004, 11],
        [2005, 10],
        [2006, 8],
        [2007, 11],
        [2008, 5],
        [2009, 7],
        [2010, 10],
        [2011, 6],
        [2012, 8],
        [2013, 55],
        [2014, 20],
        [2015, 48],
        [2016, 60],
        [2017, 86],
        [2018, 126],
        [2019, 169],
        [2020, 218],
        [2021, 266],
        [2022, 185]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning distributed representations of concepts",
      "link": "http://www.cs.toronto.edu/~hinton/absps/families.pdf",
      "year": 1986,
      "cited_by": 1430,
      "authors": ["Geoffrey E Hinton"],
      "description": "Concepts can be represented by distributed pat-terns of activity in networks of neuron-like units. One advantage of this kind oi representation is that it leads to automatic generalization. When the weights in the network are changed to incor-porate new knowledge about one concept, the changes affect the knowledge associated with other concepts that are represented by similar activity patterns. There have been numerous demonstrations of sensible generalization which have depended on the experimenter choosing appropriately similar pattems for ditterent concepts-This paper shows how the network can be made to choose thepattems itself when shown a set of propositions that\" use the concepts. it chooses patterns which make explicit the underlying features that are only implicit in the propositions it is shown.",
      "citation_histogram": [
        [1987, 11],
        [1988, 28],
        [1989, 18],
        [1990, 22],
        [1991, 34],
        [1992, 27],
        [1993, 27],
        [1994, 34],
        [1995, 33],
        [1996, 26],
        [1997, 24],
        [1998, 19],
        [1999, 23],
        [2000, 25],
        [2001, 15],
        [2002, 20],
        [2003, 25],
        [2004, 15],
        [2005, 53],
        [2006, 19],
        [2007, 27],
        [2008, 28],
        [2009, 21],
        [2010, 12],
        [2011, 9],
        [2012, 18],
        [2013, 30],
        [2014, 45],
        [2015, 57],
        [2016, 91],
        [2017, 105],
        [2018, 105],
        [2019, 123],
        [2020, 106],
        [2021, 77],
        [2022, 56]
      ],
      "detail_extracted": true
    },
    {
      "title": "The\" wake-sleep\" algorithm for unsupervised neural networks",
      "link": "https://www.science.org/doi/abs/10.1126/science.7761831",
      "year": 1995,
      "cited_by": 1383,
      "authors": [
        "Geoffrey E Hinton",
        "Peter Dayan",
        "Brendan J Frey",
        "Radford M Neal"
      ],
      "description": "An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up \"recognition\" connections convert the input into representations in successive hidden layers, and top-down \"generative\" connections reconstruct the representation in one layer from the representation in the layer above. In the \"wake\" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the \"sleep\" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.",
      "citation_histogram": [
        [1995, 12],
        [1996, 32],
        [1997, 33],
        [1998, 36],
        [1999, 46],
        [2000, 32],
        [2001, 29],
        [2002, 25],
        [2003, 29],
        [2004, 31],
        [2005, 50],
        [2006, 19],
        [2007, 32],
        [2008, 26],
        [2009, 21],
        [2010, 23],
        [2011, 21],
        [2012, 34],
        [2013, 72],
        [2014, 43],
        [2015, 60],
        [2016, 90],
        [2017, 80],
        [2018, 107],
        [2019, 84],
        [2020, 141],
        [2021, 96],
        [2022, 57]
      ],
      "detail_extracted": true
    },
    {
      "title": "Transforming auto-encoders",
      "link": "https://link.springer.com/chapter/10.1007/978-3-642-21735-7_6",
      "year": 2011,
      "cited_by": 1350,
      "authors": ["Geoffrey E Hinton", "Alex Krizhevsky", "Sida D Wang"],
      "description": " The artificial neural networks that are used to recognize shapes typically use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered features, like SIFT [6], that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instantiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the hand-engineered features currently used in computer vision because it provides an efficient way of adapting the features to the domain.",
      "citation_histogram": [
        [2011, 4],
        [2012, 12],
        [2013, 10],
        [2014, 24],
        [2015, 29],
        [2016, 36],
        [2017, 63],
        [2018, 150],
        [2019, 240],
        [2020, 269],
        [2021, 301],
        [2022, 202]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning multiple layers of representation",
      "link": "https://www.sciencedirect.com/science/article/pii/S1364661307002173",
      "year": 2007,
      "cited_by": 1341,
      "authors": ["Geoffrey E Hinton"],
      "description": "To achieve its impressive performance in tasks such as speech perception or object recognition, the brain extracts multiple levels of representation from the sensory input. Backpropagation was the first computationally efficient model of how neural networks could learn multiple layers of representation, but it required labeled training data and it did not work well in deep networks. The limitations of backpropagation learning can now be overcome by using multilayer neural networks that contain top-down connections and training them to generate sensory data rather than to classify it. Learning multilayer generative models might seem difficult, but a recent discovery makes it easy to learn nonlinear distributed representations one layer at a time.",
      "citation_histogram": [
        [2008, 8],
        [2009, 13],
        [2010, 21],
        [2011, 35],
        [2012, 35],
        [2013, 86],
        [2014, 130],
        [2015, 130],
        [2016, 145],
        [2017, 125],
        [2018, 143],
        [2019, 125],
        [2020, 146],
        [2021, 119],
        [2022, 63]
      ],
      "detail_extracted": true
    },
    {
      "title": "Advances in neural information processing systems 22",
      "link": "https://www.proceedings.com/content/034/034099webtoc.pdf",
      "year": 2009,
      "cited_by": 1330,
      "authors": ["Daniel D Lee", "P Pham", "Y Largman", "A Ng"],
      "description": "Advances in Neural Information Processing Systems 29 (Table of Contents) Page 1 \u2022 ISBN: \n978-1-5108-3881-9 Advances in Neural Information Processing Systems 29 Barcelona, Spain \n5-10 December 2016 Volume 1 of 7 Editors: Daniel D. Lee Ulrike von Luxburg Roman Garnett \nMasashi Sugiyama Isabelle Guyon 30th Annual Conference on Neural Information Processing \nSystems 2016 Page 2 Printed from e-media with permission by: Curran Associates, Inc. 57 \nMorehouse Lane Red Hook, NY 12571 Some format issues inherent in the e-media version \nmay also appear in this print version. Copyright\u00a9 (2016) by individual authors and NIPS All \nrights reserved. Printed by Curran Associates, Inc. (2017) For permission requests, please \ncontact Neural Information Processing Systems (NIPS) at the address below. Neural Information \nProcessing Systems 10010 North Torrey Pines Road La Jolla, CA 92037 USA Phone: (\u2026",
      "citation_histogram": [
        [2004, 6],
        [2005, 4],
        [2006, 9],
        [2007, 11],
        [2008, 6],
        [2009, 15],
        [2010, 9],
        [2011, 17],
        [2012, 13],
        [2013, 25],
        [2014, 22],
        [2015, 30],
        [2016, 38],
        [2017, 46],
        [2018, 89],
        [2019, 144],
        [2020, 266],
        [2021, 346],
        [2022, 209]
      ],
      "detail_extracted": true
    },
    {
      "title": "New types of deep neural network learning for speech recognition and related applications: An overview",
      "link": "https://ieeexplore.ieee.org/abstract/document/6639344/",
      "year": 2013,
      "cited_by": 1291,
      "authors": ["Li Deng", "Geoffrey Hinton", "Brian Kingsbury"],
      "description": "In this paper, we provide an overview of the invited and contributed papers presented at the special session at ICASSP-2013, entitled \u201cNew Types of Deep Neural Network Learning for Speech Recognition and Related Applications,\u201d as organized by the authors. We also describe the historical context in which acoustic models based on deep neural networks have been developed. The technical overview of the papers presented in our special session is organized into five ways of improving deep learning methods: (1) better optimization; (2) better types of neural activation function and better network architectures; (3) better ways to determine the myriad hyper-parameters of deep neural networks; (4) more appropriate ways to preprocess speech for deep neural networks; and (5) ways of leveraging multiple languages or dialects that are more easily achieved with deep neural networks than with Gaussian mixture\u00a0\u2026",
      "citation_histogram": [
        [2013, 12],
        [2014, 41],
        [2015, 65],
        [2016, 90],
        [2017, 126],
        [2018, 189],
        [2019, 196],
        [2020, 196],
        [2021, 204],
        [2022, 161]
      ],
      "detail_extracted": true
    },
    {
      "title": "Keeping the neural networks simple by minimizing the description length of the weights",
      "link": "https://dl.acm.org/doi/pdf/10.1145/168304.168306",
      "year": 1993,
      "cited_by": 1202,
      "authors": ["Geoffrey E Hinton", "Drew Van Camp"],
      "description": "Supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases. So during learning, it is important to keep the weights simple by penalizing the amount of information they contain. The amount of information in a weight can be controlled by adding Gaussian noise and the noise level can be adapted during learning to optimize the trade-off between the expected squared error of the network and the amount of information in the weights. We describe a method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units. Provided the output units are linear, the exact derivatives can be computed efficiently without time-consuming Monte Carlo simulations. The idea of minimizing the amount of information that is required to\u00a0\u2026",
      "citation_histogram": [
        [1993, 3],
        [1994, 8],
        [1995, 10],
        [1996, 5],
        [1997, 6],
        [1998, 10],
        [1999, 15],
        [2000, 13],
        [2001, 22],
        [2002, 12],
        [2003, 18],
        [2004, 21],
        [2005, 16],
        [2006, 19],
        [2007, 16],
        [2008, 20],
        [2009, 17],
        [2010, 11],
        [2011, 21],
        [2012, 28],
        [2013, 52],
        [2014, 22],
        [2015, 24],
        [2016, 37],
        [2017, 60],
        [2018, 105],
        [2019, 128],
        [2020, 167],
        [2021, 178],
        [2022, 130]
      ],
      "detail_extracted": true
    },
    {
      "title": "A scalable hierarchical distributed language model",
      "link": "https://proceedings.neurips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model",
      "year": 2008,
      "cited_by": 1201,
      "authors": ["Andriy Mnih", "Geoffrey E Hinton"],
      "description": "Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words that was two orders of magnitude faster than the non-hierarchical language model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical models and achieve state-of-the-art performance.",
      "citation_histogram": [
        [2009, 5],
        [2010, 14],
        [2011, 17],
        [2012, 26],
        [2013, 45],
        [2014, 91],
        [2015, 142],
        [2016, 153],
        [2017, 172],
        [2018, 153],
        [2019, 123],
        [2020, 122],
        [2021, 93],
        [2022, 38]
      ],
      "detail_extracted": true
    },
    {
      "title": "The appeal of parallel distributed processing",
      "link": null,
      "year": 1986,
      "cited_by": 1195,
      "authors": ["GE Hinton JL McClelland", "DE Rumelhart"],
      "description": null,
      "citation_histogram": [
        [1987, 5],
        [1988, 18],
        [1989, 29],
        [1990, 36],
        [1991, 39],
        [1992, 62],
        [1993, 52],
        [1994, 45],
        [1995, 29],
        [1996, 45],
        [1997, 39],
        [1998, 32],
        [1999, 25],
        [2000, 38],
        [2001, 27],
        [2002, 25],
        [2003, 23],
        [2004, 29],
        [2005, 14],
        [2006, 35],
        [2007, 35],
        [2008, 32],
        [2009, 37],
        [2010, 27],
        [2011, 17],
        [2012, 31],
        [2013, 42],
        [2014, 33],
        [2015, 32],
        [2016, 36],
        [2017, 34],
        [2018, 31],
        [2019, 32],
        [2020, 23],
        [2021, 45],
        [2022, 22]
      ],
      "detail_extracted": true
    },
    {
      "title": "Zero-shot learning with semantic output codes",
      "link": "https://proceedings.neurips.cc/paper/3650-zero-shot-learning-with-semantic-output-codes",
      "year": 2009,
      "cited_by": 1049,
      "authors": [
        "Mark Palatucci",
        "Dean Pomerleau",
        "Geoffrey E Hinton",
        "Tom M Mitchell"
      ],
      "description": "We consider the problem of zero-shot learning, where the goal is to learn a classifier  that must predict novel values of  that were omitted from the training set. To achieve this, we define the notion of a semantic output code classifier (SOC) which utilizes a knowledge base of semantic properties of  to extrapolate to novel classes. We provide a formalism for this type of classifier and study its theoretical properties in a PAC framework, showing conditions under which the classifier can accurately predict novel classes. As a case study, we build a SOC classifier for a neural decoding task and show that it can often predict words that people are thinking about from functional magnetic resonance images (fMRI) of their neural activity, even without training examples for those words.",
      "citation_histogram": [
        [2010, 12],
        [2011, 19],
        [2012, 20],
        [2013, 33],
        [2014, 38],
        [2015, 69],
        [2016, 89],
        [2017, 124],
        [2018, 108],
        [2019, 128],
        [2020, 162],
        [2021, 142],
        [2022, 84]
      ],
      "detail_extracted": true
    },
    {
      "title": "When does label smoothing help?",
      "link": "https://proceedings.neurips.cc/paper/2019/hash/f1748d6b0fd9d439f71450117eba2725-Abstract.html",
      "year": 2019,
      "cited_by": 1033,
      "authors": ["Rafael M\u00fcller", "Simon Kornblith", "Geoffrey E Hinton"],
      "description": "The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.",
      "citation_histogram": [
        [2019, 18],
        [2020, 198],
        [2021, 404],
        [2022, 407]
      ],
      "detail_extracted": true
    },
    {
      "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
      "link": "https://arxiv.org/abs/1701.06538",
      "year": 2017,
      "cited_by": 1032,
      "authors": [
        "Noam Shazeer",
        "Azalia Mirhoseini",
        "Krzysztof Maziarz",
        "Andy Davis",
        "Quoc Le",
        "Geoffrey Hinton",
        "Jeff Dean"
      ],
      "description": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",
      "citation_histogram": [
        [2017, 66],
        [2018, 145],
        [2019, 179],
        [2020, 186],
        [2021, 227],
        [2022, 226]
      ],
      "detail_extracted": true
    },
    {
      "title": "Big self-supervised models are strong semi-supervised learners",
      "link": "https://proceedings.neurips.cc/paper/2020/hash/fcbc95ccdd551da181207c0c1400c655-Abstract.html",
      "year": 2020,
      "cited_by": 1027,
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Kevin Swersky",
        "Mohammad Norouzi",
        "Geoffrey E Hinton"
      ],
      "description": "One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy with just 1% of the labels (13 labeled images per class) using ResNet-50, a 10X improvement in label efficiency over the previous state-of-the-art. With 10% of labels, ResNet-50 trained with our method achieves 77.5% top-1 accuracy, outperforming standard supervised training with all of the labels.",
      "citation_histogram": [
        [2020, 58],
        [2021, 468],
        [2022, 492]
      ],
      "detail_extracted": true
    },
    {
      "title": "Neural networks for machine learning lecture 6a overview of mini-batch gradient descent",
      "link": "http://www.cs.toronto.edu/~hinton/coursera/lecture6/lec6.pdf",
      "year": 2012,
      "cited_by": 1013,
      "authors": ["Geoffrey Hinton", "Nitish Srivastava", "Kevin Swersky"],
      "description": "\u2022 The standard momentum method first computes the gradient at the current location and then takes a big jump in the direction of the updated accumulated gradient.\u2022 Ilya Sutskever(2012 unpublished) suggested a new form of momentum that often works better.\u2013Inspired by the Nesterov method for optimizing convex functions.",
      "citation_histogram": [
        [2016, 5],
        [2017, 26],
        [2018, 78],
        [2019, 164],
        [2020, 231],
        [2021, 301],
        [2022, 203]
      ],
      "detail_extracted": true
    },
    {
      "title": "Grammar as a foreign language",
      "link": "https://proceedings.neurips.cc/paper/5635-grammar-as-a-foreign-language",
      "year": 2015,
      "cited_by": 1012,
      "authors": [
        "Oriol Vinyals",
        "\u0141ukasz Kaiser",
        "Terry Koo",
        "Slav Petrov",
        "Ilya Sutskever",
        "Geoffrey Hinton"
      ],
      "description": "Syntactic constituency parsing is a fundamental problem in naturallanguage processing which has been the subject of intensive researchand engineering for decades. As a result, the most accurate parsersare domain specific, complex, and inefficient. In this paper we showthat the domain agnostic attention-enhanced sequence-to-sequence modelachieves state-of-the-art results on the most widely used syntacticconstituency parsing dataset, when trained on a large synthetic corpusthat was annotated using existing parsers. It also matches theperformance of standard parsers when trained on a smallhuman-annotated dataset, which shows that this model is highlydata-efficient, in contrast to sequence-to-sequence models without theattention mechanism. Our parser is also fast, processing over ahundred sentences per second with an unoptimized CPU implementation.",
      "citation_histogram": [
        [2015, 45],
        [2016, 142],
        [2017, 188],
        [2018, 191],
        [2019, 172],
        [2020, 120],
        [2021, 97],
        [2022, 46]
      ],
      "detail_extracted": true
    },
    {
      "title": "Lesioning an attractor network: investigations of acquired dyslexia.",
      "link": "https://psycnet.apa.org/doiLanding?doi=10.1037/0033-295X.98.1.74",
      "year": 1991,
      "cited_by": 1004,
      "authors": ["Geoffrey E Hinton", "Tim Shallice"],
      "description": "A recurrent connectionist network was trained to output semantic feature vectors when presented with letter strings. When damaged, the network exhibited characteristics that resembled several of the phenomena found in deep dyslexia and semantic-access dyslexia. Damaged networks sometimes settled to the semantic vectors for semantically similar but visually dissimilar words. With severe damage, a forced-choice decision between categories was possible even when the choice of the particular semantic vector within the category was not possible. The damaged networks typically exhibited many mixed visual and semantic errors in which the output corresponded to a word that was both visually and semantically similar. Surprisingly, damage near the output sometimes caused pure visual errors. Indeed, the characteristic error pattern of deep dyslexia occurred with damage to virtually any part of the network.",
      "citation_histogram": [
        [1991, 32],
        [1992, 21],
        [1993, 32],
        [1994, 79],
        [1995, 67],
        [1996, 33],
        [1997, 53],
        [1998, 50],
        [1999, 42],
        [2000, 40],
        [2001, 37],
        [2002, 67],
        [2003, 29],
        [2004, 38],
        [2005, 23],
        [2006, 32],
        [2007, 31],
        [2008, 18],
        [2009, 19],
        [2010, 18],
        [2011, 16],
        [2012, 17],
        [2013, 37],
        [2014, 25],
        [2015, 15],
        [2016, 16],
        [2017, 14],
        [2018, 17],
        [2019, 15],
        [2020, 24],
        [2021, 13],
        [2022, 7]
      ],
      "detail_extracted": true
    },
    {
      "title": "Modeling human motion using binary latent variables",
      "link": "https://proceedings.neurips.cc/paper/2006/hash/1091660f3dff84fd648efe31391c5524-Abstract.html",
      "year": 2006,
      "cited_by": 920,
      "authors": ["Graham W Taylor", "Geoffrey E Hinton", "Sam Roweis"],
      "description": "We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued\" visible\" variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efficient and allows us to use a simple approximate learning procedure. After training, the model finds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line filling in of data lost during motion capture. Website: http://www. cs. toronto. edu/gwtaylor/publications/nips2006mhmublv/",
      "citation_histogram": [
        [2006, 3],
        [2007, 15],
        [2008, 14],
        [2009, 41],
        [2010, 32],
        [2011, 48],
        [2012, 33],
        [2013, 49],
        [2014, 81],
        [2015, 81],
        [2016, 89],
        [2017, 101],
        [2018, 96],
        [2019, 81],
        [2020, 67],
        [2021, 48],
        [2022, 31]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep belief networks",
      "link": "http://scholarpedia.org/article/Deep_Belief_Networks",
      "year": 2009,
      "cited_by": 910,
      "authors": ["Geoffrey E Hinton"],
      "description": "Deep belief nets are probabilistic generative models that are composed of multiple layers of stochastic, latent variables. The latent variables typically have binary values and are often called hidden units or feature detectors. The top two layers have undirected, symmetric connections between them and form an associative memory. The lower layers receive top-down, directed connections from the layer above. The states of the units in the lowest layer represent a data vector.",
      "citation_histogram": [
        [2012, 3],
        [2013, 6],
        [2014, 11],
        [2015, 34],
        [2016, 53],
        [2017, 74],
        [2018, 122],
        [2019, 155],
        [2020, 145],
        [2021, 155],
        [2022, 131]
      ],
      "detail_extracted": true
    },
    {
      "title": "The EM algorithm for mixtures of factor analyzers",
      "link": "http://www.cs.utoronto.ca/~hinton/absps/tr-96-1.pdf",
      "year": 1996,
      "cited_by": 908,
      "authors": ["Zoubin Ghahramani", "Geoffrey E Hinton"],
      "description": "Factor analysis, a statistical method for modeling the covariance structure of high dimensional data using a small number of latent variables, can be extended by allowing different local factor models in different regions of the input space. This results in a model which concurrently performs clustering and dimensionality reduction, and can be thought of as a reduced dimension mixture of Gaussians. We present an exact Expectation {Maximization algorithm for fitting the parameters of this mixture of factor analyzers.",
      "citation_histogram": [
        [1997, 4],
        [1998, 12],
        [1999, 19],
        [2000, 21],
        [2001, 17],
        [2002, 35],
        [2003, 25],
        [2004, 33],
        [2005, 34],
        [2006, 40],
        [2007, 41],
        [2008, 50],
        [2009, 31],
        [2010, 50],
        [2011, 32],
        [2012, 34],
        [2013, 49],
        [2014, 48],
        [2015, 37],
        [2016, 53],
        [2017, 36],
        [2018, 42],
        [2019, 46],
        [2020, 39],
        [2021, 40],
        [2022, 30]
      ],
      "detail_extracted": true
    },
    {
      "title": "On contrastive divergence learning",
      "link": "http://proceedings.mlr.press/r5/carreira-perpinan05a/carreira-perpinan05a.pdf",
      "year": 2005,
      "cited_by": 893,
      "authors": ["Miguel A Carreira-Perpinan", "Geoffrey Hinton"],
      "description": "Maximum-likelihood(ML) learning of Markov random fields is challenging because it requires estimates of averages that have an exponential number of terms. Markov chain Monte Carlo methods typically take a long time to converge on unbiased estimates, but Hinton (2002) showed that if the Markov chain is only run for a few steps, the learning can still work well and it approximately minimizes a different function called \u201ccontrastive divergence\u201d(CD). CD learning has been successfully applied to various types of random fields. Here, we study the properties of CD learning and show that it provides biased estimates in general, but that the bias is typically very small. Fast CD learning can therefore be used to get close to an ML solution and slow ML learning can then be used to fine-tune the CD solution.",
      "citation_histogram": [
        [2005, 4],
        [2006, 10],
        [2007, 14],
        [2008, 14],
        [2009, 22],
        [2010, 34],
        [2011, 27],
        [2012, 29],
        [2013, 47],
        [2014, 62],
        [2015, 74],
        [2016, 85],
        [2017, 101],
        [2018, 98],
        [2019, 83],
        [2020, 68],
        [2021, 64],
        [2022, 53]
      ],
      "detail_extracted": true
    },
    {
      "title": "Simplifying neural networks by soft weight sharing",
      "link": "https://www.taylorfrancis.com/chapters/edit/10.1201/9780429492525-13/simplifying-neural-networks-soft-weight-sharing-steven-nowlan-geoffrey-hinton",
      "year": 2018,
      "cited_by": 891,
      "authors": ["Steven J Nowlan", "Geoffrey E Hinton"],
      "description": "This chapter reports on some simulations that compare the generalization performance of networks trained using the cost criterion given in to networks trained in some other ways. It shows that the initialization procedure is used for all of the simulations. However, the mixture densities are clearly not independent of the data and cannot be regarded as classical Bayesian priors. If the Gaussians all start with high variance, the initial division of weights into subsets will be very soft. The complexity cost is a smoothed version of the obvious discrete cost function that has a value of zero for weights that are zero and a value of 1 for all other weights. The differences between the adaptive Gaussian complexity measure and the fixed complexity measure used by A. S. Weigend et al. are not as dramatic on the sunspot task as they were in the shift detection task. One way to approach the multistep prediction problem is to use\u00a0\u2026",
      "citation_histogram": [
        [1992, 4],
        [1993, 32],
        [1994, 37],
        [1995, 37],
        [1996, 35],
        [1997, 35],
        [1998, 29],
        [1999, 13],
        [2000, 8],
        [2001, 17],
        [2002, 15],
        [2003, 11],
        [2004, 17],
        [2005, 11],
        [2006, 17],
        [2007, 15],
        [2008, 13],
        [2009, 3],
        [2010, 8],
        [2011, 6],
        [2012, 16],
        [2013, 9],
        [2014, 14],
        [2015, 16],
        [2016, 23],
        [2017, 45],
        [2018, 69],
        [2019, 81],
        [2020, 97],
        [2021, 87],
        [2022, 66]
      ],
      "detail_extracted": true
    },
    {
      "title": "Matrix capsules with EM routing",
      "link": "https://openreview.net/forum?id=HJWLfGWRb&noteId=rk5MadsMf&noteId=rk5MadsMf",
      "year": 2018,
      "cited_by": 862,
      "authors": ["Geoffrey E Hinton", "Sara Sabour", "Nicholas Frosst"],
      "description": "A capsule is a group of neurons whose outputs represent different properties of the same entity. Each layer in a capsule network contains many capsules. We describe a version of capsules in which each capsule has a logistic unit to represent the presence of an entity and a 4x4 matrix which could learn to represent the relationship between that entity and the viewer (the pose). A capsule in one layer votes for the pose matrix of many different capsules in the layer above by multiplying its own pose matrix by trainable viewpoint-invariant transformation matrices that could learn to represent part-whole relationships. Each of these votes is weighted by an assignment coefficient. These coefficients are iteratively updated for each image using the Expectation-Maximization algorithm such that the output of each capsule is routed to a capsule in the layer above that receives a cluster of similar votes. The transformation matrices are trained discriminatively by backpropagating through the unrolled iterations of EM between each pair of adjacent capsule layers. On the smallNORB benchmark, capsules reduce the number of test errors by 45\\% compared to the state-of-the-art. Capsules also show far more resistance to white box adversarial attacks than our baseline convolutional neural network.",
      "citation_histogram": [
        [2018, 72],
        [2019, 165],
        [2020, 184],
        [2021, 261],
        [2022, 168]
      ],
      "detail_extracted": true
    },
    {
      "title": "A time-delay neural network architecture for isolated word recognition",
      "link": "https://www.sciencedirect.com/science/article/pii/089360809090044L",
      "year": 1990,
      "cited_by": 857,
      "authors": ["Kevin J Lang", "Alex H Waibel", "Geoffrey E Hinton"],
      "description": "A translation-invariant back-propagation network is described that performs better than a sophisticated continuous acoustic parameter hidden Markov model on a noisy, 100-speaker confusable vocabulary isolated word recognition task. The network's replicated architecture permits it to extract precise information from unaligned training patterns selected by a naive segmentation rule.",
      "citation_histogram": [
        [1990, 10],
        [1991, 25],
        [1992, 27],
        [1993, 27],
        [1994, 52],
        [1995, 34],
        [1996, 39],
        [1997, 23],
        [1998, 26],
        [1999, 18],
        [2000, 19],
        [2001, 22],
        [2002, 18],
        [2003, 20],
        [2004, 22],
        [2005, 18],
        [2006, 14],
        [2007, 20],
        [2008, 7],
        [2009, 15],
        [2010, 13],
        [2011, 15],
        [2012, 23],
        [2013, 33],
        [2014, 33],
        [2015, 26],
        [2016, 32],
        [2017, 40],
        [2018, 35],
        [2019, 42],
        [2020, 41],
        [2021, 31],
        [2022, 22]
      ],
      "detail_extracted": true
    },
    {
      "title": "Feudal reinforcement learning",
      "link": "https://proceedings.neurips.cc/paper/1992/hash/d14220ee66aeec73c49038385428ec4c-Abstract.html",
      "year": 1992,
      "cited_by": 850,
      "authors": ["Peter Dayan", "Geoffrey E Hinton"],
      "description": "One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time. This paper shows how to create a Q-Iearning managerial hierarchy in which high level managers learn how to set tasks to their sub (cid: 173) managers who, in turn, learn how to satisfy them. Sub-managers need not initially understand their managers' commands. They simply learn to maximise their reinforcement in the context of the current command. We illustrate the system using a simple maze task.. As the system learns how to get around, satisfying commands at the multiple levels, it explores more efficiently than standard, flat, Q-Iearning and builds a more comprehensive map.",
      "citation_histogram": [
        [1993, 4],
        [1994, 15],
        [1995, 8],
        [1996, 11],
        [1997, 12],
        [1998, 33],
        [1999, 13],
        [2000, 14],
        [2001, 22],
        [2002, 16],
        [2003, 19],
        [2004, 23],
        [2005, 17],
        [2006, 19],
        [2007, 20],
        [2008, 17],
        [2009, 18],
        [2010, 12],
        [2011, 16],
        [2012, 15],
        [2013, 17],
        [2014, 14],
        [2015, 16],
        [2016, 20],
        [2017, 26],
        [2018, 60],
        [2019, 98],
        [2020, 87],
        [2021, 96],
        [2022, 82]
      ],
      "detail_extracted": true
    },
    {
      "title": "Regularizing neural networks by penalizing confident output distributions",
      "link": "https://arxiv.org/abs/1701.06548",
      "year": 2017,
      "cited_by": 843,
      "authors": [
        "Gabriel Pereyra",
        "George Tucker",
        "Jan Chorowski",
        "\u0141ukasz Kaiser",
        "Geoffrey Hinton"
      ],
      "description": "We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers.",
      "citation_histogram": [
        [2016, 3],
        [2017, 28],
        [2018, 100],
        [2019, 117],
        [2020, 208],
        [2021, 230],
        [2022, 148]
      ],
      "detail_extracted": true
    },
    {
      "title": "A theoretical framework for back-propagation",
      "link": "https://www.researchgate.net/profile/Yann-Lecun/publication/2360531_A_Theoretical_Framework_for_Back-Propagation/links/0deec519dfa297eac1000000/A-Theoretical-Framework-for-Back-Propagation.pdf",
      "year": 1988,
      "cited_by": 829,
      "authors": ["Yann LeCun", "D Touresky", "G Hinton", "T Sejnowski"],
      "description": "Among all the supervised learning algo-rithms, back propagation (BP) is probably the most wi (l) dely used. Although nu-merous experimental works have demonstrated its capabilities, a deeper theoretical understanding of the algorithm is definitely needed. We present a mathematical framework for studying back-propagation based on the Lagrangian formalism. In this framework, inspired by optimal control theory, back-propagation is formulated as an optimization problem with nonlinear constraints. The Lagrange function is the sum of an output objective function and a constraint term which describes the network dynamics. This approach suggests many natural extensions to the basic algorithm. It also provides an extremely simple for-mulation (and derivation) of continuous re-current network equations as described by Pineda [Pineda, 1987]. Other easily described variations involve either additional terms in the\u00a0\u2026",
      "citation_histogram": [
        [1989, 5],
        [1990, 8],
        [1991, 12],
        [1992, 10],
        [1993, 17],
        [1994, 15],
        [1995, 11],
        [1996, 8],
        [1997, 7],
        [1998, 2],
        [1999, 2],
        [2000, 4],
        [2001, 3],
        [2002, 5],
        [2003, 5],
        [2004, 6],
        [2005, 8],
        [2006, 2],
        [2007, 4],
        [2008, 6],
        [2009, 4],
        [2010, 8],
        [2011, 12],
        [2012, 10],
        [2013, 10],
        [2014, 10],
        [2015, 30],
        [2016, 34],
        [2017, 65],
        [2018, 133],
        [2019, 132],
        [2020, 131],
        [2021, 101]
      ],
      "detail_extracted": true
    },
    {
      "title": "Boltzmann machines: Constraint satisfaction networks that learn",
      "link": "http://www.csri.utoronto.ca/~hinton/absps/bmtr.pdf",
      "year": 1984,
      "cited_by": 810,
      "authors": [
        "Geoffrey E Hinton",
        "Terrence J Sejnowski",
        "David H Ackley"
      ],
      "description": "The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficien\u00fcy two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the pre-existing hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general\u00a0\u2026",
      "citation_histogram": [
        [1984, 4],
        [1985, 22],
        [1986, 64],
        [1987, 32],
        [1988, 68],
        [1989, 35],
        [1990, 39],
        [1991, 23],
        [1992, 36],
        [1993, 38],
        [1994, 30],
        [1995, 16],
        [1996, 21],
        [1997, 9],
        [1998, 15],
        [1999, 10],
        [2000, 13],
        [2001, 14],
        [2002, 11],
        [2003, 19],
        [2004, 9],
        [2005, 8],
        [2006, 10],
        [2007, 8],
        [2008, 6],
        [2009, 7],
        [2010, 9],
        [2011, 12],
        [2012, 11],
        [2013, 12],
        [2014, 19],
        [2015, 13],
        [2016, 23],
        [2017, 21],
        [2018, 20],
        [2019, 34],
        [2020, 21],
        [2021, 23],
        [2022, 15]
      ],
      "detail_extracted": true
    },
    {
      "title": "Optimal perceptual inference",
      "link": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.6007&rep=rep1&type=pdf",
      "year": 1983,
      "cited_by": 800,
      "authors": ["Geoffrey E Hinton", "Terrence J Sejnowski"],
      "description": "When a vision system creates an interpretation of some input datn, it assigns truth values or probabilities to intcrnal hypothcses about the world. We present a non-dctcrministic method for assigning truth values that avoids many of the problcms encountered by existing relaxation methods. Instead of rcprcscnting probabilitics with realnumbers, we usc a more dircct encoding in which thc probability",
      "citation_histogram": [
        [1983, 4],
        [1984, 10],
        [1985, 19],
        [1986, 43],
        [1987, 18],
        [1988, 60],
        [1989, 19],
        [1990, 21],
        [1991, 10],
        [1992, 24],
        [1993, 13],
        [1994, 14],
        [1995, 20],
        [1996, 18],
        [1997, 16],
        [1998, 12],
        [1999, 13],
        [2000, 10],
        [2001, 15],
        [2002, 9],
        [2003, 11],
        [2004, 7],
        [2005, 12],
        [2006, 7],
        [2007, 6],
        [2008, 15],
        [2009, 14],
        [2010, 10],
        [2011, 11],
        [2012, 22],
        [2013, 20],
        [2014, 19],
        [2015, 27],
        [2016, 28],
        [2017, 33],
        [2018, 31],
        [2019, 38],
        [2020, 32],
        [2021, 40],
        [2022, 33]
      ],
      "detail_extracted": true
    },
    {
      "title": "Three new graphical models for statistical language modelling",
      "link": "https://dl.acm.org/doi/abs/10.1145/1273496.1273577",
      "year": 2007,
      "cited_by": 733,
      "authors": ["Andriy Mnih", "Geoffrey Hinton"],
      "description": "The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best n-gram\u00a0\u2026",
      "citation_histogram": [
        [2008, 7],
        [2009, 7],
        [2010, 9],
        [2011, 9],
        [2012, 21],
        [2013, 27],
        [2014, 68],
        [2015, 110],
        [2016, 107],
        [2017, 98],
        [2018, 79],
        [2019, 72],
        [2020, 54],
        [2021, 36],
        [2022, 22]
      ],
      "detail_extracted": true
    },
    {
      "title": "Unsupervised learning: foundations of neural computation",
      "link": "https://books.google.com/books?hl=en&lr=&id=yj04Y0lje4cC&oi=fnd&pg=PR7&dq=info:DCbGy-cTd90J:scholar.google.com&ots=b-uzXRa6dx&sig=C2Fyjt8DRl9Kt3ybe4-3TtKXi5s",
      "year": 1999,
      "cited_by": 730,
      "authors": ["Geoffrey E Hinton", "Terrence J Sejnowski"],
      "description": "Since its founding in 1989 by Terrence Sejnowski, Neural Computation has become the leading journal in the field. Foundations of Neural Computation collects, by topic, the most significant papers that have appeared in the journal over the past nine years. This volume of Foundations of Neural Computation, on unsupervised learning algorithms, focuses on neural network learning algorithms that do not require an explicit teacher. The goal of unsupervised learning is to extract an efficient internal representation of the statistical structure implicit in the inputs. These algorithms provide insights into the development of the cerebral cortex and implicit learning in humans. They are also of interest to engineers working in areas such as computer vision and speech recognition who seek efficient representations of raw input data.",
      "citation_histogram": [
        [1999, 5],
        [2000, 9],
        [2001, 7],
        [2002, 11],
        [2003, 10],
        [2004, 9],
        [2005, 14],
        [2006, 19],
        [2007, 9],
        [2008, 15],
        [2009, 15],
        [2010, 23],
        [2011, 13],
        [2012, 20],
        [2013, 26],
        [2014, 25],
        [2015, 24],
        [2016, 22],
        [2017, 20],
        [2018, 18],
        [2019, 52],
        [2020, 136],
        [2021, 138],
        [2022, 80]
      ],
      "detail_extracted": true
    },
    {
      "title": "A simple way to initialize recurrent networks of rectified linear units",
      "link": "https://arxiv.org/abs/1504.00941",
      "year": 2015,
      "cited_by": 723,
      "authors": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"],
      "description": "Learning long term dependencies in recurrent networks is difficult due to vanishing and exploding gradients. To overcome this difficulty, researchers have developed sophisticated optimization techniques and network architectures. In this paper, we propose a simpler solution that use recurrent neural networks composed of rectified linear units. Key to our solution is the use of the identity matrix or its scaled version to initialize the recurrent weight matrix. We find that our solution is comparable to LSTM on our four benchmarks: two toy problems involving long-range temporal structures, a large language modeling problem and a benchmark speech recognition problem.",
      "citation_histogram": [
        [2015, 20],
        [2016, 86],
        [2017, 89],
        [2018, 112],
        [2019, 143],
        [2020, 99],
        [2021, 104],
        [2022, 62]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parameter estimation for linear dynamical systems",
      "link": "http://learning.eng.cam.ac.uk/zoubin/papers/tr-96-2.pdf",
      "year": 1996,
      "cited_by": 709,
      "authors": ["Zoubin Ghahramani", "Geoffrey E Hinton"],
      "description": "Linear systems have been used extensively in engineering to model and control the behavior of dynamical systems. In this note, we present the Expectation Maximization (EM) algorithm for estimating the parameters of linear systems (Shumway and Sto er, 1982). We also point out the relationship between linear dynamical systems, factor analysis, and hidden Markov models.",
      "citation_histogram": [
        [1998, 3],
        [1999, 6],
        [2000, 5],
        [2001, 6],
        [2002, 10],
        [2003, 20],
        [2004, 23],
        [2005, 14],
        [2006, 24],
        [2007, 36],
        [2008, 34],
        [2009, 22],
        [2010, 30],
        [2011, 32],
        [2012, 42],
        [2013, 35],
        [2014, 34],
        [2015, 58],
        [2016, 41],
        [2017, 39],
        [2018, 39],
        [2019, 40],
        [2020, 53],
        [2021, 31],
        [2022, 27]
      ],
      "detail_extracted": true
    },
    {
      "title": "Classical and Bayesian inference in neuroimaging: theory",
      "link": "https://www.sciencedirect.com/science/article/pii/S1053811902910906",
      "year": 2002,
      "cited_by": 695,
      "authors": [
        "Karl J Friston",
        "William Penny",
        "Christophe Phillips",
        "S Kiebel",
        "G Hinton",
        "John Ashburner"
      ],
      "description": "This paper reviews hierarchical observation models, used in functional neuroimaging, in a Bayesian light. It emphasizes the common ground shared by classical and Bayesian methods to show that conventional analyses of neuroimaging data can be usefully extended within an empirical Bayesian framework. In particular we formulate the procedures used in conventional data analysis in terms of hierarchical linear models and establish a connection between classical inference and parametric empirical Bayes (PEB) through covariance component estimation. This estimation is based on an expectation maximization or EM algorithm. The key point is that hierarchical models not only provide for appropriate inference at the highest level but that one can revisit lower levels suitably equipped to make Bayesian inferences. Bayesian inferences eschew many of the difficulties encountered with classical inference and\u00a0\u2026",
      "citation_histogram": [
        [2002, 5],
        [2003, 14],
        [2004, 25],
        [2005, 28],
        [2006, 43],
        [2007, 58],
        [2008, 57],
        [2009, 40],
        [2010, 37],
        [2011, 40],
        [2012, 52],
        [2013, 36],
        [2014, 39],
        [2015, 37],
        [2016, 37],
        [2017, 25],
        [2018, 29],
        [2019, 23],
        [2020, 19],
        [2021, 26],
        [2022, 15]
      ],
      "detail_extracted": true
    },
    {
      "title": "Convolutional deep belief networks on cifar-10",
      "link": "https://www.cs.toronto.edu/~kriz/conv-cifar10-aug2010.pdf",
      "year": 2010,
      "cited_by": 670,
      "authors": ["Alex Krizhevsky", "Geoff Hinton"],
      "description": "We describe how to train a two-layer convolutional Deep Belief Network (DBN) on the 1.6 million tiny images dataset.When training a convolutional DBN, one must decide what to do with the edge pixels of teh images. As the pixels near the edge of an image contribute to the fewest convolutional filter outputs, the model may see it fit to tailor its few convolutional filters to better model the edge pixels. This is undesirable becaue it usually comes at the expense of a good model for the interior parts of the image. We investigate several ways of dealing with the edge pixels when training a convolutional DBN. Using a combination of locally-connected convolutional units and globally-connected units, as well as a few tricks to reduce the effects of overfitting, we achieve state-of-the-art performance in the classification task of the CIFAR-10 subset of the tiny images dataset.",
      "citation_histogram": [
        [2011, 13],
        [2012, 16],
        [2013, 17],
        [2014, 10],
        [2015, 23],
        [2016, 38],
        [2017, 53],
        [2018, 66],
        [2019, 95],
        [2020, 124],
        [2021, 118],
        [2022, 83]
      ],
      "detail_extracted": true
    },
    {
      "title": "How neural networks learn from experience",
      "link": "https://www.jstor.org/stable/24939221",
      "year": 1992,
      "cited_by": 663,
      "authors": ["Geoffrey E Hinton"],
      "description": "The brain is a remarkable computer. It interprets imprecise information from the senses at an incredibly rapid rate. It discerns a whisper in a noisy room, a face in a dimly lit alley and a hidden agenda in a political statement. Most impressive of all, the brain learns-without any explicit instruc tions-to create the internal representations that make these skills possible. Much is still unknown about how the brain trains itself to process information, so theo ries abound. To test these hypotheses, my col leagues and I have attempted to mimic the brain's le g processes by creating networks of artificial neurons. We con struct these neural networks by first trying to deduce the es sential features of neurons and their interconnections. We then typically program a computer to simulate these features. Because our knowledge of neurons is incomplete and our computing power is limited, our models are necessarily gross idealizations\u00a0\u2026",
      "citation_histogram": [
        [1992, 2],
        [1993, 21],
        [1994, 21],
        [1995, 21],
        [1996, 22],
        [1997, 21],
        [1998, 30],
        [1999, 21],
        [2000, 29],
        [2001, 15],
        [2002, 22],
        [2003, 17],
        [2004, 16],
        [2005, 21],
        [2006, 34],
        [2007, 19],
        [2008, 15],
        [2009, 20],
        [2010, 21],
        [2011, 13],
        [2012, 16],
        [2013, 26],
        [2014, 22],
        [2015, 20],
        [2016, 17],
        [2017, 18],
        [2018, 21],
        [2019, 26],
        [2020, 23],
        [2021, 37],
        [2022, 20]
      ],
      "detail_extracted": true
    },
    {
      "title": "On rectified linear units for speech processing",
      "link": "https://ieeexplore.ieee.org/abstract/document/6638312/",
      "year": 2013,
      "cited_by": 642,
      "authors": [
        "Matthew D Zeiler",
        "M Ranzato",
        "Rajat Monga",
        "Min Mao",
        "Kun Yang",
        "Quoc Viet Le",
        "Patrick Nguyen",
        "Alan Senior",
        "Vincent Vanhoucke",
        "Jeffrey Dean",
        "Geoffrey E Hinton"
      ],
      "description": "Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity, which is typically a logistic function. In this work, we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting, we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting, we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several\u00a0\u2026",
      "citation_histogram": [
        [2013, 10],
        [2014, 47],
        [2015, 79],
        [2016, 78],
        [2017, 88],
        [2018, 74],
        [2019, 66],
        [2020, 71],
        [2021, 74],
        [2022, 50]
      ],
      "detail_extracted": true
    },
    {
      "title": "Replicated softmax: an undirected topic model",
      "link": "https://proceedings.neurips.cc/paper/3856-replicated-softmax-an-undirected-topic-model",
      "year": 2009,
      "cited_by": 638,
      "authors": ["Geoffrey E Hinton", "Russ R Salakhutdinov"],
      "description": "We show how to model documents as bags of words using family of two-layer, undirected graphical models. Each member of the family has the same number of binary hidden units but a different number of``softmax visible units. All of the softmax units in all of the models in the family share the same weights to the binary hidden units. We describe efficient inference and learning procedures for such a family. Each member of the family models the probability distribution of documents of a specific length as a product of topic-specific distributions rather than as a mixture and this gives much better generalization than Latent Dirichlet Allocation for modeling the log probabilities of held-out documents. The low-dimensional topic vectors learned by the undirected family are also much better than LDA topic vectors for retrieving documents that are similar to a query document. The learned topics are more general than those found by LDA because precision is achieved by intersecting many general topics rather than by selecting a single precise topic to generate each word.",
      "citation_histogram": [
        [2009, 3],
        [2010, 5],
        [2011, 6],
        [2012, 14],
        [2013, 21],
        [2014, 40],
        [2015, 78],
        [2016, 73],
        [2017, 84],
        [2018, 66],
        [2019, 88],
        [2020, 64],
        [2021, 51],
        [2022, 37]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning to detect roads in high-resolution aerial images",
      "link": "https://link.springer.com/chapter/10.1007/978-3-642-15567-3_16",
      "year": 2010,
      "cited_by": 626,
      "authors": ["Volodymyr Mnih", "Geoffrey E Hinton"],
      "description": " Reliably extracting information from aerial imagery is a difficult problem with many practical applications. One specific case of this problem is the task of automatically detecting roads. This task is a difficult vision problem because of occlusions, shadows, and a wide variety of non-road objects. Despite 30 years of work on automatic road detection, no automatic or semi-automatic road detection system is currently on the market and no published method has been shown to work reliably on large datasets of urban imagery. We propose detecting roads using a neural network with millions of trainable weights which looks at a much larger context than was used in previous attempts at learning the task. The network is trained on massive amounts of data using a consumer GPU. We demonstrate that predictive performance can be substantially improved by initializing the feature detectors using recently developed\u00a0\u2026",
      "citation_histogram": [
        [2011, 6],
        [2012, 11],
        [2013, 18],
        [2014, 28],
        [2015, 28],
        [2016, 63],
        [2017, 61],
        [2018, 78],
        [2019, 87],
        [2020, 76],
        [2021, 86],
        [2022, 72]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep belief networks for phone recognition",
      "link": "http://www.cs.utoronto.ca/~gdahl/papers/dbnPhoneRec.pdf",
      "year": 2009,
      "cited_by": 589,
      "authors": ["Abdel-rahman Mohamed", "George Dahl", "Geoffrey Hinton"],
      "description": "Hidden Markov Models (HMMs) have been the state-of-the-art techniques for acoustic modeling despite their unrealistic independence assumptions and the very limited representational capacity of their hidden states. There are many proposals in the research community for deeper models that are capable of modeling the many types of variability present in the speech generation process. Deep Belief Networks (DBNs) have recently proved to be very effective for a variety of machine learning problems and this paper applies DBNs to acoustic modeling. On the standard TIMIT corpus, DBNs consistently outperform other techniques and the best DBN achieves a phone error rate (PER) of 23.0% on the TIMIT core test set.",
      "citation_histogram": [
        [2010, 19],
        [2011, 30],
        [2012, 33],
        [2013, 56],
        [2014, 47],
        [2015, 57],
        [2016, 50],
        [2017, 52],
        [2018, 56],
        [2019, 67],
        [2020, 44],
        [2021, 42],
        [2022, 30]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning a nonlinear embedding by preserving class neighbourhood structure",
      "link": "http://proceedings.mlr.press/v2/salakhutdinov07a.html",
      "year": 2007,
      "cited_by": 586,
      "authors": ["Ruslan Salakhutdinov", "Geoff Hinton"],
      "description": "We show how to pretrain and fine-tune a multilayer neural network to learn a nonlinear transformation from the input space to a lowdimensional feature space in which K-nearest neighbour classification performs well. We also show how the non-linear transformation can be improved using unlabeled data. Our method achieves a much lower error rate than Support Vector Machines or standard backpropagation on a widely used version of the MNIST handwritten digit recognition task. If some of the dimensions of the low-dimensional feature space are not used for nearest neighbor classification, our method uses these dimensions to explicitly represent transformations of the digits that do not affect their identity.",
      "citation_histogram": [
        [2006, 2],
        [2007, 5],
        [2008, 16],
        [2009, 18],
        [2010, 27],
        [2011, 30],
        [2012, 26],
        [2013, 42],
        [2014, 44],
        [2015, 49],
        [2016, 49],
        [2017, 41],
        [2018, 55],
        [2019, 49],
        [2020, 39],
        [2021, 48],
        [2022, 35]
      ],
      "detail_extracted": true
    },
    {
      "title": "Exponential family harmoniums with an application to information retrieval",
      "link": "https://proceedings.neurips.cc/paper/2004/hash/0e900ad84f63618452210ab8baae0218-Abstract.html",
      "year": 2004,
      "cited_by": 581,
      "authors": ["Max Welling", "Michal Rosen-Zvi", "Geoffrey E Hinton"],
      "description": "Directed graphical models with one layer of observed random variables and one or more layers of hidden random variables have been the dominant modelling paradigm in many research fields. Although this approach has met with considerable success, the causal semantics of these models can make it difficult to infer the posterior distribution over the hidden variables. In this paper we propose an alternative two-layer model based on exponential family distributions and the semantics of undirected models. Inference in these \u201cexponential family harmoniums\u201d is fast while learning is performed by minimizing contrastive divergence. A member of this family is then studied as an alternative probabilistic model for latent semantic indexing. In experiments it is shown that they perform well on document retrieval tasks and provide an elegant solution to searching with keywords.",
      "citation_histogram": [
        [2006, 12],
        [2007, 16],
        [2008, 22],
        [2009, 29],
        [2010, 37],
        [2011, 31],
        [2012, 39],
        [2013, 26],
        [2014, 50],
        [2015, 57],
        [2016, 49],
        [2017, 63],
        [2018, 35],
        [2019, 42],
        [2020, 26],
        [2021, 22],
        [2022, 17]
      ],
      "detail_extracted": true
    },
    {
      "title": "Stochastic neighbor embedding",
      "link": "https://ci.nii.ac.jp/naid/10015057936/",
      "year": 2003,
      "cited_by": 566,
      "authors": ["Geoffrey Hinton"],
      "description": "CiNii \u8ad6\u6587 - Stochastic neighbor embedding CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3\n] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \n\u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\n\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 [2/17\u66f4\u65b0]2022\u5e744\u67081\u65e5\u304b\u3089\u306e\nCiNii Articles\u306eCiNii Research\u3078\u306e\u7d71\u5408\u306b\u3064\u3044\u3066 Stochastic neighbor embedding HINTON G. \u88ab\n\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 HINTON G. \u53ce\u9332\u520a\u884c\u7269 Advances in neural information processing systems \nAdvances in neural information processing systems 15, 857-864, 2003 MIT Press \u88ab\u5f15\u7528\u6587\u732e\n: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Self-organizing maps and clustering methods for matrix data SEO \nSambu , OBERMAYER Klaus Neural networks : the official journal of the International Neural \nNetwork Society 17(8), 1211-1229, 2004-10-01 \u53c2\u8003\u6587\u732e45\u4ef6 \u88ab\u5f15\u7528\u6587\u732e3\u4ef6 Tweet \u5404\u7a2e\u2026",
      "citation_histogram": [
        [2005, 3],
        [2006, 1],
        [2007, 2],
        [2008, 1],
        [2009, 3],
        [2010, 10],
        [2011, 8],
        [2012, 8],
        [2013, 11],
        [2014, 9],
        [2015, 5],
        [2016, 10],
        [2017, 27],
        [2018, 66],
        [2019, 96],
        [2020, 127],
        [2021, 105],
        [2022, 71]
      ],
      "detail_extracted": true
    },
    {
      "title": "Glove-talk: A neural network interface between a data-glove and a speech synthesizer",
      "link": "https://ieeexplore.ieee.org/abstract/document/182690/",
      "year": 1993,
      "cited_by": 555,
      "authors": ["Sidney S Fels", "Geoffrey E Hinton"],
      "description": "To illustrate the potential of multilayer neural networks for adaptive interfaces, a VPL Data-Glove connected to a DECtalk speech synthesizer via five neural networks was used to implement a hand-gesture to speech system. Using minor variations of the standard backpropagation learning procedure, the complex mapping of hand movements to speech is learned using data obtained from a single 'speaker' in a simple training phase. With a 203 gesture-to-word vocabulary, the wrong word is produced less than 1% of the time, and no word is produced about 5% of the time. Adaptive control of the speaking rate and word stress is also available. The training times and final performance speed are improved by using small, separate networks for each naturally defined subtask. The system demonstrates that neural networks can be used to develop the complex mappings required in a high bandwidth interface that adapts\u00a0\u2026",
      "citation_histogram": [
        [1993, 7],
        [1994, 9],
        [1995, 15],
        [1996, 15],
        [1997, 15],
        [1998, 19],
        [1999, 14],
        [2000, 19],
        [2001, 15],
        [2002, 14],
        [2003, 16],
        [2004, 28],
        [2005, 20],
        [2006, 18],
        [2007, 15],
        [2008, 17],
        [2009, 15],
        [2010, 19],
        [2011, 25],
        [2012, 17],
        [2013, 23],
        [2014, 22],
        [2015, 27],
        [2016, 28],
        [2017, 15],
        [2018, 25],
        [2019, 18],
        [2020, 29],
        [2021, 18],
        [2022, 11]
      ],
      "detail_extracted": true
    },
    {
      "title": "Experiments on Learning by Back Propagation.",
      "link": "https://eric.ed.gov/?id=ED286930",
      "year": 1986,
      "cited_by": 552,
      "authors": ["D. C. Plaut", "S. Nowlan", "G. E. Hinton"],
      "description": "Rumelhart, Hinton and Williams [Rumelhart 86] describe a learning procedure for layered networks of deterministic, neuion-like units. This paper describes further research on the learning procedure. We start by describing the units, the way they are connected, the learning procedure, and the extension to iterative nets. We then give an example in which a network learns a set of filters that enabla it to discriminate formant-like patterns in the presence of noise.The speed of learning is strongly dependent on the shape of the surface formed by the error measure in\" weight space.\" We give examples of the shape of the error surface for a typical task and illustrate how an acceleration method speeds up descent in weight space.",
      "citation_histogram": [
        [1987, 6],
        [1988, 11],
        [1989, 9],
        [1990, 8],
        [1991, 15],
        [1992, 14],
        [1993, 11],
        [1994, 15],
        [1995, 23],
        [1996, 18],
        [1997, 20],
        [1998, 21],
        [1999, 17],
        [2000, 12],
        [2001, 13],
        [2002, 15],
        [2003, 14],
        [2004, 16],
        [2005, 9],
        [2006, 14],
        [2007, 19],
        [2008, 9],
        [2009, 11],
        [2010, 9],
        [2011, 16],
        [2012, 16],
        [2013, 18],
        [2014, 19],
        [2015, 12],
        [2016, 27],
        [2017, 23],
        [2018, 20],
        [2019, 15],
        [2020, 16],
        [2021, 18],
        [2022, 15]
      ],
      "detail_extracted": true
    },
    {
      "title": "Variational learning for switching state-space models",
      "link": "https://ieeexplore.ieee.org/abstract/document/6789465/",
      "year": 2000,
      "cited_by": 548,
      "authors": ["Zoubin Ghahramani", "Geoffrey E Hinton"],
      "description": "We introduce a new statistical model for time series that iteratively segments data into regimes with approximately linear dynamics and learns the parameters of each of these linear regimes. This model combines and generalizes two of the most widely used stochastic time-series models\u2014hidden Markov models and linear dynamical systems\u2014and is closely related to models that are widely used in the control and econometrics literatures. It can also be derived by extending the mixture of experts neural network (Jacobs, Jordan, Nowlan, & Hinton, 1991) to its fully dynamical version, in which both expert and gating networks are recurrent. Inferring the posterior probabilities of the hidden states of this model is computationally intractable, and therefore the exact expectation maximization (EM) algorithm cannot be applied. However, we present a variational approximation that maximizes a lower bound on the log\u00a0\u2026",
      "citation_histogram": [
        [2000, 3],
        [2001, 8],
        [2002, 14],
        [2003, 29],
        [2004, 25],
        [2005, 29],
        [2006, 28],
        [2007, 23],
        [2008, 30],
        [2009, 23],
        [2010, 23],
        [2011, 21],
        [2012, 31],
        [2013, 27],
        [2014, 22],
        [2015, 27],
        [2016, 31],
        [2017, 27],
        [2018, 28],
        [2019, 17],
        [2020, 27],
        [2021, 33],
        [2022, 15]
      ],
      "detail_extracted": true
    },
    {
      "title": "Self-organizing neural network that discovers surfaces in random-dot stereograms",
      "link": "https://www.nature.com/articles/355161a0",
      "year": 1992,
      "cited_by": 542,
      "authors": ["Suzanna Becker", "Geoffrey E Hinton"],
      "description": "THE standard form of back-propagation learning1 is implausible as a model of perceptual learning because it requires an external teacher to specify the desired output of the network. We show how the external teacher can be replaced by internally derived teaching signals. These signals are generated by using the assumption that different parts of the perceptual input have common causes in the external world. Small modules that look at separate but related parts of the perceptual input discover these common causes by striving to produce outputs that agree with each other (Fig. la). The modules may look at different modalities (such as vision and touch), or the same modality at different times (for example, the consecutive two-dimensional views of a rotating three-dimensional object), or even spatially adjacent parts of the same image. Our simulations show that when our learning procedure is applied to adjacent\u00a0\u2026",
      "citation_histogram": [
        [1991, 2],
        [1992, 7],
        [1993, 11],
        [1994, 15],
        [1995, 20],
        [1996, 21],
        [1997, 42],
        [1998, 12],
        [1999, 14],
        [2000, 17],
        [2001, 17],
        [2002, 12],
        [2003, 18],
        [2004, 20],
        [2005, 17],
        [2006, 11],
        [2007, 16],
        [2008, 14],
        [2009, 5],
        [2010, 15],
        [2011, 9],
        [2012, 11],
        [2013, 9],
        [2014, 5],
        [2015, 11],
        [2016, 7],
        [2017, 14],
        [2018, 17],
        [2019, 14],
        [2020, 30],
        [2021, 47],
        [2022, 51]
      ],
      "detail_extracted": true
    },
    {
      "title": "The recurrent temporal restricted boltzmann machine",
      "link": "https://proceedings.neurips.cc/paper/3567-the-recurrent-temporal-restricted-boltzmann-machine",
      "year": 2008,
      "cited_by": 535,
      "authors": ["Ilya Sutskever", "Geoffrey E Hinton", "Graham W Taylor"],
      "description": "The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (ie, generate nice-looking samples of) several very high dimensional sequences, such as motion capture data and the pixels of low resolution videos of balls bouncing in a box. The major disadvantage of the TRBM is that exact inference is extremely hard, since even computing a Gibbs update for a single variable of the posterior is exponentially expensive. This difficulty has necessitated the use of a heuristic inference procedure, that nonetheless was accurate enough for successful learning. In this paper we introduce the Recurrent TRBM, which is a very slight modification of the TRBM for which exact inference is very easy and exact gradient learning is almost tractable. We demonstrate that the RTRBM is better than an analogous TRBM at generating motion capture and videos of bouncing balls.",
      "citation_histogram": [
        [2009, 9],
        [2010, 4],
        [2011, 15],
        [2012, 10],
        [2013, 21],
        [2014, 39],
        [2015, 46],
        [2016, 55],
        [2017, 54],
        [2018, 68],
        [2019, 62],
        [2020, 56],
        [2021, 49],
        [2022, 43]
      ],
      "detail_extracted": true
    },
    {
      "title": "Neural networks for machine learning",
      "link": "https://scholar.google.com/scholar?cluster=7674779070165002518&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 526,
      "authors": ["Geoffrey Hinton", "Nitsh Srivastava", "Kevin Swersky"],
      "description": null,
      "citation_histogram": [
        [2013, 2],
        [2014, 9],
        [2015, 9],
        [2016, 26],
        [2017, 36],
        [2018, 55],
        [2019, 98],
        [2020, 95],
        [2021, 108],
        [2022, 85]
      ],
      "detail_extracted": true
    },
    {
      "title": "Using very deep autoencoders for content-based image retrieval.",
      "link": "https://www.cs.utoronto.ca/~hinton/absps/esann-deep-final.pdf",
      "year": 2011,
      "cited_by": 508,
      "authors": ["Alex Krizhevsky", "Geoffrey E Hinton"],
      "description": "We show how to learn many layers of features on color images and we use these features to initialize deep autoencoders. We then use the autoencoders to map images to short binary codes. Using semantic hashing [6], 28-bit codes can be used to retrieve images that are similar to a query image in a time that is independent of the size of the database. This extremely fast retrieval makes it possible to search using multiple different transformations of the query image. 256-bit binary codes allow much more accurate matching and can be used to prune the set of images found using the 28-bit codes.",
      "citation_histogram": [
        [2012, 7],
        [2013, 7],
        [2014, 16],
        [2015, 36],
        [2016, 48],
        [2017, 70],
        [2018, 71],
        [2019, 90],
        [2020, 57],
        [2021, 60],
        [2022, 35]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning to combine foveal glimpses with a third-order Boltzmann machine",
      "link": "https://proceedings.neurips.cc/paper/2010/hash/677e09724f0e2df9b6c000b75b5da10d-Abstract.html",
      "year": 2010,
      "cited_by": 498,
      "authors": ["Hugo Larochelle", "Geoffrey Hinton"],
      "description": "We describe a model based on a Boltzmann machine with third-order connections that can learn how to accumulate information about a shape over several fixations. The model uses a retina that only has enough high resolution pixels to cover a small area of the image, so it must decide on a sequence of fixations and it must combine the glimpse\" at each fixation with the location of the fixation before integrating the information with information from other glimpses of the same object. We evaluate this model on a synthetic dataset and two image classification datasets, showing that it can perform at least as well as a model trained on whole images.\"",
      "citation_histogram": [
        [2010, 2],
        [2011, 9],
        [2012, 10],
        [2013, 7],
        [2014, 18],
        [2015, 13],
        [2016, 31],
        [2017, 36],
        [2018, 61],
        [2019, 73],
        [2020, 76],
        [2021, 98],
        [2022, 60]
      ],
      "detail_extracted": true
    },
    {
      "title": "SMEM algorithm for mixture models",
      "link": "https://direct.mit.edu/neco/article-abstract/12/9/2109/6442",
      "year": 2000,
      "cited_by": 497,
      "authors": [
        "Naonori Ueda",
        "Ryohei Nakano",
        "Zoubin Ghahramani",
        "Geoffrey E Hinton"
      ],
      "description": " We present a split-and-merge expectation-maximization (SMEM) algorithm to overcome the local maxima problem in parameter estimation of finite mixture models. In the case of mixture models, local maxima often involve having too many components of a mixture model in one part of the space and too few in another, widely separated part of the space. To escape from such configurations, we repeatedly perform simultaneous split-and-merge operations using a new criterion for efficiently selecting the split-and-merge candidates. We apply the proposed algorithm to the training of gaussian mixtures and mixtures of factor analyzers using synthetic and real data and show the effectiveness of using the split- and-merge operations to improve the likelihood of both the training data and of held-out test data. We also show the practical usefulness of the proposed algorithm by applying it to image compression and\u00a0\u2026",
      "citation_histogram": [
        [1999, 5],
        [2000, 11],
        [2001, 16],
        [2002, 20],
        [2003, 25],
        [2004, 33],
        [2005, 25],
        [2006, 33],
        [2007, 23],
        [2008, 29],
        [2009, 29],
        [2010, 24],
        [2011, 19],
        [2012, 30],
        [2013, 31],
        [2014, 23],
        [2015, 26],
        [2016, 19],
        [2017, 19],
        [2018, 11],
        [2019, 15],
        [2020, 9],
        [2021, 12],
        [2022, 5]
      ],
      "detail_extracted": true
    },
    {
      "title": "Application of deep belief networks for natural language understanding",
      "link": "https://ieeexplore.ieee.org/abstract/document/6737243/",
      "year": 2014,
      "cited_by": 496,
      "authors": ["Ruhi Sarikaya", "Geoffrey E Hinton", "Anoop Deoras"],
      "description": "Applications of Deep Belief Nets (DBN) to various problems have been the subject of a number of recent studies ranging from image classification and speech recognition to audio classification. In this study we apply DBNs to a natural language understanding problem. The recent surge of activity in this area was largely spurred by the development of a greedy layer-wise pretraining method that uses an efficient learning algorithm called Contrastive Divergence (CD). CD allows DBNs to learn a multi-layer generative model from unlabeled data and the features discovered by this model are then used to initialize a feed-forward neural network which is fine-tuned with backpropagation. We compare a DBN-initialized neural network to three widely used text classification algorithms: Support Vector Machines (SVM), boosting and Maximum Entropy (MaxEnt). The plain DBN-based model gives a call-routing classification\u00a0\u2026",
      "citation_histogram": [
        [2014, 2],
        [2015, 25],
        [2016, 59],
        [2017, 68],
        [2018, 86],
        [2019, 65],
        [2020, 82],
        [2021, 73],
        [2022, 33]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parallel visual computation",
      "link": "https://www.nature.com/articles/306021a0",
      "year": 1983,
      "cited_by": 495,
      "authors": [
        "Dana H Ballard",
        "Geoffrey E Hinton",
        "Terrence J Sejnowski"
      ],
      "description": "The functional abilities and parallel architecture of the human visual system are a rich source of ideas about visual processing. Any visual task that we can perform quickly and effortlessly is likely to have a computational solution using a parallel algorithm. Recently, several such parallel algorithms have been found that exploit information implicit in an image to compute intrinsic properties of surfaces, such as surface orientation, reflectance and depth. These algorithms require a computational architecture that has similarities to that of visual cortex in primates.",
      "citation_histogram": [
        [1984, 12],
        [1985, 32],
        [1986, 36],
        [1987, 25],
        [1988, 35],
        [1989, 53],
        [1990, 44],
        [1991, 24],
        [1992, 8],
        [1993, 13],
        [1994, 9],
        [1995, 7],
        [1996, 6],
        [1997, 8],
        [1998, 1],
        [1999, 14],
        [2000, 4],
        [2001, 3],
        [2002, 1],
        [2003, 3],
        [2004, 5],
        [2005, 3],
        [2006, 5],
        [2007, 8],
        [2008, 6],
        [2009, 10],
        [2010, 5],
        [2011, 7],
        [2012, 15],
        [2013, 13],
        [2014, 17],
        [2015, 4],
        [2016, 9],
        [2017, 6],
        [2018, 5],
        [2019, 8],
        [2020, 10],
        [2021, 6],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "A distributed connectionist production system",
      "link": "https://www.sciencedirect.com/science/article/pii/0364021388900298",
      "year": 1988,
      "cited_by": 485,
      "authors": ["David S Touretzky", "Geoffrey E Hinton"],
      "description": "DCPS is a connectionist production system interpreter that uses distributed representations. As a connectionist model it consists of many simple, richly interconnected neuron-like computing units that cooperate to solve problems in parallel. One motivation for constructing DCPS was to demonstrate that connectionist models are capable of representing and using explicit rules. A second motivation was to show how \u201ccoarse coding\u201d or \u201cdistributed representations\u201d can be used to construct a working memory that requires far fewer units than the number of different facts that can potentially be stored. The simulation we present is intended as a detailed demonstration of the feasibility of certain ideas and should not be viewed as a full implementation of production systems. Our current model only has a few of the many interesting emergent properties that we eventually hope to demonstrate: It is damage-resistant, it performs\u00a0\u2026",
      "citation_histogram": [
        [1987, 4],
        [1988, 5],
        [1989, 39],
        [1990, 36],
        [1991, 23],
        [1992, 38],
        [1993, 62],
        [1994, 26],
        [1995, 27],
        [1996, 23],
        [1997, 16],
        [1998, 13],
        [1999, 15],
        [2000, 14],
        [2001, 12],
        [2002, 5],
        [2003, 8],
        [2004, 7],
        [2005, 6],
        [2006, 1],
        [2007, 7],
        [2008, 10],
        [2009, 6],
        [2010, 7],
        [2011, 4],
        [2012, 3],
        [2013, 17],
        [2014, 11],
        [2015, 2],
        [2016, 4],
        [2017, 1],
        [2018, 3],
        [2019, 3],
        [2020, 5],
        [2021, 5],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Modeling the manifolds of images of handwritten digits",
      "link": "https://ieeexplore.ieee.org/abstract/document/554192/",
      "year": 1997,
      "cited_by": 473,
      "authors": ["Geoffrey E Hinton", "Peter Dayan", "Michael Revow"],
      "description": "This paper describes two new methods for modeling the manifolds of digitized images of handwritten digits. The models allow a priori information about the structure of the manifolds to be combined with empirical data. Accurate modeling of the manifolds allows digits to be discriminated using the relative probability densities under the alternative models. One of the methods is grounded in principal components analysis, the other in factor analysis. Both methods are based on locally linear low-dimensional approximations to the underlying data manifold. Links with other methods that model the manifold are discussed.",
      "citation_histogram": [
        [1996, 3],
        [1997, 8],
        [1998, 24],
        [1999, 16],
        [2000, 35],
        [2001, 27],
        [2002, 22],
        [2003, 24],
        [2004, 26],
        [2005, 14],
        [2006, 19],
        [2007, 22],
        [2008, 26],
        [2009, 20],
        [2010, 26],
        [2011, 17],
        [2012, 17],
        [2013, 14],
        [2014, 16],
        [2015, 17],
        [2016, 6],
        [2017, 17],
        [2018, 16],
        [2019, 13],
        [2020, 4],
        [2021, 10],
        [2022, 5]
      ],
      "detail_extracted": true
    },
    {
      "title": "Distilling a neural network into a soft decision tree",
      "link": "https://arxiv.org/abs/1711.09784",
      "year": 2017,
      "cited_by": 471,
      "authors": ["Nicholas Frosst", "Geoffrey Hinton"],
      "description": "Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.",
      "citation_histogram": [
        [2018, 50],
        [2019, 83],
        [2020, 118],
        [2021, 135],
        [2022, 80]
      ],
      "detail_extracted": true
    },
    {
      "title": "Lookahead optimizer: k steps forward, 1 step back",
      "link": "https://proceedings.neurips.cc/paper/2019/hash/90fd4f88f588ae64038134f1eeaa023f-Abstract.html",
      "year": 2019,
      "cited_by": 464,
      "authors": [
        "Michael Zhang",
        "James Lucas",
        "Jimmy Ba",
        "Geoffrey E Hinton"
      ],
      "description": "The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches:(1) adaptive learning rate schemes, such as AdaGrad and Adam and (2) accelerated schemes, such as heavy-ball and Nesterov momentum. In this paper, we propose a new optimization algorithm, Lookahead, that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by looking ahead at the sequence of``fast weights\" generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings on ImageNet, CIFAR-10/100, neural machine translation, and Penn Treebank.",
      "citation_histogram": [
        [2019, 14],
        [2020, 120],
        [2021, 190],
        [2022, 137]
      ],
      "detail_extracted": true
    },
    {
      "title": "Factored conditional restricted Boltzmann machines for modeling motion style",
      "link": "https://dl.acm.org/doi/abs/10.1145/1553374.1553505",
      "year": 2009,
      "cited_by": 456,
      "authors": ["Graham W Taylor", "Geoffrey E Hinton"],
      "description": "The Conditional Restricted Boltzmann Machine (CRBM) is a recently proposed model for time series that has a rich, distributed hidden state and permits simple, exact inference. We present a new model, based on the CRBM that preserves its most important computational properties and includes multiplicative three-way interactions that allow the effective interaction weight between two units to be modulated by the dynamic state of a third unit. We factor the three-way weight tensor implied by the multiplicative model, reducing the number of parameters from O (N 3) to O (N 2). The result is an efficient, compact model whose effectiveness we demonstrate by modeling human motion. Like the CRBM, our model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve the model's ability to blend motion styles or to transition smoothly among them.",
      "citation_histogram": [
        [2009, 5],
        [2010, 14],
        [2011, 21],
        [2012, 17],
        [2013, 20],
        [2014, 49],
        [2015, 62],
        [2016, 41],
        [2017, 54],
        [2018, 42],
        [2019, 36],
        [2020, 35],
        [2021, 36],
        [2022, 19]
      ],
      "detail_extracted": true
    },
    {
      "title": "Similarity of neural network representations revisited",
      "link": "http://proceedings.mlr.press/v97/kornblith19a.html",
      "year": 2019,
      "cited_by": 450,
      "authors": [
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Honglak Lee",
        "Geoffrey Hinton"
      ],
      "description": "Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.",
      "citation_histogram": [
        [2019, 23],
        [2020, 75],
        [2021, 181],
        [2022, 169]
      ],
      "detail_extracted": true
    },
    {
      "title": "Mapping part-whole hierarchies into connectionist networks",
      "link": "https://www.sciencedirect.com/science/article/pii/000437029090004J",
      "year": 1990,
      "cited_by": 446,
      "authors": ["Geoffrey E Hinton"],
      "description": "Three different ways of mapping part-whole hierarchies into connectionist networks are described. The simplest scheme uses a fixed mapping and is inadequate for most tasks because it fails to share units and connections between different pieces of the part-whole hierarchy. Two alternative schemes are described, each of which involves a different method of time-sharing connections and units. The scheme we finally arrive at suggests that neural networks have two quite different methods for performing inference. Simple \u201cintuitive\u201d inferences can be performed by a single settling of a network without changing the way in which the world is mapped into the network. More complex \u201crational\u201d inferences involve a sequence of such settlings with mapping changes after each settling.",
      "citation_histogram": [
        [1991, 6],
        [1992, 7],
        [1993, 16],
        [1994, 20],
        [1995, 20],
        [1996, 18],
        [1997, 20],
        [1998, 15],
        [1999, 9],
        [2000, 16],
        [2001, 21],
        [2002, 44],
        [2003, 9],
        [2004, 6],
        [2005, 8],
        [2006, 8],
        [2007, 13],
        [2008, 9],
        [2009, 11],
        [2010, 9],
        [2011, 5],
        [2012, 9],
        [2013, 13],
        [2014, 18],
        [2015, 9],
        [2016, 9],
        [2017, 10],
        [2018, 17],
        [2019, 13],
        [2020, 17],
        [2021, 19],
        [2022, 12]
      ],
      "detail_extracted": true
    },
    {
      "title": "Attend, infer, repeat: Fast scene understanding with generative models",
      "link": "https://proceedings.neurips.cc/paper/2016/hash/52947e0ade57a09e4a1386d08f17b656-Abstract.html",
      "year": 2016,
      "cited_by": 443,
      "authors": [
        "SM Eslami",
        "Nicolas Heess",
        "Theophane Weber",
        "Yuval Tassa",
        "David Szepesvari",
        "Geoffrey E Hinton"
      ],
      "description": "We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects-counting, locating and classifying the elements of a scene-without any supervision, eg, decomposing 3D images with various numbers of objects in a single forward pass of a neural network at unprecedented speed. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.",
      "citation_histogram": [
        [2016, 29],
        [2017, 28],
        [2018, 52],
        [2019, 83],
        [2020, 89],
        [2021, 98],
        [2022, 55]
      ],
      "detail_extracted": true
    },
    {
      "title": "Binary coding of speech spectrograms using a deep auto-encoder",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.185.1908&rep=rep1&type=pdf",
      "year": 2010,
      "cited_by": 442,
      "authors": [
        "Li Deng",
        "Michael L Seltzer",
        "Dong Yu",
        "Alex Acero",
        "Abdel-rahman Mohamed",
        "Geoff Hinton"
      ],
      "description": "This paper reports our recent exploration of the layer-by-layer learning strategy for training a multi-layer generative model of patches of speech spectrograms. The top layer of the generative model learns binary codes that can be used for efficient compression of speech and could also be used for scalable speech recognition or rapid speech content retrieval. Each layer of the generative model is fully connected to the layer below and the weights on these connections are pretrained efficiently by using the contrastive divergence approximation to the log likelihood gradient. After layer-bylayer pre-training we \u201cunroll\u201d the generative model to form a deep auto-encoder, whose parameters are then fine-tuned using back-propagation. To reconstruct the full-length speech spectrogram, individual spectrogram segments predicted by their respective binary codes are combined using an overlapand-add method. Experimental\u00a0\u2026",
      "citation_histogram": [
        [2010, 6],
        [2011, 12],
        [2012, 12],
        [2013, 18],
        [2014, 33],
        [2015, 35],
        [2016, 48],
        [2017, 53],
        [2018, 56],
        [2019, 70],
        [2020, 38],
        [2021, 44],
        [2022, 14]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep learning\u2014a technology with the potential to transform health care",
      "link": "https://jamanetwork.com/journals/jama/article-abstract/2701666",
      "year": 2018,
      "cited_by": 440,
      "authors": ["Geoffrey Hinton"],
      "description": "Widespread application of artificial intelligence in health care has been anticipated for half a century. For most of that time, the dominant approach to artificial intelligence was inspired by logic: researchers assumed that the essence of intelligence was manipulating symbolic expressions, using rules of inference. This approach produced expert systems and graphical models that attemptedtoautomatethereasoningprocessesofexperts. Inthelastdecade, however, aradicallydifferentapproach toartificialintelligence, calleddeeplearning, hasproduced majorbreakthroughsandisnowusedonbillionsofdigital devices for complex tasks such as speech recognition, image interpretation, and language translation. The purpose of this Viewpoint is to give health care professionals an intuitive understanding of the technology underlying deep learning. In an accompanying Viewpoint, Naylor1 outlines some of the factors propelling\u00a0\u2026",
      "citation_histogram": [
        [2018, 18],
        [2019, 77],
        [2020, 127],
        [2021, 123],
        [2022, 91]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning to label aerial images from noisy data",
      "link": "https://www.cs.toronto.edu/~vmnih/docs/noisy_maps.pdf",
      "year": 2012,
      "cited_by": 433,
      "authors": ["Volodymyr Mnih", "Geoffrey E Hinton"],
      "description": "When training a system to label images, the amount of labeled training data tends to be a limiting factor. We consider the task of learning to label aerial images from existing maps. These provide abundant labels, but the labels are often incomplete and sometimes poorly registered. We propose two robust loss functions for dealing with these kinds of label noise and use the loss functions to train a deep neural network on two challenging aerial image datasets. The robust loss functions lead to big improvements in performance and our best system substantially outperforms the best published results on the task we consider.",
      "citation_histogram": [
        [2013, 4],
        [2014, 9],
        [2015, 23],
        [2016, 39],
        [2017, 50],
        [2018, 62],
        [2019, 66],
        [2020, 72],
        [2021, 65],
        [2022, 30]
      ],
      "detail_extracted": true
    },
    {
      "title": "Implementing semantic networks in parallel hardware",
      "link": "https://ci.nii.ac.jp/naid/10007991166/",
      "year": 1981,
      "cited_by": 428,
      "authors": ["Geoffrey E Hinton"],
      "description": "CiNii \u8ad6\u6587 - Implementing Semantic Networks in Parallel Hardware CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \n\u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\n\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005\nID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 \n[2022\u5e741\u6708\u7de0\u5207]CiNii Articles\u3078\u306eCiNii Research\u3078\u306e\u7d71\u5408\u306b\u4f34\u3046\u6a5f\u95a2\u8a8d\u8a3c\u306e\u79fb\u884c\u78ba\u8a8d\u306b\u3064\u3044\u3066 \n[1/6\u66f4\u65b0]2022\u5e744\u67081\u65e5\u304b\u3089\u306eCiNii Articles\u306eCiNii Research\u3078\u306e\u7d71\u5408\u306b\u3064\u3044\u3066 Implementing \nSemantic Networks in Parallel Hardware HINTON GE \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 HINTON GE \u53ce\u9332\n\u520a\u884c\u7269 Parallel Models of Association Memory Parallel Models of Association Memory, 1981 \nErlbaum \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u591a\u5c64\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u77ac\u6642\u5b66\u7fd2\u6cd5 \u585a\u672c \u7fa9\u660e , \u751f\u5929\u76ee \u7ae0 \n\u60c5\u5831\u51e6\u7406\u5b66\u4f1a\u8ad6\u6587\u8a8c 34(9), 1882-1891, 1993-09-15 \u53c2\u8003\u6587\u732e22\u4ef6 \u88ab\u5f15\u7528\u6587\u732e5\u4ef6 \u6559\u80b2\u6a5f\u95a2DX\u2026",
      "citation_histogram": [
        [1984, 10],
        [1985, 12],
        [1986, 12],
        [1987, 13],
        [1988, 23],
        [1989, 18],
        [1990, 22],
        [1991, 25],
        [1992, 23],
        [1993, 9],
        [1994, 13],
        [1995, 14],
        [1996, 8],
        [1997, 10],
        [1998, 10],
        [1999, 6],
        [2000, 10],
        [2001, 2],
        [2002, 10],
        [2003, 13],
        [2004, 8],
        [2005, 8],
        [2006, 8],
        [2007, 7],
        [2008, 8],
        [2009, 8],
        [2010, 12],
        [2011, 7],
        [2012, 11],
        [2013, 16],
        [2014, 7],
        [2015, 11],
        [2016, 7],
        [2017, 7],
        [2018, 3],
        [2019, 4],
        [2020, 8],
        [2021, 6],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "To recognize shapes, first learn to generate images",
      "link": "https://www.sciencedirect.com/science/article/pii/S0079612306650346",
      "year": 2007,
      "cited_by": 427,
      "authors": ["Geoffrey E Hinton"],
      "description": "The uniformity of the cortical architecture and the ability of functions to move to different areas of cortex following early damage strongly suggest that there is a single basic learning algorithm for extracting underlying structure from richly structured, high-dimensional sensory data. There have been many attempts to design such an algorithm, but until recently they all suffered from serious computational weaknesses. This chapter describes several of the proposed algorithms and shows how they can be combined to produce hybrid methods that work efficiently in networks with many layers and millions of adaptive connections.",
      "citation_histogram": [
        [2006, 3],
        [2007, 6],
        [2008, 8],
        [2009, 13],
        [2010, 25],
        [2011, 16],
        [2012, 18],
        [2013, 55],
        [2014, 24],
        [2015, 41],
        [2016, 32],
        [2017, 37],
        [2018, 40],
        [2019, 39],
        [2020, 19],
        [2021, 31],
        [2022, 17]
      ],
      "detail_extracted": true
    },
    {
      "title": "3D object recognition with deep belief nets",
      "link": "https://proceedings.neurips.cc/paper/2009/hash/6e7b33fdea3adc80ebd648fffb665bb8-Abstract.html",
      "year": 2009,
      "cited_by": 423,
      "authors": ["Vinod Nair", "Geoffrey E Hinton"],
      "description": "We introduce a new type of Deep Belief Net and evaluate it on a 3D object recognition task. The top-level model is a third-order Boltzmann machine, trained using a hybrid algorithm that combines both generative and discriminative gradients. Performance is evaluated on the NORB database (normalized-uniform version), which contains stereo-pair images of objects under different lighting conditions and viewpoints. Our model achieves 6.5% error on the test set, which is close to the best published result for NORB (5.9%) using a convolutional neural net that has built-in knowledge of translation invariance. It substantially outperforms shallow models such as SVMs (11.6%). DBNs are especially suited for semi-supervised learning, and to demonstrate this we consider a modified version of the NORB recognition task in which additional unlabeled images are created by applying small translations to the images in the database. With the extra unlabeled data (and the same amount of labeled data as before), our model achieves 5.2% error, making it the current best result for NORB.",
      "citation_histogram": [
        [2010, 19],
        [2011, 26],
        [2012, 19],
        [2013, 23],
        [2014, 52],
        [2015, 52],
        [2016, 48],
        [2017, 48],
        [2018, 35],
        [2019, 31],
        [2020, 28],
        [2021, 21],
        [2022, 13]
      ],
      "detail_extracted": true
    },
    {
      "title": "Phone recognition with the mean-covariance restricted Boltzmann machine",
      "link": "https://proceedings.neurips.cc/paper/2010/hash/b73ce398c39f506af761d2277d853a92-Abstract.html",
      "year": 2010,
      "cited_by": 422,
      "authors": [
        "George Dahl",
        "Marc'Aurelio Ranzato",
        "Abdel-rahman Mohamed",
        "Geoffrey E Hinton"
      ],
      "description": "Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonal-covariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), first introduced for modeling natural images, is a much more representationally efficient and powerful way of modeling the covariance structure of speech data. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5\\%, which is superior to all published results on speaker-independent TIMIT to date.",
      "citation_histogram": [
        [2010, 2],
        [2011, 30],
        [2012, 25],
        [2013, 28],
        [2014, 44],
        [2015, 33],
        [2016, 48],
        [2017, 42],
        [2018, 47],
        [2019, 35],
        [2020, 35],
        [2021, 28],
        [2022, 18]
      ],
      "detail_extracted": true
    },
    {
      "title": "Neuroanimator: Fast neural network emulation and control of physics-based models",
      "link": "https://dl.acm.org/doi/pdf/10.1145/280814.280816",
      "year": 1998,
      "cited_by": 401,
      "authors": ["Radek Grzeszczuk", "Demetri Terzopoulos", "Geoffrey Hinton"],
      "description": "Animation through the numerical simulation of physicsbased graphics models offers unsurpassed realism, but it can be computationally demanding. Likewise, the search for controllers that enable physics-based models to produce desired animations usually entails formidable computational cost. This paper demonstrates the possibility of replacing the numerical simulation and control of dynamic models with a dramatically more efficient alternative. In particular, we propose the NeuroAnimator, a novel approach to creating physically realistic animation that exploits neural networks. NeuroAnimators are automatically trained off-line to emulate physical dynamics through the observation of physicsbased models in action. Depending on the model, its neural network emulator can yield physically realistic animation one or two orders of magnitude faster than conventional numerical simulation. Furthermore, by exploiting\u00a0\u2026",
      "citation_histogram": [
        [1998, 2],
        [1999, 16],
        [2000, 21],
        [2001, 16],
        [2002, 33],
        [2003, 30],
        [2004, 26],
        [2005, 20],
        [2006, 24],
        [2007, 16],
        [2008, 14],
        [2009, 14],
        [2010, 14],
        [2011, 10],
        [2012, 8],
        [2013, 11],
        [2014, 5],
        [2015, 6],
        [2016, 7],
        [2017, 15],
        [2018, 20],
        [2019, 16],
        [2020, 15],
        [2021, 23],
        [2022, 18]
      ],
      "detail_extracted": true
    },
    {
      "title": "An alternative model for mixtures of experts",
      "link": "https://proceedings.neurips.cc/paper/1994/hash/c8fbbc86abe8bd6a5eb6a3b4d0411301-Abstract.html",
      "year": 1994,
      "cited_by": 400,
      "authors": ["Lei Xu", "Michael Jordan", "Geoffrey E Hinton"],
      "description": "We propose an alternative model for mixtures of experts which uses a different parametric form for the gating network. The modified model is trained by the EM algorithm. In comparison with earlier models-trained by either EM or gradient ascent-there is no need to select a learning stepsize. We report simulation experiments which show that the new architecture yields faster convergence. We also apply the new model to two problem domains: piecewise nonlinear function approximation and the combination of multiple previously trained classifiers.",
      "citation_histogram": [
        [1994, 3],
        [1995, 6],
        [1996, 25],
        [1997, 27],
        [1998, 13],
        [1999, 23],
        [2000, 15],
        [2001, 10],
        [2002, 13],
        [2003, 14],
        [2004, 12],
        [2005, 18],
        [2006, 11],
        [2007, 17],
        [2008, 14],
        [2009, 11],
        [2010, 18],
        [2011, 7],
        [2012, 8],
        [2013, 12],
        [2014, 12],
        [2015, 16],
        [2016, 10],
        [2017, 15],
        [2018, 15],
        [2019, 13],
        [2020, 10],
        [2021, 20],
        [2022, 8]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep belief networks using discriminative features for phone recognition",
      "link": "https://ieeexplore.ieee.org/abstract/document/5947494/",
      "year": 2011,
      "cited_by": 393,
      "authors": [
        "Abdel-rahman Mohamed",
        "Tara N Sainath",
        "George Dahl",
        "Bhuvana Ramabhadran",
        "Geoffrey E Hinton",
        "Michael A Picheny"
      ],
      "description": "Deep Belief Networks (DBNs) are multi-layer generative models. They can be trained to model windows of coefficients extracted from speech and they discover multiple layers of features that capture the higher-order statistical structure of the data. These features can be used to initialize the hidden units of a feed-forward neural network that is then trained to predict the HMM state for the central frame of the window. Initializing with features that are good at generating speech makes the neural network perform much better than initializing with random weights. DBNs have already been used successfully for phone recognition with input coefficients that are MFCCs or filterbank outputs. In this paper, we demonstrate that they work even better when their inputs are speaker adaptive, discriminative features. On the standard TIMIT corpus, they give phone error rates of 19.6% using monophone HMMs and a bigram language\u00a0\u2026",
      "citation_histogram": [
        [2011, 3],
        [2012, 16],
        [2013, 23],
        [2014, 42],
        [2015, 51],
        [2016, 73],
        [2017, 35],
        [2018, 41],
        [2019, 34],
        [2020, 30],
        [2021, 28],
        [2022, 13]
      ],
      "detail_extracted": true
    },
    {
      "title": "Vocal tract length perturbation (VTLP) improves speech recognition",
      "link": "https://www.cs.utoronto.ca/~hinton/absps/perturb.pdf",
      "year": 2013,
      "cited_by": 391,
      "authors": ["Navdeep Jaitly", "Geoffrey E Hinton"],
      "description": "Augmenting datasets by transforming inputs in a way that does not change the label is a crucial ingredient of the state of the art methods for object recognition using neural networks. However this approach has (to our knowledge) not been exploited successfully in speech recognition (with or without neural networks). In this paper we lay the foundation for this approach, and show one way of augmenting speech datasets by transforming spectrograms, using a random linear warping along the frequency dimension. In practice this can be achieved by using warping techniques that are used for vocal tract length normalization (VTLN)-with the difference that a warp factor is generated randomly each time, during training, rather than fitting a single warp factor to each training and test speaker (or utterance). At test time, a prediction is made by averaging the predictions over multiple warp factors. When this technique is applied to TIMIT using Deep Neural Networks (DNN) of different depths, the Phone Error Rate (PER) improved by an average of 0.65% on the test set. For a Convolutional neural network (CNN) with convolutional layer in the bottom, a gain of 1.0% was observed. These improvements were achieved without increasing the number of training epochs, and suggest that data transformations should be an important component of training neural networks for speech, especially for data limited projects.",
      "citation_histogram": [
        [2013, 1],
        [2014, 8],
        [2015, 16],
        [2016, 21],
        [2017, 29],
        [2018, 42],
        [2019, 67],
        [2020, 59],
        [2021, 76],
        [2022, 69]
      ],
      "detail_extracted": true
    },
    {
      "title": "Speech recognition with deep recurrent neural networks. Acoustics, speech and signal processing (icassp)",
      "link": "https://scholar.google.com/scholar?cluster=16456034524845799036&hl=en&oi=scholarr",
      "year": 2013,
      "cited_by": 386,
      "authors": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"],
      "description": null,
      "citation_histogram": [
        [2014, 1],
        [2015, 7],
        [2016, 19],
        [2017, 39],
        [2018, 63],
        [2019, 75],
        [2020, 68],
        [2021, 79],
        [2022, 31]
      ],
      "detail_extracted": true
    },
    {
      "title": "Backpropagation and the brain",
      "link": "https://www.nature.com/articles/s41583-020-0277-3",
      "year": 2020,
      "cited_by": 380,
      "authors": [
        "Timothy P Lillicrap",
        "Adam Santoro",
        "Luke Marris",
        "Colin J Akerman",
        "Geoffrey Hinton"
      ],
      "description": "During learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on\u00a0\u2026",
      "citation_histogram": [
        [2019, 3],
        [2020, 68],
        [2021, 168],
        [2022, 138]
      ],
      "detail_extracted": true
    },
    {
      "title": "Using fast weights to improve persistent contrastive divergence",
      "link": "https://dl.acm.org/doi/abs/10.1145/1553374.1553506",
      "year": 2009,
      "cited_by": 379,
      "authors": ["Tijmen Tieleman", "Geoffrey Hinton"],
      "description": "The most commonly used learning algorithm for restricted Boltzmann machines is contrastive divergence which starts a Markov chain at a data point and runs the chain for only a few iterations to get a cheap, low variance estimate of the sufficient statistics under the model. Tieleman (2008) showed that better learning can be achieved by estimating the model's statistics using a small set of persistent\" fantasy particles\" that are not reinitialized to data points after each weight update. With sufficiently small weight updates, the fantasy particles represent the equilibrium distribution accurately but to explain why the method works with much larger weight updates it is necessary to consider the interaction between the weight updates and the Markov chain. We show that the weight updates force the Markov chain to mix fast, and using this insight we develop an even faster mixing chain that uses an auxiliary set of\" fast weights\u00a0\u2026",
      "citation_histogram": [
        [2009, 3],
        [2010, 22],
        [2011, 16],
        [2012, 18],
        [2013, 21],
        [2014, 41],
        [2015, 42],
        [2016, 50],
        [2017, 44],
        [2018, 18],
        [2019, 35],
        [2020, 25],
        [2021, 28],
        [2022, 12]
      ],
      "detail_extracted": true
    },
    {
      "title": "Understanding how deep belief networks perform acoustic modelling",
      "link": "https://ieeexplore.ieee.org/abstract/document/6288863/",
      "year": 2012,
      "cited_by": 342,
      "authors": ["Abdel-rahman Mohamed", "Geoffrey Hinton", "Gerald Penn"],
      "description": "Deep Belief Networks (DBNs) are a very competitive alternative to Gaussian mixture models for relating states of a hidden Markov model to frames of coefficients derived from the acoustic input. They are competitive for three reasons: DBNs can be fine-tuned as neural networks; DBNs have many non-linear hidden layers; and DBNs are generatively pre-trained. This paper illustrates how each of these three aspects contributes to the DBN's good recognition performance using both phone recognition performance on the TIMIT corpus and a dimensionally reduced visualization of the relationships between the feature vectors learned by the DBNs that preserves the similarity structure of the feature vectors at multiple scales. The same two methods are also used to investigate the most suitable type of input representation for a DBN.",
      "citation_histogram": [
        [2012, 5],
        [2013, 28],
        [2014, 39],
        [2015, 47],
        [2016, 41],
        [2017, 37],
        [2018, 40],
        [2019, 31],
        [2020, 35],
        [2021, 26],
        [2022, 11]
      ],
      "detail_extracted": true
    },
    {
      "title": "Some demonstrations of the effects of structural descriptions in mental imagery",
      "link": "https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog0303_3",
      "year": 1979,
      "cited_by": 326,
      "authors": ["Geoffrey Hinton"],
      "description": "A visual imagery task is presented which is beyond the limits of normal human ability, and some of the factors contributing to its difficulty are isolated by comparing the difficulty of related tasks. It is argued that complex objects are assigned hierarchical structural descriptions by being parsed into parts, each of which has its own local system of significant directions. Two quite different schemas for a wire\u2010frame cube are used to illustrate this theory, and some striking perceptual differences to which they give rise are described. The difficulty of certain mental imagery tasks is shown to depend on which of the alternative structural descriptions of an object is used, and this is interpreted as evidence that structural descriptions are an important component of mental images. Finally, it is argued that analog transformations like mental folding involve changing the values of continuous variables in a structural description.",
      "citation_histogram": [
        [1983, 8],
        [1984, 1],
        [1985, 6],
        [1986, 5],
        [1987, 2],
        [1988, 12],
        [1989, 7],
        [1990, 10],
        [1991, 5],
        [1992, 10],
        [1993, 13],
        [1994, 12],
        [1995, 3],
        [1996, 6],
        [1997, 6],
        [1998, 5],
        [1999, 4],
        [2000, 4],
        [2001, 2],
        [2002, 40],
        [2003, 5],
        [2004, 3],
        [2005, 3],
        [2006, 2],
        [2007, 2],
        [2008, 3],
        [2009, 3],
        [2010, 2],
        [2011, 1],
        [2012, 7],
        [2013, 10],
        [2014, 3],
        [2015, 1],
        [2016, 4],
        [2017, 6],
        [2018, 4],
        [2019, 3],
        [2020, 19],
        [2021, 9]
      ],
      "detail_extracted": true
    },
    {
      "title": "Large scale distributed neural network training through online distillation",
      "link": "https://arxiv.org/abs/1804.03235",
      "year": 2018,
      "cited_by": 318,
      "authors": [
        "Rohan Anil",
        "Gabriel Pereyra",
        "Alexandre Passos",
        "Robert Ormandi",
        "George E Dahl",
        "Geoffrey E Hinton"
      ],
      "description": "Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing  tokens and based on the Common Crawl repository of web data.",
      "citation_histogram": [
        [2018, 20],
        [2019, 53],
        [2020, 83],
        [2021, 81],
        [2022, 79]
      ],
      "detail_extracted": true
    },
    {
      "title": "Generative models for discovering sparse distributed representations",
      "link": "https://royalsocietypublishing.org/doi/abs/10.1098/rstb.1997.0101",
      "year": 1997,
      "cited_by": 311,
      "authors": ["Geoffrey E Hinton", "Zoubin Ghahramani"],
      "description": "We describe a hierarchical, generative model that can be viewed as a nonlinear generalization of factor analysis and can be implemented in a neural network. The model uses bottom\u2013up, top\u2013down and lateral connections to perform Bayesian perceptual inference correctly. Once perceptual inference has been performed the connection strengths can be updated using a very simple learning rule that only requires locally available information. We demonstrate that the network learns to extract sparse, distributed, hierarchical representations.",
      "citation_histogram": [
        [1997, 12],
        [1998, 14],
        [1999, 32],
        [2000, 14],
        [2001, 9],
        [2002, 24],
        [2003, 13],
        [2004, 13],
        [2005, 8],
        [2006, 13],
        [2007, 18],
        [2008, 11],
        [2009, 10],
        [2010, 6],
        [2011, 4],
        [2012, 5],
        [2013, 43],
        [2014, 5],
        [2015, 7],
        [2016, 9],
        [2017, 5],
        [2018, 5],
        [2019, 5],
        [2020, 11],
        [2021, 4],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "A parallel computation that assigns canonical object-based frames of reference",
      "link": "https://dl.acm.org/doi/abs/10.5555/1623264.1623282",
      "year": 1981,
      "cited_by": 310,
      "authors": ["Geoffrey E Hinton"],
      "description": "A viewpoint-independent description of the shape of an object can be generated by imposing a canonical frame of reference on the object and describing the spatial dispositions of the parts relative to this object-based frame. When a familiar object is in an unusual orientation, the deciding factor in the choice of the canonical object-based frame may be the fact that relative to this frame the object has a familiar shape description. This may suggest that we first hypothesise an object-based frame and then test the resultant shape description for familiarity. However, it is possible to organise the interactions between units in a parallel network so that the pattern of activity in the network simultaneously converges on a representation of the shape and a representation of the object-based frame of reference. The connections in the network are determined by the constraints inherent in the image formation process.",
      "citation_histogram": [
        [1983, 3],
        [1984, 6],
        [1985, 7],
        [1986, 2],
        [1987, 9],
        [1988, 7],
        [1989, 38],
        [1990, 7],
        [1991, 9],
        [1992, 9],
        [1993, 6],
        [1994, 9],
        [1995, 7],
        [1996, 8],
        [1997, 7],
        [1998, 8],
        [1999, 19],
        [2000, 1],
        [2001, 3],
        [2002, 7],
        [2003, 3],
        [2004, 6],
        [2005, 5],
        [2006, 3],
        [2007, 3],
        [2008, 10],
        [2009, 3],
        [2010, 5],
        [2011, 6],
        [2012, 2],
        [2013, 5],
        [2014, 3],
        [2015, 5],
        [2016, 5],
        [2017, 10],
        [2018, 12],
        [2019, 12],
        [2020, 13],
        [2021, 15],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Massively parallel architectures for Al: NETL, Thistle, and Boltzmann machines",
      "link": "https://www.researchgate.net/profile/Terrence_Sejnowski/publication/221605073_Massively_Parallel_Architectures_for_AI_NETL_Thistle_and_Boltzmann_Machines/links/54a4b0170cf257a636072712.pdf",
      "year": 1983,
      "cited_by": 308,
      "authors": [
        "Scott E Fahlman",
        "Geoffrey E Hinton",
        "Terrence J Sejnowski"
      ],
      "description": "It is becoming increasingly apparent that some aspects of intelligent behavior require enormous computational power and that some sort of massively parallel computing architecture is the most plausible way to deliver sueh power. Parallelism, rather than raw speed of the computing elements. seems to be the way that the brain gets such jobs done. But even if the need for massive parallelism is admitted, there is still the question of what kind of parallel architecture best fits the needs of various A1 tasks.In this paper we will attempt to isolate a number of basic computational tasks that an intelligent system must perform. We will describe several families of massively parallel computing architectures, and we will see which of these computational tasks can be handled by each of these families. In particular, we will describe a new architecture, which we call the Boltzmann machine, whose abilities appear to include a number of tasks that are inefficient or impossible on the other architectures.",
      "citation_histogram": [
        [1984, 7],
        [1985, 14],
        [1986, 12],
        [1987, 19],
        [1988, 13],
        [1989, 20],
        [1990, 12],
        [1991, 9],
        [1992, 11],
        [1993, 6],
        [1994, 8],
        [1995, 3],
        [1996, 5],
        [1997, 3],
        [1998, 2],
        [1999, 3],
        [2000, 2],
        [2001, 3],
        [2002, 2],
        [2003, 5],
        [2004, 3],
        [2005, 7],
        [2006, 10],
        [2007, 15],
        [2008, 6],
        [2009, 6],
        [2010, 8],
        [2011, 7],
        [2012, 6],
        [2013, 2],
        [2014, 4],
        [2015, 2],
        [2016, 8],
        [2017, 15],
        [2018, 9],
        [2019, 7],
        [2020, 12],
        [2021, 10],
        [2022, 9]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning to represent spatial transformations with factored higher-order Boltzmann machines",
      "link": "https://direct.mit.edu/neco/article-abstract/22/6/1473/7548",
      "year": 2010,
      "cited_by": 305,
      "authors": ["Roland Memisevic", "Geoffrey E Hinton"],
      "description": " To allow the hidden units of a restricted Boltzmann machine to model the transformation between two successive images, Memisevic and Hinton  introduced three-way multiplicative interactions that use the intensity of a pixel in the first image as a multiplicative gain on a learned, symmetric weight between a pixel in the second image and a hidden unit. This creates cubically many parameters, which form a three-dimensional interaction tensor. We describe a low-rank approximation to this interaction tensor that uses a sum of factors, each of which is a three-way outer product. This approximation allows efficient learning of transformations between larger image patches. Since each factor can be viewed as an image filter, the model as a whole learns optimal filter pairs for efficiently representing transformations. We demonstrate the learning of optimal filter pairs from various synthetic and real image sequences. We\u00a0\u2026",
      "citation_histogram": [
        [2010, 12],
        [2011, 31],
        [2012, 15],
        [2013, 30],
        [2014, 23],
        [2015, 34],
        [2016, 34],
        [2017, 30],
        [2018, 27],
        [2019, 20],
        [2020, 17],
        [2021, 16],
        [2022, 10]
      ],
      "detail_extracted": true
    },
    {
      "title": "The development of the time-delay neural network architecture for speech recognition",
      "link": "https://cir.nii.ac.jp/crid/1570854174687654784",
      "year": 1988,
      "cited_by": 302,
      "authors": ["KJ Lang"],
      "description": null,
      "citation_histogram": [
        [1989, 6],
        [1990, 17],
        [1991, 10],
        [1992, 4],
        [1993, 8],
        [1994, 7],
        [1995, 14],
        [1996, 8],
        [1997, 6],
        [1998, 23],
        [1999, 10],
        [2000, 6],
        [2001, 10],
        [2002, 15],
        [2003, 12],
        [2004, 13],
        [2005, 14],
        [2006, 3],
        [2007, 18],
        [2008, 7],
        [2009, 5],
        [2010, 7],
        [2011, 9],
        [2012, 11],
        [2013, 6],
        [2014, 6],
        [2015, 3],
        [2016, 6],
        [2017, 8],
        [2018, 4],
        [2019, 4],
        [2020, 2],
        [2021, 11],
        [2022, 5]
      ],
      "detail_extracted": true
    },
    {
      "title": "Neurocomputing: Foundations of research",
      "link": "https://scholar.google.com/scholar?cluster=343198997852404094&hl=en&oi=scholarr",
      "year": 1988,
      "cited_by": 300,
      "authors": [
        "David E Rumelhart",
        "Geoffrey E Hinton",
        "Ronald J Williams"
      ],
      "description": null,
      "citation_histogram": [
        [2012, 6],
        [2013, 6],
        [2014, 7],
        [2015, 22],
        [2016, 41],
        [2017, 23],
        [2018, 63],
        [2019, 62],
        [2020, 42],
        [2021, 13],
        [2022, 10]
      ],
      "detail_extracted": true
    },
    {
      "title": "Global coordination of local linear models",
      "link": "https://proceedings.neurips.cc/paper/2001/hash/850af92f8d9903e7a4e0559a98ecc857-Abstract.html",
      "year": 2001,
      "cited_by": 295,
      "authors": ["Sam Roweis", "Lawrence Saul", "Geoffrey E Hinton"],
      "description": "High dimensional data that lies on or near a low dimensional manifold can be described by a collection of local linear models. Such a description, however, does not provide a global parameterization of the manifold\u2014arguably an important goal of unsupervised learning. In this paper, we show how to learn a collection of local linear models that solves this more difficult problem. Our local linear models are represented by a mixture of factor analyzers, and the \u201cglobal coordination\u201d of these models is achieved by adding a regularizing term to the standard maximum likelihood objective function. The regularizer breaks a degeneracy in the mixture model\u2019s parameter space, favoring models whose internal coordinate systems are aligned in a consistent way. As a result, the internal coordinates change smoothly and continuously as one traverses a connected path on the manifold\u2014even when the path crosses the domains of many different local models. The regularizer takes the form of a Kullback-Leibler divergence and illustrates an unexpected application of variational methods: not to perform approximate inference in intractable probabilistic models, but to learn more useful internal representations in tractable ones.",
      "citation_histogram": [
        [2001, 1],
        [2002, 11],
        [2003, 10],
        [2004, 9],
        [2005, 20],
        [2006, 19],
        [2007, 17],
        [2008, 16],
        [2009, 15],
        [2010, 24],
        [2011, 24],
        [2012, 11],
        [2013, 9],
        [2014, 25],
        [2015, 17],
        [2016, 23],
        [2017, 6],
        [2018, 8],
        [2019, 9],
        [2020, 10],
        [2021, 5],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Models of information processing in the brain",
      "link": "https://www.taylorfrancis.com/chapters/edit/10.4324/9781315807997-7/models-information-processing-brain-james-anderson-geoffrey-hinton-23",
      "year": 2014,
      "cited_by": 292,
      "authors": ["James A Anderson", "Geoffrey E Hinton"],
      "description": "If many computational operations are performed at once, a system can obviously operate faster. However, one part does not know what the other parts are  currently doing because the parts operate simultaneously. This causes serious  problems of coordination and lateral information transfer from one part of a  parallel system to another. These problems have made it hard to program general  purpose computers that execute many instructions at once though numerous  special purpose systems have been developed for specific tasks.",
      "citation_histogram": [
        [1983, 3],
        [1984, 8],
        [1985, 10],
        [1986, 12],
        [1987, 37],
        [1988, 37],
        [1989, 12],
        [1990, 15],
        [1991, 15],
        [1992, 10],
        [1993, 6],
        [1994, 9],
        [1995, 5],
        [1996, 5],
        [1997, 10],
        [1998, 6],
        [1999, 4],
        [2000, 6],
        [2001, 1],
        [2002, 4],
        [2003, 2],
        [2004, 3],
        [2005, 6],
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 2],
        [2011, 3],
        [2012, 12],
        [2013, 2],
        [2014, 3],
        [2015, 6],
        [2016, 1],
        [2017, 6],
        [2018, 5],
        [2019, 2],
        [2020, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Modeling pixel means and covariances using factorized third-order Boltzmann machines",
      "link": "https://ieeexplore.ieee.org/abstract/document/5539962/",
      "year": 2010,
      "cited_by": 292,
      "authors": ["Marc'Aurelio Ranzato", "Geoffrey E Hinton"],
      "description": "Learning a generative model of natural images is a useful way of extracting features that capture interesting regularities. Previous work on learning such models has focused on methods in which the latent features are used to determine the mean and variance of each pixel independently, or on methods in which the hidden units determine the covariance matrix of a zero-mean Gaussian distribution. In this work, we propose a probabilistic model that combines these two approaches into a single framework. We represent each image using one set of binary latent features that model the image-specific covariance and a separate set that model the mean. We show that this approach provides a probabilistic framework for the widely used simple-cell complex-cell architecture, it produces very realistic samples of natural images and it extracts features that yield state-of-the-art recognition accuracy on the challenging CIFAR\u00a0\u2026",
      "citation_histogram": [
        [2010, 12],
        [2011, 31],
        [2012, 33],
        [2013, 27],
        [2014, 35],
        [2015, 32],
        [2016, 32],
        [2017, 27],
        [2018, 19],
        [2019, 21],
        [2020, 9],
        [2021, 4],
        [2022, 5]
      ],
      "detail_extracted": true
    },
    {
      "title": "Factored 3-way restricted boltzmann machines for modeling natural images",
      "link": "https://proceedings.mlr.press/v9/ranzato10a.html",
      "year": 2010,
      "cited_by": 280,
      "authors": [
        "Marc\u2019Aurelio Ranzato",
        "Alex Krizhevsky",
        "Geoffrey Hinton"
      ],
      "description": "Deep belief nets have been successful in modeling handwritten characters, but it has proved more difficult to apply them to real images. The problem lies in the restricted Boltzmann machine (RBM) which is used as a module for learning deep belief nets one layer at a time. The Gaussian-Binary RBMs that have been used to model real-valued data are not a good way to model the covariance structure of natural images. We propose a factored 3-way RBM that uses the states of its hidden units to represent abnormalities in the local covariance structure of an image. This provides a probabilistic framework for the widely used simple/complex cell architecture. Our model learns binary features that work very well for object recognition on the \u201ctiny images\u201d data set. Even better features are obtained by then using standard binary RBM\u2019s to learn a deeper model.",
      "citation_histogram": [
        [2010, 11],
        [2011, 22],
        [2012, 17],
        [2013, 22],
        [2014, 36],
        [2015, 29],
        [2016, 34],
        [2017, 30],
        [2018, 22],
        [2019, 23],
        [2020, 11],
        [2021, 10],
        [2022, 6]
      ],
      "detail_extracted": true
    },
    {
      "title": "Using fast weights to deblur old memories",
      "link": "https://books.google.com/books?hl=en&lr=&id=KnVBTk-hS10C&oi=fnd&pg=PA177&dq=info:SxrkWsb1ZpoJ:scholar.google.com&ots=XQGeOZHIie&sig=q5xoigEOJiE_Rc49iYs88cRgEt4",
      "year": 1987,
      "cited_by": 277,
      "authors": ["Geoffrey E Hinton", "David C Plaut"],
      "description": "Connectionist models usually have a single weight on each connection. Some interesting new properties emerge if each connection has two weights: A slowly changing, plastic weight which stores long-term knowledge and a fast-changing, elastic weight which stores temporary knowledge and spontaneously decays towards zero. If a network learns a set of associations and then these associations are\" blurred\" by subsequent learning, all the original associations can be\" deblurred\" by rehearsing on just a few of them. The rehearsal allows the fast weights to take on values that temporarily cancel out the changes in the slow weights caused by the subsequent learning.",
      "citation_histogram": [
        [1988, 5],
        [1989, 5],
        [1990, 11],
        [1991, 7],
        [1992, 4],
        [1993, 6],
        [1994, 5],
        [1995, 8],
        [1996, 7],
        [1997, 4],
        [1998, 4],
        [1999, 8],
        [2000, 4],
        [2001, 3],
        [2002, 4],
        [2003, 2],
        [2004, 4],
        [2005, 5],
        [2006, 2],
        [2007, 5],
        [2008, 2],
        [2009, 3],
        [2010, 4],
        [2011, 1],
        [2012, 2],
        [2013, 7],
        [2014, 7],
        [2015, 5],
        [2016, 1],
        [2017, 11],
        [2018, 18],
        [2019, 26],
        [2020, 29],
        [2021, 32],
        [2022, 22]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning multilevel distributed representations for high-dimensional sequences",
      "link": "http://proceedings.mlr.press/v2/sutskever07a.html",
      "year": 2007,
      "cited_by": 274,
      "authors": ["Ilya Sutskever", "Geoffrey Hinton"],
      "description": "We describe a new family of non-linear sequence models that are substantially more powerful than hidden Markov models or linear dynamical systems. Our models have simple approximate inference and learning procedures that work well in practice. Multilevel representations of sequential data can be learned one hidden layer at a time, and adding extra hidden layers improves the resulting generative models. The models can be trained with very high-dimensional, very non-linear data such as raw pixel sequences. Their performance is demonstrated using synthetic video sequences of two balls bouncing in a box.",
      "citation_histogram": [
        [2006, 2],
        [2007, 5],
        [2008, 7],
        [2009, 12],
        [2010, 5],
        [2011, 13],
        [2012, 6],
        [2013, 16],
        [2014, 22],
        [2015, 31],
        [2016, 27],
        [2017, 37],
        [2018, 38],
        [2019, 23],
        [2020, 12],
        [2021, 13],
        [2022, 5]
      ],
      "detail_extracted": true
    },
    {
      "title": "Symbols among the neurons: Details of a connectionist inference architecture",
      "link": "https://www.ijcai.org/Proceedings/85-1/Papers/044.pdf",
      "year": 1985,
      "cited_by": 271,
      "authors": ["David S Touretzky", "Geoffrey E Hinton"],
      "description": "Pattern matching and variable binding are easily implemented in conventional computer architectures, but not necessarily in all architectures. In a distributed neural network architecture each symbol is represented by activity in many units and each unit contributes to the representation of many symbols. Manipulating symbols using this type of distributed representation is not as easy as with a local representation whore each unit denotes one symbol, but there is evidence that the distributed approach is the one chosen by nature. We describe a working implementation of a production system interpreter in a neural network using distributed representations for both symbols and rules. The research provides a detailed account of two important symbolic reasoning operations, pattern matching and variable binding, as emergent properties of collections of neuron-like elements. The success of our production system implementation goes some way towards answering a common criticism of connectionist theories: that they aren't powerful enough to do symbolic reasoning.",
      "citation_histogram": [
        [1985, 1],
        [1986, 5],
        [1987, 23],
        [1988, 54],
        [1989, 16],
        [1990, 24],
        [1991, 13],
        [1992, 18],
        [1993, 10],
        [1994, 9],
        [1995, 8],
        [1996, 5],
        [1997, 4],
        [1998, 9],
        [1999, 6],
        [2000, 2],
        [2001, 1],
        [2002, 1],
        [2003, 3],
        [2004, 2],
        [2005, 4],
        [2006, 6],
        [2007, 4],
        [2008, 4],
        [2009, 1],
        [2010, 3],
        [2011, 2],
        [2012, 3],
        [2013, 2],
        [2014, 1],
        [2015, 1],
        [2016, 1],
        [2017, 5],
        [2018, 4],
        [2019, 3],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "On deep generative models with applications to recognition",
      "link": "https://ieeexplore.ieee.org/abstract/document/5995710/",
      "year": 2011,
      "cited_by": 268,
      "authors": [
        "Marc'Aurelio Ranzato",
        "Joshua Susskind",
        "Volodymyr Mnih",
        "Geoffrey Hinton"
      ],
      "description": "The most popular way to use probabilistic models in vision is first to extract some descriptors of small image patches or object parts using well-engineered features, and then to use statistical learning tools to model the dependencies among these features and eventual labels. Learning probabilistic models directly on the raw pixel values has proved to be much more difficult and is typically only used for regularizing discriminative methods. In this work, we use one of the best, pixel-level, generative models of natural images-a gated MRF-as the lowest level of a deep belief network (DBN) that has several hidden layers. We show that the resulting DBN is very good at coping with occlusion when predicting expression categories from face images, and it can produce features that perform comparably to SIFT descriptors for discriminating different types of scene. The generative ability of the model also makes it easy to see\u00a0\u2026",
      "citation_histogram": [
        [2011, 1],
        [2012, 25],
        [2013, 19],
        [2014, 38],
        [2015, 26],
        [2016, 32],
        [2017, 28],
        [2018, 24],
        [2019, 17],
        [2020, 21],
        [2021, 21],
        [2022, 10]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deterministic Boltzmann learning performs steepest descent in weight-space",
      "link": "https://ieeexplore.ieee.org/abstract/document/6796919/",
      "year": 1989,
      "cited_by": 266,
      "authors": ["Geoffrey E Hinton"],
      "description": "The Boltzmann machine learning procedure has been successfully applied in deterministic networks of analog units that use a mean field approximation to efficiently simulate a truly stochastic system (Peterson and Anderson 1987). This type of \u201cdeterministic Boltzmann machine\u201d (DBM) learns much faster than the equivalent \u201cstochastic Boltzmann machine\u201d (SBM), but since the learning procedure for DBM's is only based on an analogy with SBM's, there is no existing proof that it performs gradient descent in any function, and it has only been justified by simulations. By using the appropriate interpretation for the way in which a DBM represents the probability of an output vector given an input vector, it is shown that the DBM performs steepest descent in the same function as the original SBM, except at rare discontinuities. A very simple way of forcing the weights to become symmetrical is also described, and this makes\u00a0\u2026",
      "citation_histogram": [
        [1989, 1],
        [1990, 9],
        [1991, 18],
        [1992, 13],
        [1993, 15],
        [1994, 10],
        [1995, 13],
        [1996, 9],
        [1997, 10],
        [1998, 12],
        [1999, 10],
        [2000, 7],
        [2001, 7],
        [2002, 6],
        [2003, 5],
        [2004, 6],
        [2005, 4],
        [2006, 7],
        [2007, 3],
        [2008, 5],
        [2009, 7],
        [2010, 3],
        [2011, 4],
        [2012, 4],
        [2013, 6],
        [2014, 10],
        [2015, 2],
        [2016, 11],
        [2017, 4],
        [2018, 9],
        [2019, 9],
        [2020, 4],
        [2021, 11],
        [2022, 7]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning translation invariant recognition in a massively parallel networks",
      "link": "https://link.springer.com/chapter/10.1007/3-540-17943-7_117",
      "year": 1987,
      "cited_by": 265,
      "authors": ["Geoffrey E Hinton"],
      "description": " One major goal of research on massively parallel networks of neuron-like processing elements is to discover efficient methods for recognizing patterns. Another goal is to discover general learning procedures that allow networks to construct the internal representations that are required for complex tasks. This paper describes a recently developed procedure that can learn to perform a recognition task. The network is trained on examples in which the input vector represents an instance of a pattern in a particular position and the required output vector represents its name. After prolonged training, the network develops canonical internal representations of the patterns and it uses these canonical representations to identify familiar patterns in novel positions.",
      "citation_histogram": [
        [1988, 4],
        [1989, 8],
        [1990, 3],
        [1991, 9],
        [1992, 7],
        [1993, 5],
        [1994, 9],
        [1995, 7],
        [1996, 6],
        [1997, 8],
        [1998, 8],
        [1999, 9],
        [2000, 18],
        [2001, 12],
        [2002, 5],
        [2003, 12],
        [2004, 7],
        [2005, 8],
        [2006, 4],
        [2007, 4],
        [2008, 2],
        [2009, 5],
        [2010, 5],
        [2011, 7],
        [2012, 10],
        [2013, 6],
        [2014, 5],
        [2015, 1],
        [2016, 8],
        [2017, 7],
        [2018, 8],
        [2019, 10],
        [2020, 15],
        [2021, 13],
        [2022, 9]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning a better representation of speech soundwaves using restricted boltzmann machines",
      "link": "https://ieeexplore.ieee.org/abstract/document/5947700/",
      "year": 2011,
      "cited_by": 255,
      "authors": ["Navdeep Jaitly", "Geoffrey Hinton"],
      "description": "State of the art speech recognition systems rely on preprocessed speech features such as Mel cepstrum or linear predictive coding coefficients that collapse high dimensional speech sound waves into low dimensional encodings. While these have been successfully applied in speech recognition systems, such low dimensional encodings may lose some relevant information and express other information in a way that makes it difficult to use for discrimination. Higher dimensional encodings could both improve performance in recognition tasks, and also be applied to speech synthesis by better modeling the statistical structure of the sound waves. In this paper we present a novel approach for modeling speech sound waves using a Restricted Boltzmann machine (RBM) with a novel type of hidden variable and we report initial results demonstrating phoneme recognition performance better than the current state-of-the\u00a0\u2026",
      "citation_histogram": [
        [2010, 1],
        [2011, 8],
        [2012, 6],
        [2013, 11],
        [2014, 19],
        [2015, 18],
        [2016, 29],
        [2017, 21],
        [2018, 34],
        [2019, 31],
        [2020, 32],
        [2021, 33],
        [2022, 11]
      ],
      "detail_extracted": true
    },
    {
      "title": "Using deep belief nets to learn covariance kernels for Gaussian processes",
      "link": "https://proceedings.neurips.cc/paper/2007/hash/4b6538a44a1dfdc2b83477cd76dee98e-Abstract.html",
      "year": 2007,
      "cited_by": 253,
      "authors": ["Geoffrey E Hinton", "Russ R Salakhutdinov"],
      "description": "We show how to use unlabeled data and a deep belief net (DBN) to learn a good covariance kernel for a Gaussian process. We first learn a deep generative model of the unlabeled data using the fast, greedy algorithm introduced by Hinton et. al. If the data is high-dimensional and highly-structured, a Gaussian kernel applied to the top layer of features in the DBN works much better than a similar kernel applied to the raw input. Performance at both regression and classification can then be further improved by using backpropagation through the DBN to discriminatively fine-tune the covariance kernel.",
      "citation_histogram": [
        [2006, 1],
        [2007, 1],
        [2008, 3],
        [2009, 9],
        [2010, 3],
        [2011, 8],
        [2012, 6],
        [2013, 8],
        [2014, 17],
        [2015, 22],
        [2016, 33],
        [2017, 28],
        [2018, 25],
        [2019, 24],
        [2020, 18],
        [2021, 30],
        [2022, 15]
      ],
      "detail_extracted": true
    },
    {
      "title": "Unsupervised learning of image transformations",
      "link": "https://ieeexplore.ieee.org/abstract/document/4270061/",
      "year": 2007,
      "cited_by": 248,
      "authors": ["Roland Memisevic", "Geoffrey Hinton"],
      "description": "We describe a probabilistic model for learning rich, distributed representations of image transformations. The basic model is defined as a gated conditional random field that is trained to predict transformations of its inputs using a factorial set of latent variables. Inference in the model consists in extracting the transformation, given a pair of images, and can be performed exactly and efficiently. We show that, when trained on natural videos, the model develops domain specific motion features, in the form of fields of locally transformed edge filters. When trained on affine, or more general, transformations of still images, the model develops codes for these transformations, and can subsequently perform recognition tasks that are invariant under these transformations. It can also fantasize new transformations on previously unseen images. We describe several variations of the basic model and provide experimental results\u00a0\u2026",
      "citation_histogram": [
        [2006, 1],
        [2007, 3],
        [2008, 5],
        [2009, 5],
        [2010, 12],
        [2011, 18],
        [2012, 11],
        [2013, 10],
        [2014, 27],
        [2015, 19],
        [2016, 26],
        [2017, 24],
        [2018, 30],
        [2019, 22],
        [2020, 14],
        [2021, 15],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Using generative models for handwritten digit recognition",
      "link": "https://ieeexplore.ieee.org/abstract/document/506410/",
      "year": 1996,
      "cited_by": 248,
      "authors": [
        "Michael Revow",
        "Christopher KI Williams",
        "Geoffrey E Hinton"
      ],
      "description": "We describe a method of recognizing handwritten digits by fitting generative models that are built from deformable B-splines with Gaussian \"ink generators\" spaced along the length of the spline. The splines are adjusted using a novel elastic matching procedure based on the expectation maximization algorithm that maximizes the likelihood of the model generating the data. This approach has many advantages: 1) the system not only produces a classification of the digit but also a rich description of the instantiation parameters which can yield information such as the writing style; 2) the generative models can perform recognition driven segmentation; 3) the method involves a relatively small number of parameters and hence training is relatively easy and fast; and 4) unlike many other recognition schemes, it does not rely on some form of pre-normalization of input images, but can handle arbitrary scalings, translations\u00a0\u2026",
      "citation_histogram": [
        [1995, 3],
        [1996, 9],
        [1997, 11],
        [1998, 11],
        [1999, 10],
        [2000, 12],
        [2001, 6],
        [2002, 9],
        [2003, 11],
        [2004, 9],
        [2005, 15],
        [2006, 10],
        [2007, 4],
        [2008, 6],
        [2009, 6],
        [2010, 5],
        [2011, 7],
        [2012, 3],
        [2013, 5],
        [2014, 20],
        [2015, 16],
        [2016, 7],
        [2017, 11],
        [2018, 9],
        [2019, 8],
        [2020, 11],
        [2021, 8]
      ],
      "detail_extracted": true
    },
    {
      "title": "Visualizing non-metric similarities in multiple maps",
      "link": "https://link.springer.com/article/10.1007/s10994-011-5273-4",
      "year": 2012,
      "cited_by": 246,
      "authors": ["Laurens Van der Maaten", "Geoffrey Hinton"],
      "description": " Techniques for multidimensional scaling visualize objects as points in a low-dimensional metric map. As a result, the visualizations are subject to the fundamental limitations of metric spaces. These limitations prevent multidimensional scaling from faithfully representing non-metric similarity data such as word associations or event co-occurrences. In particular, multidimensional scaling cannot faithfully represent intransitive pairwise similarities in a visualization, and it cannot faithfully visualize \u201ccentral\u201d objects. In this paper, we present an extension of a recently proposed multidimensional scaling technique called t-SNE. The extension aims to address the problems of traditional multidimensional scaling techniques when these techniques are used to visualize non-metric similarities. The new technique, called multiple maps t-SNE, alleviates these problems by constructing a collection of maps that reveal\u00a0\u2026",
      "citation_histogram": [
        [2012, 3],
        [2013, 4],
        [2014, 7],
        [2015, 17],
        [2016, 17],
        [2017, 19],
        [2018, 26],
        [2019, 46],
        [2020, 35],
        [2021, 48],
        [2022, 22]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning representations by recirculation",
      "link": "https://proceedings.neurips.cc/paper/1987/hash/35f4a8d465e6e1edc05f3d8ab658c551-Abstract.html",
      "year": 1987,
      "cited_by": 245,
      "authors": ["Geoffrey E Hinton", "James McClelland"],
      "description": "We describe a new learning procedure for networks that contain groups of non (cid: 173)",
      "citation_histogram": [
        [1988, 1],
        [1989, 2],
        [1990, 4],
        [1991, 7],
        [1992, 5],
        [1993, 5],
        [1994, 4],
        [1995, 3],
        [1996, 4],
        [1997, 4],
        [1998, 8],
        [1999, 7],
        [2000, 5],
        [2001, 3],
        [2002, 10],
        [2003, 9],
        [2004, 7],
        [2005, 3],
        [2006, 6],
        [2007, 6],
        [2008, 7],
        [2009, 7],
        [2010, 2],
        [2011, 5],
        [2012, 2],
        [2013, 6],
        [2014, 6],
        [2015, 7],
        [2016, 6],
        [2017, 12],
        [2018, 19],
        [2019, 19],
        [2020, 13],
        [2021, 16],
        [2022, 12]
      ],
      "detail_extracted": true
    },
    {
      "title": "Using expectation-maximization for reinforcement learning",
      "link": "https://direct.mit.edu/neco/article-abstract/9/2/271/6042",
      "year": 1997,
      "cited_by": 242,
      "authors": ["Peter Dayan", "Geoffrey E Hinton"],
      "description": " We discuss Hinton's (1989) relative payoff procedure (RPP), a static reinforcement learning algorithm whose foundation is not stochastic gradient ascent. We show circumstances under which applying the RPP is guaranteed to increase the mean return, even though it can make large changes in the values of the parameters. The proof is based on a mapping between the RPP and a form of the expectation-maximization procedure of Dempster, Laird, and Rubin (1977).",
      "citation_histogram": [
        [2004, 2],
        [2005, 2],
        [2006, 8],
        [2007, 11],
        [2008, 14],
        [2009, 10],
        [2010, 13],
        [2011, 25],
        [2012, 16],
        [2013, 16],
        [2014, 6],
        [2015, 12],
        [2016, 5],
        [2017, 19],
        [2018, 20],
        [2019, 23],
        [2020, 20],
        [2021, 14]
      ],
      "detail_extracted": true
    },
    {
      "title": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models",
      "link": "https://ieeexplore.ieee.org/abstract/document/196523/",
      "year": 1988,
      "cited_by": 238,
      "authors": [
        "Alex Waibel",
        "Toshiyuki Hanazawa",
        "Geoffrey Hinton",
        "Kiyohiro Shikano",
        "K Lang"
      ],
      "description": "A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models\u00a0\u2026",
      "citation_histogram": [
        [1988, 9],
        [1989, 32],
        [1990, 27],
        [1991, 23],
        [1992, 17],
        [1993, 8],
        [1994, 9],
        [1995, 17],
        [1996, 6],
        [1997, 8],
        [1998, 8],
        [1999, 9],
        [2000, 1],
        [2001, 1],
        [2002, 2],
        [2003, 4],
        [2004, 4],
        [2005, 2],
        [2006, 1],
        [2007, 2],
        [2008, 2],
        [2009, 1],
        [2010, 2],
        [2011, 3],
        [2012, 2],
        [2013, 4],
        [2014, 6],
        [2015, 8],
        [2016, 4],
        [2017, 3],
        [2018, 5],
        [2019, 3],
        [2020, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Glove-TalkII-a neural-network interface which maps gestures to parallel formant speech synthesizer controls",
      "link": "https://ieeexplore.ieee.org/abstract/document/655042/",
      "year": 1998,
      "cited_by": 237,
      "authors": ["Sidney S Fels", "Geoffrey E Hinton"],
      "description": "Glove-TalkII is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped continuously to ten control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently, the best version of Glove-TalkII uses several input devices (including a Cyberglove, a ContactGlove, a three-space tracker, and a foot pedal), a parallel formant speech synthesizer, and three neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed user\u00a0\u2026",
      "citation_histogram": [
        [1997, 1],
        [1998, 3],
        [1999, 2],
        [2000, 4],
        [2001, 12],
        [2002, 7],
        [2003, 7],
        [2004, 14],
        [2005, 8],
        [2006, 12],
        [2007, 10],
        [2008, 11],
        [2009, 12],
        [2010, 10],
        [2011, 19],
        [2012, 14],
        [2013, 14],
        [2014, 12],
        [2015, 12],
        [2016, 9],
        [2017, 5],
        [2018, 7],
        [2019, 11],
        [2020, 5],
        [2021, 8],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Connectionist architectures for artificial intelligence",
      "link": "https://www.computer.org/csdl/magazine/co/1987/01/01663364/13rRUwgyOfQ",
      "year": 1987,
      "cited_by": 235,
      "authors": ["Scott E. Fahlman", "Geoffrey E. Hinton"],
      "description": "C urrent AI technology can do a good job of emulating many of man's higher mental functions, but some of themost fundamental aspects of human intelligence have proven more elusive. Al can match the best human experts on certain narrow technical prob-lems, but it cannot begin to approach the common sense and sensory abilities of a five-year-old child. Some important ingre-dients of intelligence seem to be missing, and our technology of symbolic represen-tation and heuristic search, based on serial computers, doesnot seem tobeclosingthe gap. Among the missing elements are the following:* The human memory can store a huge quantity and variety of knowledge, and can find relevant items in this storehouse very quickly and without apparent effort. The phenomenon we call common senseis complex, but it derives in part from the ready availability of a large body of assorted knowledge about the world. Our\u00a0\u2026",
      "citation_histogram": [
        [1987, 7],
        [1988, 28],
        [1989, 30],
        [1990, 20],
        [1991, 24],
        [1992, 20],
        [1993, 18],
        [1994, 10],
        [1995, 5],
        [1996, 10],
        [1997, 3],
        [1998, 4],
        [1999, 3],
        [2000, 2],
        [2001, 1],
        [2002, 1],
        [2003, 4],
        [2004, 1],
        [2005, 1],
        [2006, 2],
        [2007, 1],
        [2008, 3],
        [2009, 1],
        [2010, 3],
        [2011, 4],
        [2012, 2],
        [2013, 4],
        [2014, 3],
        [2015, 2],
        [2016, 3],
        [2017, 2],
        [2018, 3],
        [2019, 2],
        [2020, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Lecture 6a overview of mini\u2013batch gradient descent",
      "link": "https://scholar.google.com/scholar?cluster=14614781802384140841&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 234,
      "authors": ["Geoffrey Hinton", "Nitish Srivastava", "Kevin Swersky"],
      "description": "\u2022 The standard momentum method first computes the gradient at the current locaRon and then takes a big jump in the direcRon of the updated accumulated gradient.\u2022 Ilya Sutskever (2012 unpublished) suggested a new form of momentum that often works be er.\u2013Inspired by the Nesterov method for opRmizing convex funcRons.",
      "citation_histogram": [
        [2015, 1],
        [2016, 9],
        [2017, 27],
        [2018, 50],
        [2019, 41],
        [2020, 44],
        [2021, 40],
        [2022, 22]
      ],
      "detail_extracted": true
    },
    {
      "title": "Frames of reference and mental imagery",
      "link": "http://www.cs.toronto.edu/~hinton/absps/framesAndImagery.pdf",
      "year": 1981,
      "cited_by": 219,
      "authors": ["Geoffrey E Hinton", "Lawrence M Parsons"],
      "description": "Successiveiy perceived parts of a scene or object must be related to one another to create a representation of the whole (3-iochberg, 1968), and the same object when seen from a different viewpoint must be seen to have the same three dimensional spatiai structure. These perceptual requirements can be met by a system that explicitiy represents and manipuiates reiationsbips between a viewer-centered frame of reference and frames of reference that are embedded in external objects-The computational apparatus that is required for handling these relationships during normal perception can also be used for simulating continu~ ous spatiai transformations and for performing imagery tasks. The use of a three-dimensional, viewencentered frame of reference as a common space in which to coordinate the various frames embedded in objects may be what ctistirt\u00bb guishes visual imagery from other methods of spatial\u00a0\u2026",
      "citation_histogram": [
        [1982, 33],
        [1983, 3],
        [1984, 6],
        [1985, 6],
        [1986, 2],
        [1987, 8],
        [1988, 9],
        [1989, 7],
        [1990, 11],
        [1991, 4],
        [1992, 8],
        [1993, 5],
        [1994, 8],
        [1995, 8],
        [1996, 5],
        [1997, 4],
        [1998, 7],
        [1999, 1],
        [2000, 5],
        [2001, 2],
        [2002, 3],
        [2003, 4],
        [2004, 3],
        [2005, 4],
        [2006, 7],
        [2007, 2],
        [2008, 2],
        [2009, 3],
        [2010, 5],
        [2011, 6],
        [2012, 4],
        [2013, 9],
        [2014, 5],
        [2015, 5],
        [2016, 2],
        [2017, 1],
        [2018, 4],
        [2019, 2],
        [2020, 2],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Stacked capsule autoencoders",
      "link": "https://proceedings.neurips.cc/paper/2019/hash/2e0d41e02c5be4668ec1b0730b3346a8-Abstract.html",
      "year": 2019,
      "cited_by": 218,
      "authors": [
        "Adam Kosiorek",
        "Sara Sabour",
        "Yee Whye Teh",
        "Geoffrey E Hinton"
      ],
      "description": "Objects are composed of a set of geometrically organized parts. We introduce an unsupervised capsule autoencoder (SCAE), which explicitly uses geometric relationships between parts to reason about objects. Since these relationships do not depend on the viewpoint, our model is robust to viewpoint changes. SCAE consists of two stages. In the first stage, the model predicts presences and poses of part templates directly from the image and tries to reconstruct the image by appropriately arranging the templates. In the second stage, the SCAE predicts parameters of a few object capsules, which are then used to reconstruct part poses. Inference in this model is amortized and performed by off-the-shelf neural encoders, unlike in previous capsule networks. We find that object capsule presences are highly informative of the object class, which leads to state-of-the-art results for unsupervised classification on SVHN (55%) and MNIST (98.7%).",
      "citation_histogram": [
        [2018, 1],
        [2019, 12],
        [2020, 48],
        [2021, 96],
        [2022, 59]
      ],
      "detail_extracted": true
    },
    {
      "title": "Boltzmann machine",
      "link": "http://var.scholarpedia.org/article/Boltzmann_machine",
      "year": 2007,
      "cited_by": 214,
      "authors": ["Geoffrey E Hinton"],
      "description": "A Boltzmann machine is a network of symmetrically connected, neuron-like units that make stochastic decisions about whether to be on or off. Boltzmann machines have a simple learning algorithm (Hinton & Sejnowski, 1983) that allows them to discover interesting features that represent complex regularities in the training data. The learning algorithm is very slow in networks with many layers of feature detectors, but it is fast in\" restricted Boltzmann machines\" that have a single layer of feature detectors. Many hidden layers can be learned efficiently by composing restricted Boltzmann machines, using the feature activations of one as the training data for the next.Boltzmann machines are used to solve two quite different computational problems. For a search problem, the weights on the connections are fixed and are used to represent a cost function. The stochastic dynamics of a Boltzmann machine then allow it to sample binary state vectors that have low values of the cost function.",
      "citation_histogram": [
        [2006, 1],
        [2007, 1],
        [2008, 4],
        [2009, 5],
        [2010, 7],
        [2011, 5],
        [2012, 5],
        [2013, 5],
        [2014, 5],
        [2015, 10],
        [2016, 14],
        [2017, 14],
        [2018, 14],
        [2019, 35],
        [2020, 48],
        [2021, 28]
      ],
      "detail_extracted": true
    },
    {
      "title": "Modeling documents with deep boltzmann machines",
      "link": "https://arxiv.org/abs/1309.6865",
      "year": 2013,
      "cited_by": 213,
      "authors": [
        "Nitish Srivastava",
        "Ruslan R Salakhutdinov",
        "Geoffrey E Hinton"
      ],
      "description": "We introduce a Deep Boltzmann Machine model suitable for modeling and extracting latent semantic representations from a large unstructured collection of documents. We overcome the apparent difficulty of training a DBM with judicious parameter tying. This parameter tying enables an efficient pretraining algorithm and a state initialization scheme that aids inference. The model can be trained just as efficiently as a standard Restricted Boltzmann Machine. Our experiments show that the model assigns better log probability to unseen data than the Replicated Softmax model. Features extracted from our model outperform LDA, Replicated Softmax, and DocNADE models on document retrieval and document classification tasks.",
      "citation_histogram": [
        [2013, 2],
        [2014, 12],
        [2015, 37],
        [2016, 37],
        [2017, 26],
        [2018, 38],
        [2019, 24],
        [2020, 16],
        [2021, 14],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Distilling the knowledge in a neural network. arXiv 2015",
      "link": "https://scholar.google.com/scholar?cluster=12644168116540971745&hl=en&oi=scholarr",
      "year": 2015,
      "cited_by": 211,
      "authors": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"],
      "description": null,
      "citation_histogram": [
        [2018, 3],
        [2019, 11],
        [2020, 31],
        [2021, 85],
        [2022, 81]
      ],
      "detail_extracted": true
    },
    {
      "title": "Energy-based models for sparse overcomplete representations",
      "link": "https://www.jmlr.org/papers/volume4/teh03a/teh03a.pdf",
      "year": 2003,
      "cited_by": 210,
      "authors": [
        "Yee Whye Teh",
        "Max Welling",
        "Simon Osindero",
        "Geoffrey E Hinton"
      ],
      "description": "We present a new way of extending independent components analysis (ICA) to overcomplete representations. In contrast to the causal generative extensions of ICA which maintain marginal independence of sources, we define features as deterministic (linear) functions of the inputs. This assumption results in marginal dependencies among the features, but conditional independence of the features given the inputs. By assigning energies to the features a probability distribution over the input states is defined through the Boltzmann distribution. Free parameters of this model are trained using the contrastive divergence objective (Hinton, 2002). When the number of features is equal to the number of input dimensions this energy-based model reduces to noiseless ICA and we show experimentally that the proposed learning algorithm is able to perform blind source separation on speech data. In additional experiments we train overcomplete energy-based models to extract features from various standard data-sets containing speech, natural images, hand-written digits and faces.",
      "citation_histogram": [
        [2003, 3],
        [2004, 5],
        [2005, 12],
        [2006, 11],
        [2007, 14],
        [2008, 9],
        [2009, 11],
        [2010, 13],
        [2011, 8],
        [2012, 9],
        [2013, 11],
        [2014, 8],
        [2015, 6],
        [2016, 13],
        [2017, 6],
        [2018, 15],
        [2019, 18],
        [2020, 13],
        [2021, 14],
        [2022, 9]
      ],
      "detail_extracted": true
    },
    {
      "title": "Robust boltzmann machines for recognition and denoising",
      "link": "https://ieeexplore.ieee.org/abstract/document/6247936/",
      "year": 2012,
      "cited_by": 206,
      "authors": ["Yichuan Tang", "Ruslan Salakhutdinov", "Geoffrey Hinton"],
      "description": "While Boltzmann Machines have been successful at unsupervised learning and density modeling of images and speech data, they can be very sensitive to noise in the data. In this paper, we introduce a novel model, the Robust Boltzmann Machine (RoBM), which allows Boltzmann Machines to be robust to corruptions. In the domain of visual recognition, the RoBM is able to accurately deal with occlusions and noise by using multiplicative gating to induce a scale mixture of Gaussians over pixels. Image denoising and in-painting correspond to posterior inference in the RoBM. Our model is trained in an unsupervised fashion with unlabeled noisy data and can learn the spatial structure of the occluders. Compared to standard algorithms, the RoBM is significantly better at recognition and denoising on several face databases.",
      "citation_histogram": [
        [2012, 1],
        [2013, 9],
        [2014, 20],
        [2015, 20],
        [2016, 24],
        [2017, 29],
        [2018, 18],
        [2019, 23],
        [2020, 24],
        [2021, 17],
        [2022, 13]
      ],
      "detail_extracted": true
    },
    {
      "title": "Rate-coded restricted Boltzmann machines for face recognition",
      "link": "https://proceedings.neurips.cc/paper/2000/hash/c366c2c97d47b02b24c3ecade4c40a01-Abstract.html",
      "year": 2000,
      "cited_by": 200,
      "authors": ["Yee Whye Teh", "Geoffrey E Hinton"],
      "description": "We describe a neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual. Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known. Our method compares favorably with other methods in the literature. The generative model consists of a single layer of rate-coded, non-linear feature detectors and it has the property that, given a data vector, the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation. The weights of the feature detectors are learned by com (cid: 173) paring the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations.",
      "citation_histogram": [
        [2001, 3],
        [2002, 10],
        [2003, 2],
        [2004, 3],
        [2005, 6],
        [2006, 6],
        [2007, 6],
        [2008, 5],
        [2009, 5],
        [2010, 5],
        [2011, 7],
        [2012, 5],
        [2013, 8],
        [2014, 9],
        [2015, 12],
        [2016, 12],
        [2017, 12],
        [2018, 19],
        [2019, 20],
        [2020, 17],
        [2021, 15],
        [2022, 10]
      ],
      "detail_extracted": true
    },
    {
      "title": "Reinforcement learning with factored states and actions",
      "link": "https://www.jmlr.org/papers/volume5/sallans04a/sallans04a.pdf",
      "year": 2004,
      "cited_by": 199,
      "authors": ["Brian Sallans", "Geoffrey E Hinton"],
      "description": "A novel approximation method is presented for approximating the value function and selecting good actions for Markov decision processes with large state and action spaces. The method approximates state-action values as negative free energies in an undirected graphical model called a product of experts. The model parameters can be learned efficiently because values and derivatives can be efficiently computed for a product of experts. Actions can be found even in large factored action spaces by the use of Markov chain Monte Carlo sampling. Simulation results show that the product of experts approximation can be used to solve large problems. In one simulation it is used to find actions in action spaces of size 240.",
      "citation_histogram": [
        [2005, 2],
        [2006, 7],
        [2007, 2],
        [2008, 5],
        [2009, 9],
        [2010, 8],
        [2011, 3],
        [2012, 6],
        [2013, 8],
        [2014, 14],
        [2015, 21],
        [2016, 15],
        [2017, 18],
        [2018, 16],
        [2019, 15],
        [2020, 16],
        [2021, 15],
        [2022, 18]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep learning for AI",
      "link": "https://dl.acm.org/doi/abs/10.1145/3448250",
      "year": 2021,
      "cited_by": 196,
      "authors": ["Yoshua Bengio", "Yann Lecun", "Geoffrey Hinton"],
      "description": "How can neural networks learn the rich internal representations required for difficult tasks such as recognizing objects or understanding language? ",
      "citation_histogram": [
        [2020, 2],
        [2021, 55],
        [2022, 135]
      ],
      "detail_extracted": true
    },
    {
      "title": "Varieties of Helmholtz machine",
      "link": "https://www.sciencedirect.com/science/article/pii/S0893608096000093",
      "year": 1996,
      "cited_by": 195,
      "authors": ["Peter Dayan", "Geoffrey E Hinton"],
      "description": "The Helmholtz machine is a new unsupervised learning architecture that uses top-down connections to build probability density models of input and bottom-up connections to build inverses to those models. The wake-sleep learning algorithm for the machine involves just the purely local delta rule. This paper suggests a number of different varieties of Helmholtz machines, each with its own strengths and weaknesses, and relates them to cortical information processing. Copyright \u00a9 1996 Elsevier Science Ltd.",
      "citation_histogram": [
        [1996, 2],
        [1997, 8],
        [1998, 3],
        [1999, 2],
        [2000, 3],
        [2001, 8],
        [2002, 7],
        [2003, 6],
        [2004, 5],
        [2005, 2],
        [2006, 4],
        [2007, 7],
        [2008, 6],
        [2009, 2],
        [2010, 7],
        [2011, 1],
        [2012, 3],
        [2013, 35],
        [2014, 9],
        [2015, 12],
        [2016, 16],
        [2017, 9],
        [2018, 5],
        [2019, 7],
        [2020, 6],
        [2021, 10],
        [2022, 7]
      ],
      "detail_extracted": true
    },
    {
      "title": "Holography, associative memory, and inductive generalization",
      "link": "https://books.google.com/books?hl=en&lr=&id=Ug7sAgAAQBAJ&oi=fnd&pg=PA103&dq=info:QT3bfkVabiUJ:scholar.google.com&ots=Z9gDHrlDIV&sig=dB3tcenhnGt5kLT53clbC3_yt0k",
      "year": 1985,
      "cited_by": 193,
      "authors": ["David Willshaw", "GE Hinton", "JA Anderson"],
      "description": "In this chapter I review the work on the theory of associative memory that was carried out by myself in collaboration with OP Buneman and HC Longuet-Higgins at the Theoretical Psychology Unit, Edinburgh University, between 1967 and 1972. We were interested in the basic mathematical problems encountered in designing associative memory devices that would store their information in a nonlocal fashion. We were particularly interested in the design of memory models that could be implemented in neural tissue. Some of this work has already been published (Willshaw, 1972; Willshaw & Buneman, 1972; Willshaw, Buneman & Longuet-Higgins, 1969), but no overall review exists and many of the results in my thesis (Willshaw, 1971) have not been previously published. Our task is to design structures for the storage and retrieval of items of information, which are called patterns. Each pattern to be stored must be identified with a second pattern, to be used as a cue or address to retrieve the first from store. We are therefore properly concerned with an associative memory, a device that stores pairs of patterns in such a way that presentation of one member of a pair will elicit the other from store. In fact the two members of a pair need not be distinct. If both were part of the same pattern, the device would be functioning as a content-addressable memory. The technological advances made in the development of the hologram in the 1960s had led people to suggest that the brain functioned on holographic princi ples. It had long been thought that the brain might store information in a manner resistant to local damage and that allowed for correct retrieval\u00a0\u2026",
      "citation_histogram": [
        [1984, 1],
        [1985, 4],
        [1986, 5],
        [1987, 29],
        [1988, 8],
        [1989, 6],
        [1990, 6],
        [1991, 10],
        [1992, 6],
        [1993, 6],
        [1994, 6],
        [1995, 5],
        [1996, 5],
        [1997, 3],
        [1998, 4],
        [1999, 3],
        [2000, 8],
        [2001, 3],
        [2002, 2],
        [2003, 3],
        [2004, 3],
        [2005, 4],
        [2006, 3],
        [2007, 4],
        [2008, 1],
        [2009, 7],
        [2010, 2],
        [2011, 2],
        [2012, 4],
        [2013, 8],
        [2014, 2],
        [2015, 3],
        [2016, 3],
        [2017, 4],
        [2018, 1],
        [2019, 2],
        [2020, 3],
        [2021, 3],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning sparse topographic representations with products of student-t distributions",
      "link": "https://proceedings.neurips.cc/paper/2002/hash/bb1662b7c5f22a0f905fd59e718ca05e-Abstract.html",
      "year": 2002,
      "cited_by": 192,
      "authors": ["Max Welling", "Simon Osindero", "Geoffrey E Hinton"],
      "description": "We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some filter outputs. We encourage the system to find sparse features by using a Studentt distribution to model each filter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent filters, the system learns a topographic map in which the orientation, spatial frequency and location of the filters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efficient learning procedure that works well even for highly overcomplete sets of filters. Once the model has been learned it can be used as a prior to derive the \u201citerated Wiener filter\u201d for the purpose of denoising images.",
      "citation_histogram": [
        [2003, 3],
        [2004, 5],
        [2005, 6],
        [2006, 10],
        [2007, 14],
        [2008, 8],
        [2009, 16],
        [2010, 16],
        [2011, 17],
        [2012, 13],
        [2013, 10],
        [2014, 13],
        [2015, 18],
        [2016, 11],
        [2017, 8],
        [2018, 2],
        [2019, 6],
        [2020, 8],
        [2021, 5]
      ],
      "detail_extracted": true
    },
    {
      "title": "Assessing the scalability of biologically-motivated deep learning algorithms and architectures",
      "link": "https://proceedings.neurips.cc/paper/2018/hash/63c3ddcc7b23daa1e42dc41f9a44a873-Abstract.html",
      "year": 2018,
      "cited_by": 187,
      "authors": [
        "Sergey Bartunov",
        "Adam Santoro",
        "Blake Richards",
        "Luke Marris",
        "Geoffrey E Hinton",
        "Timothy Lillicrap"
      ],
      "description": "The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and explore performance in both fully-and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.",
      "citation_histogram": [
        [2018, 10],
        [2019, 34],
        [2020, 53],
        [2021, 63],
        [2022, 26]
      ],
      "detail_extracted": true
    },
    {
      "title": "Using fast weights to attend to the recent past",
      "link": "https://proceedings.neurips.cc/paper/2016/hash/9f44e956e3a2b7b5598c625fcc802c36-Abstract.html",
      "year": 2016,
      "cited_by": 183,
      "authors": [
        "Jimmy Ba",
        "Geoffrey E Hinton",
        "Volodymyr Mnih",
        "Joel Z Leibo",
        "Catalin Ionescu"
      ],
      "description": "Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These``fast weights''can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proven helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.",
      "citation_histogram": [
        [2016, 3],
        [2017, 17],
        [2018, 30],
        [2019, 30],
        [2020, 36],
        [2021, 44],
        [2022, 22]
      ],
      "detail_extracted": true
    },
    {
      "title": "Proceedings of the 27th international conference on machine learning (ICML-10)",
      "link": "https://scholar.google.com/scholar?cluster=9172445749533722661&hl=en&oi=scholarr",
      "year": 2010,
      "cited_by": 179,
      "authors": ["V Nair", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [2014, 1],
        [2015, 1],
        [2016, 2],
        [2017, 6],
        [2018, 15],
        [2019, 45],
        [2020, 37],
        [2021, 45],
        [2022, 26]
      ],
      "detail_extracted": true
    },
    {
      "title": "Dynamical binary latent variable models for 3d human pose tracking",
      "link": "https://ieeexplore.ieee.org/abstract/document/5540157/",
      "year": 2010,
      "cited_by": 178,
      "authors": [
        "Graham W Taylor",
        "Leonid Sigal",
        "David J Fleet",
        "Geoffrey E Hinton"
      ],
      "description": "We introduce a new class of probabilistic latent variable model called the Implicit Mixture of Conditional Restricted Boltzmann Machines (imCRBM) for use in human pose tracking. Key properties of the imCRBM are as follows: (1) learning is linear in the number of training exemplars so it can be learned from large datasets; (2) it learns coherent models of multiple activities; (3) it automatically discovers atomic \u201cmovemes\u201d and (4) it can infer transitions between activities, even when such transitions are not present in the training set. We describe the model and how it is learned and we demonstrate its use in the context of Bayesian filtering for multi-view and monocular pose tracking. The model handles difficult scenarios including multiple activities and transitions among activities. We report state-of-the-art results on the HumanEva dataset.",
      "citation_histogram": [
        [2010, 1],
        [2011, 18],
        [2012, 13],
        [2013, 15],
        [2014, 16],
        [2015, 19],
        [2016, 22],
        [2017, 18],
        [2018, 9],
        [2019, 19],
        [2020, 7],
        [2021, 12],
        [2022, 5]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parallel distributed processing",
      "link": "https://scholar.google.com/scholar?cluster=17371598283786016807&hl=en&oi=scholarr",
      "year": 1986,
      "cited_by": 177,
      "authors": ["GE Hinton", "JL McClelland", "DE Rumelhart"],
      "description": null,
      "citation_histogram": [
        [1987, 1],
        [1988, 37],
        [1989, 27],
        [1990, 3],
        [1991, 10],
        [1992, 34],
        [1993, 1],
        [1994, 1],
        [1995, 3],
        [1996, 4],
        [1997, 4],
        [1998, 1],
        [1999, 5],
        [2000, 4],
        [2001, 3],
        [2002, 2],
        [2003, 1],
        [2004, 2],
        [2005, 6],
        [2006, 5],
        [2007, 2],
        [2008, 3],
        [2009, 4],
        [2010, 3],
        [2011, 1],
        [2012, 1],
        [2013, 5],
        [2014, 2],
        [2015, 1],
        [2016, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning sets of filters using back-propagation",
      "link": "https://www.sciencedirect.com/science/article/pii/088523088790026X",
      "year": 1987,
      "cited_by": 175,
      "authors": ["David C Plaut", "Geoffrey E Hinton"],
      "description": "A learning procedure, called back-propagation, for layered networks of deterministic, neuron-like units has been described previously. The ability of the procedure automatically to discover useful internal representations makes it a powerful tool for attacking difficult problems like speech recognition. This paper describes further research on the learning procedure and presents an example in which a network learns a set of filters that enable it to discriminate formant-like patterns in the presence of noise. The generality of the learning procedure is illustrated by a second example in which a similar network learns an edge detection task. The speed of learning is strongly dependent on the shape of the surface formed by the error measure in \u201cweight space\u201d. Examples are given of the error surface for a simple task and an acceleration method that speeds up descent in weight space is illustrated. The main drawback of the\u00a0\u2026",
      "citation_histogram": [
        [1987, 1],
        [1988, 5],
        [1989, 10],
        [1990, 8],
        [1991, 9],
        [1992, 8],
        [1993, 8],
        [1994, 3],
        [1995, 4],
        [1996, 6],
        [1997, 4],
        [1998, 3],
        [1999, 4],
        [2000, 3],
        [2001, 3],
        [2002, 4],
        [2003, 1],
        [2004, 6],
        [2005, 2],
        [2006, 3],
        [2007, 3],
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 4],
        [2012, 6],
        [2013, 3],
        [2014, 6],
        [2015, 8],
        [2016, 7],
        [2017, 5],
        [2018, 2],
        [2019, 6],
        [2020, 11],
        [2021, 7],
        [2022, 6]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep, narrow sigmoid belief networks are universal approximators",
      "link": "https://direct.mit.edu/neco/article-abstract/20/11/2629/7367",
      "year": 2008,
      "cited_by": 174,
      "authors": ["Ilya Sutskever", "Geoffrey E Hinton"],
      "description": " In this note, we show that exponentially deep belief networks can approximate any distribution over binary vectors to arbitrary accuracy, even when the width of each layer is limited to the dimensionality of the data. We further show that such networks can be greedily learned in an easy yet impractical way.",
      "citation_histogram": [
        [2008, 1],
        [2009, 1],
        [2010, 14],
        [2011, 8],
        [2012, 5],
        [2013, 9],
        [2014, 11],
        [2015, 12],
        [2016, 15],
        [2017, 18],
        [2018, 18],
        [2019, 18],
        [2020, 15],
        [2021, 16],
        [2022, 10]
      ],
      "detail_extracted": true
    },
    {
      "title": "A new learning algorithm for mean field Boltzmann machines",
      "link": "https://link.springer.com/chapter/10.1007/3-540-46084-5_57",
      "year": 2002,
      "cited_by": 174,
      "authors": ["Max Welling", "Geoffrey E Hinton"],
      "description": " We present a new learning algorithm for Mean Field Boltzmann Machines based on the contrastive divergence optimization criterion. In addition to minimizing the divergence between the data distribution and the equilibrium distribution, we maximize the divergence between one-step reconstructions of the data and the equilibrium distribution. This eliminates the need to estimate equilibrium statistics, so we do not need to approximate the multimodal probability distribution of the free network with the unimodal mean field distribution. We test the learning algorithm on the classification of digits.",
      "citation_histogram": [
        [2002, 2],
        [2003, 1],
        [2004, 4],
        [2005, 6],
        [2006, 4],
        [2007, 15],
        [2008, 6],
        [2009, 9],
        [2010, 11],
        [2011, 8],
        [2012, 6],
        [2013, 11],
        [2014, 11],
        [2015, 16],
        [2016, 12],
        [2017, 10],
        [2018, 10],
        [2019, 11],
        [2020, 8],
        [2021, 7],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep belief nets for natural language call-routing",
      "link": "https://ieeexplore.ieee.org/abstract/document/5947649/",
      "year": 2011,
      "cited_by": 173,
      "authors": ["Ruhi Sarikaya", "Geoffrey E Hinton", "Bhuvana Ramabhadran"],
      "description": "This paper considers application of Deep Belief Nets (DBNs) to natural language call routing. DBNs have been successfully applied to a number of tasks, including image, audio and speech classification, thanks to the recent discovery of an efficient learning technique. DBNs learn a multi-layer generative model from unlabeled data and the features discovered by this model are then used to initialize a feed-forward neural network which is fine-tuned with backpropagation. We compare a DBN-initialized neural network to three widely used text classification algorithms; Support Vector machines (SVM), Boosting and Maximum Entropy (MaxEnt). The DBN-based model gives a call-routing classification accuracy that is equal to the best of the other models even though it currently uses an impoverished representation of the input.",
      "citation_histogram": [
        [2012, 5],
        [2013, 7],
        [2014, 20],
        [2015, 12],
        [2016, 17],
        [2017, 10],
        [2018, 21],
        [2019, 29],
        [2020, 16],
        [2021, 23],
        [2022, 12]
      ],
      "detail_extracted": true
    },
    {
      "title": "Shape representation in parallel systems",
      "link": "https://www.ijcai.org/Proceedings/81-2/Papers/106.pdf",
      "year": 1981,
      "cited_by": 173,
      "authors": ["Geoffrey E Hinton"],
      "description": "There has been a recent revival of interest in parallel systems in which computation is performed by excitatory and inhibitory interactions within a network of relatively simple, neuronlike units [1 2 3 4]. At the early stages of visual processing, individual units can represent hypotheses about how small local fragments of the visual input should be interpreted, and interactions between units can encode knowledge about the constraints between local interpretations. Higher up in the visual system, the representational issues are more complex. This paper considers the difficulties involved in representing shapes in parallel systems, and suggests ways of overcoming them. In doing so, it provides a mechanism for shape perception and visual attention which allows a novel interpretation of the Gestalt slogan that the whole is more than the sum of itsparts.",
      "citation_histogram": [
        [1982, 3],
        [1983, 7],
        [1984, 9],
        [1985, 10],
        [1986, 8],
        [1987, 9],
        [1988, 13],
        [1989, 11],
        [1990, 7],
        [1991, 7],
        [1992, 5],
        [1993, 5],
        [1994, 4],
        [1995, 4],
        [1996, 2],
        [1997, 7],
        [1998, 1],
        [1999, 3],
        [2000, 3],
        [2001, 4],
        [2002, 1],
        [2003, 5],
        [2004, 4],
        [2005, 2],
        [2006, 2],
        [2007, 4],
        [2008, 3],
        [2009, 2],
        [2010, 3],
        [2011, 2],
        [2012, 1],
        [2013, 3],
        [2014, 3],
        [2015, 4],
        [2016, 2],
        [2017, 2],
        [2018, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Adaptive elastic models for hand-printed character recognition",
      "link": "https://proceedings.neurips.cc/paper/1991/hash/df877f3865752637daa540ea9cbc474f-Abstract.html",
      "year": 1991,
      "cited_by": 172,
      "authors": [
        "Geoffrey E Hinton",
        "Christopher Williams",
        "Michael D Revow"
      ],
      "description": "Hand-printed digits can be modeled as splines that are governed by about 8 control points. For each known digit. the control points have preferred., home\" locations, and deformations of the digit are generated by moving the control points away from their home locations. Images of digits can be produced by placing Gaussian ink generators uniformly along the spline. Real images can be recognized by finding the digit model most likely to have generated the data. For each digit model we use an elastic matching algorithm to minimize an energy function that includes both the defor (cid: 173) mation energy of the digit model and the log probability that the model would generate the inked pixels in the image. The model with the lowest total energy wins. If a uniform noise process is included in the model of image generation, some of the inked pixels can be rejected as noise as a digit model is fitting a poorly segmented image. The digit models learn by modifying the home locations of the control points.",
      "citation_histogram": [
        [1992, 11],
        [1993, 7],
        [1994, 15],
        [1995, 11],
        [1996, 7],
        [1997, 3],
        [1998, 15],
        [1999, 7],
        [2000, 10],
        [2001, 4],
        [2002, 3],
        [2003, 3],
        [2004, 4],
        [2005, 10],
        [2006, 4],
        [2007, 5],
        [2008, 1],
        [2009, 4],
        [2010, 5],
        [2011, 4],
        [2012, 3],
        [2013, 4],
        [2014, 5],
        [2015, 8],
        [2016, 2],
        [2017, 6],
        [2018, 1],
        [2019, 4],
        [2020, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Visualizing data using t-sne",
      "link": "https://scholar.google.com/scholar?cluster=14666634430892436215&hl=en&oi=scholarr",
      "year": 2008,
      "cited_by": 163,
      "authors": ["Laurens Van", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2013, 6],
        [2014, 11],
        [2015, 14],
        [2016, 26],
        [2017, 16],
        [2018, 17],
        [2019, 13],
        [2020, 13],
        [2021, 18],
        [2022, 29]
      ],
      "detail_extracted": true
    },
    {
      "title": "Who said what: Modeling individual labelers improves classification",
      "link": "https://ojs.aaai.org/index.php/AAAI/article/view/11756",
      "year": 2018,
      "cited_by": 162,
      "authors": [
        "Melody Guan",
        "Varun Gulshan",
        "Andrew Dai",
        "Geoffrey Hinton"
      ],
      "description": "Data are often labeled by many different experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. This reduces the workload on individual experts and also gives a better estimate of the unobserved ground truth. When experts disagree, the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. These approaches, however, do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information, we propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improvements in computer-aided diagnosis of diabetic retinopathy. We also show that our method performs better than competing algorithms by Welinder and Perona (2010); Mnih and Hinton (2012). Our work offers an innovative approach for dealing with the myriad real-world settings that use expert opinions to define labels for training.",
      "citation_histogram": [
        [2017, 3],
        [2018, 12],
        [2019, 28],
        [2020, 31],
        [2021, 54],
        [2022, 33]
      ],
      "detail_extracted": true
    },
    {
      "title": "Generating facial expressions with deep belief nets",
      "link": "https://books.google.com/books?hl=en&lr=&id=_SmhDwAAQBAJ&oi=fnd&pg=PA421&dq=info:-vafoywDP00J:scholar.google.com&ots=PSAEGXB0l3&sig=RqYO8tnsC0U-XuBxj8B4ho3mGE4",
      "year": 2008,
      "cited_by": 162,
      "authors": [
        "Joshua M Susskind",
        "Geoffrey E Hinton",
        "Javier R Movellan",
        "Adam K Anderson"
      ],
      "description": "Realistic facial expression animation requires a powerful \u201canimator\u201d(or graphics program) that can represent the kinds of variations in facial appearance that are both possible and likely to occur in a given context. If the goal is fully determined as in character animation for film, knowledge can be provided in the form of human higher-level descriptions. However, for generating facial expressions for interactive interfaces, such as animated avatars, correct expressions for a given context must be generated on the fly. A simple solution is to rely on a set of prototypical expressions or basis shapes that are linearly combined to create every facial expression in an animated sequence (Kleiser, 1989; Parke, 1972). An innovative algorithm for fitting basis shapes to images was proposed by Blanz and Vetter (1999). The main problem with the basis shape approach is that the full range of appearance variation required for convincing expressive behavior is far beyond the capacity of what a small set of basis shapes can accommodate. Moreover, even if many expression components are used to create a repertoire of basis shapes (Joshi, Tien, Desbrun, & Pighin, 2007; Lewis, Matt, & Nickson, 2000), the interface may need to render different identities or mixtures of facial expressions not captured by the learned basis shapes. A representation of facial appearance for animation must be powerful enough to capture the right constraints for realistic expression generation yet flexible enough to accommodate different identities and behaviors. Besides the obvious utility of such a representation to animated facial interfaces, a good model of facial expression generation\u00a0\u2026",
      "citation_histogram": [
        [2008, 1],
        [2009, 1],
        [2010, 9],
        [2011, 4],
        [2012, 3],
        [2013, 8],
        [2014, 17],
        [2015, 15],
        [2016, 10],
        [2017, 14],
        [2018, 25],
        [2019, 15],
        [2020, 19],
        [2021, 13],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Evaluation of adaptive mixtures of competing experts",
      "link": "https://proceedings.neurips.cc/paper/1990/hash/432aca3a1e345e339f35a30c8f65edce-Abstract.html",
      "year": 1990,
      "cited_by": 162,
      "authors": ["Steven Nowlan", "Geoffrey E Hinton"],
      "description": "We compare the performance of the modular architecture, composed of competing expert networks, suggested by Jacobs, Jordan, Nowlan and Hinton (1991) to the performance of a single back-propagation network on a complex, but low-dimensional, vowel recognition task. Simulations reveal that this system is capable of uncovering interesting decompositions in a complex task. The type of decomposition is strongly influenced by the nature of the input to the gating network that decides which expert to use for each case. The modular architecture also exhibits consistently better generalization on many variations of the task.",
      "citation_histogram": [
        [1991, 1],
        [1992, 6],
        [1993, 7],
        [1994, 6],
        [1995, 10],
        [1996, 5],
        [1997, 9],
        [1998, 7],
        [1999, 5],
        [2000, 5],
        [2001, 6],
        [2002, 4],
        [2003, 5],
        [2004, 1],
        [2005, 2],
        [2006, 3],
        [2007, 4],
        [2008, 4],
        [2009, 3],
        [2010, 5],
        [2011, 7],
        [2012, 4],
        [2013, 1],
        [2014, 5],
        [2015, 7],
        [2016, 3],
        [2017, 6],
        [2018, 1],
        [2019, 4],
        [2020, 11],
        [2021, 9],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Separating figure from ground with a parallel network",
      "link": "https://journals.sagepub.com/doi/abs/10.1068/p150197",
      "year": 1986,
      "cited_by": 162,
      "authors": [
        "Paul K Kienker",
        "Terrence J Sejnowski",
        "Geoffrey E Hinton",
        "Lee E Schumacher"
      ],
      "description": "The differentiation of figure from ground plays an important role in the perceptual organization of visual stimuli. The rapidity with which we can discriminate the inside from the outside of a figure suggests that at least this step in the process may be performed in visual cortex by a large number of neurons in several different areas working together in parallel. We have attempted to simulate this collective computation by designing a network of simple processing units that receives two types of information: bottom-up input from the image containing the outlines of a figure, which may be incomplete, and a top-down attentional input that biases one part of the image to be the inside of the figure. No presegmentation of the image was assumed. Two methods for performing the computation were explored: gradient descent, which seeks locally optimal states, and simulated annealing, which attempts to find globally optimal\u00a0\u2026",
      "citation_histogram": [
        [1985, 1],
        [1986, 1],
        [1987, 5],
        [1988, 3],
        [1989, 7],
        [1990, 11],
        [1991, 12],
        [1992, 3],
        [1993, 2],
        [1994, 8],
        [1995, 8],
        [1996, 10],
        [1997, 1],
        [1998, 4],
        [1999, 1],
        [2000, 1],
        [2001, 2],
        [2002, 5],
        [2003, 1],
        [2004, 3],
        [2005, 3],
        [2006, 4],
        [2007, 4],
        [2008, 2],
        [2009, 3],
        [2010, 10],
        [2011, 7],
        [2012, 8],
        [2013, 9],
        [2014, 6],
        [2015, 1],
        [2016, 3],
        [2017, 2],
        [2018, 4],
        [2019, 3],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Conditional Restricted Boltzmann Machines for Structured Output Prediction",
      "link": "https://arxiv.org/abs/1202.3748",
      "year": 2011,
      "cited_by": 158,
      "authors": ["Volodymyr Mnih", "Hugo Larochelle", "Geoffrey E Hinton"],
      "description": "Conditional Restricted Boltzmann Machines (CRBMs) are rich probabilistic models that have recently been applied to a wide range of problems, including collaborative filtering, classification, and modeling motion capture data. While much progress has been made in training non-conditional RBMs, these algorithms are not applicable to conditional models and there has been almost no work on training and generating predictions from conditional RBMs for structured output problems. We first argue that standard Contrastive Divergence-based learning may not be suitable for training CRBMs. We then identify two distinct types of structured output prediction problems and propose an improved learning algorithm for each. The first problem type is one where the output space has arbitrary structure but the set of likely output configurations is relatively small, such as in multi-label classification. The second problem is one where the output space is arbitrarily structured but where the output space variability is much greater, such as in image denoising or pixel labeling. We show that the new learning algorithms can work much better than Contrastive Divergence on both types of problems.",
      "citation_histogram": [
        [2012, 5],
        [2013, 13],
        [2014, 16],
        [2015, 21],
        [2016, 15],
        [2017, 16],
        [2018, 19],
        [2019, 14],
        [2020, 11],
        [2021, 14],
        [2022, 12]
      ],
      "detail_extracted": true
    },
    {
      "title": "Unsupervised discovery of nonlinear structure using contrastive backpropagation",
      "link": "https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog0000_76",
      "year": 2006,
      "cited_by": 157,
      "authors": [
        "Geoffrey Hinton",
        "Simon Osindero",
        "Max Welling",
        "Yee\u2010Whye Teh"
      ],
      "description": " We describe a way of modeling high\u2010dimensional data vectors by using an unsupervised, nonlinear, multilayer neural network in which the activity of each neuron\u2010like unit makes an additive contribution to a global energy score that indicates how surprised the network is by the data vector. The connection weights that determine how the activity of each unit depends on the activities in earlier layers are learned by minimizing the energy assigned to data vectors that are actually observed and maximizing the energy assigned to \u201cconfabulations\u201d that are generated by perturbing an observed data vector in a direction that decreases its energy under the current model.",
      "citation_histogram": [
        [2006, 2],
        [2007, 1],
        [2008, 1],
        [2009, 2],
        [2010, 7],
        [2011, 3],
        [2012, 2],
        [2013, 4],
        [2014, 5],
        [2015, 6],
        [2016, 18],
        [2017, 13],
        [2018, 17],
        [2019, 25],
        [2020, 19],
        [2021, 20],
        [2022, 12]
      ],
      "detail_extracted": true
    },
    {
      "title": "Analyzing cooperative computation",
      "link": "http://iiif.library.cmu.edu/file/Newell_box00088_fld06082_doc0001/Newell_box00088_fld06082_doc0001.pdf",
      "year": 1983,
      "cited_by": 156,
      "authors": ["Geoffrey E Hinton", "Terrence J Sejnowski"],
      "description": "Making a perceptual interpretation can be viewed as a computational process in which a plausible combination is chosen from among a large set of interdependent hypotheses. In a cooperative computation the hypotheses are implemented by units that interact non-linearly and in parallel via excitatory and inhibitory links (Julesz, 1971; Marr & Poggio, 1976; Sejnowski, 1976). A particular perceptual task is specified by external inputs to some of the units and the whole system must then discover a stable state of activity in which the active units represent the hypotheses that are taken as true. We describe a search procedure based on statistical mechanics that finds a plausible combination of hypotheses quickly, and we show that the hardware units required for its efficient implementation are remarkably similar to neurons. We also show that even though the individual units are non-linear, there is a linear relationship between the synaptic weights and the logs of the probabilities of global states into which the system settles. This linear relationship makes it possible to implement a convergent learning procedure which specifies just how the synaptic weights need to be changed in order to increase or decrease the probabilities of global states.",
      "citation_histogram": [
        [1983, 3],
        [1984, 5],
        [1985, 8],
        [1986, 5],
        [1987, 5],
        [1988, 39],
        [1989, 6],
        [1990, 5],
        [1991, 5],
        [1992, 9],
        [1993, 5],
        [1994, 1],
        [1995, 1],
        [1996, 2],
        [1997, 1],
        [1998, 1],
        [1999, 1],
        [2000, 1],
        [2001, 1],
        [2002, 2],
        [2003, 4],
        [2004, 7],
        [2005, 2],
        [2006, 2],
        [2007, 3],
        [2008, 7],
        [2009, 1],
        [2010, 2],
        [2011, 2],
        [2012, 1],
        [2013, 3],
        [2014, 4],
        [2015, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "A better way to pretrain deep boltzmann machines",
      "link": "https://proceedings.neurips.cc/paper/2012/hash/7d771e0e8f3633ab54856925ecdefc5d-Abstract.html",
      "year": 2012,
      "cited_by": 155,
      "authors": ["Geoffrey E Hinton", "Russ R Salakhutdinov"],
      "description": "We describe how the pre-training algorithm for Deep Boltzmann Machines (DBMs) is related to the pre-training algorithm for Deep Belief Networks and we show that under certain conditions, the pre-training procedure improves the variational lower bound of a two-hidden-layer DBM. Based on this analysis, we develop a different method of pre-training DBMs that distributes the modelling work more evenly over the hidden layers. Our results on the MNIST and NORB datasets demonstrate that the new pre-training algorithm allows us to learn better generative models.",
      "citation_histogram": [
        [2012, 1],
        [2013, 10],
        [2014, 13],
        [2015, 24],
        [2016, 18],
        [2017, 19],
        [2018, 19],
        [2019, 13],
        [2020, 13],
        [2021, 15],
        [2022, 9]
      ],
      "detail_extracted": true
    },
    {
      "title": "Recognizing handwritten digits using mixtures of linear models",
      "link": "https://proceedings.neurips.cc/paper/1994/hash/5c936263f3428a40227908d5a3847c0b-Abstract.html",
      "year": 1994,
      "cited_by": 155,
      "authors": ["Geoffrey E Hinton", "Michael Revow", "Peter Dayan"],
      "description": "We construct a mixture of locally linear generative models of a col (cid: 173) lection of pixel-based images of digits, and use them for recogni (cid: 173) tion. Different models of a given digit are used to capture different styles of writing, and new images are classified by evaluating their log-likelihoods under each model. We use an EM-based algorithm in which the M-step is computationally straightforward principal components analysis (PCA). Incorporating tangent-plane informa (cid: 173) tion [12] about expected local deformations only requires adding tangent vectors into the sample covariance matrices for the PCA, and it demonstrably improves performance.",
      "citation_histogram": [
        [1994, 1],
        [1995, 3],
        [1996, 7],
        [1997, 9],
        [1998, 7],
        [1999, 10],
        [2000, 11],
        [2001, 6],
        [2002, 15],
        [2003, 7],
        [2004, 8],
        [2005, 2],
        [2006, 11],
        [2007, 4],
        [2008, 6],
        [2009, 1],
        [2010, 3],
        [2011, 4],
        [2012, 3],
        [2013, 2],
        [2014, 6],
        [2015, 4],
        [2016, 2],
        [2017, 4],
        [2018, 3],
        [2019, 2],
        [2020, 6],
        [2021, 5],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Neural additive models: Interpretable machine learning with neural nets",
      "link": "https://proceedings.neurips.cc/paper/2021/hash/251bd0442dfcc53b5a761e050f8022b8-Abstract.html",
      "year": 2021,
      "cited_by": 150,
      "authors": [
        "Rishabh Agarwal",
        "Levi Melnick",
        "Nicholas Frosst",
        "Xuezhou Zhang",
        "Ben Lengerich",
        "Rich Caruana",
        "Geoffrey E Hinton"
      ],
      "description": "Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but are more flexible because they are based on neural nets instead of boosted trees. To demonstrate this, we show how NAMs can be used for multitask learning on synthetic data and on the COMPAS recidivism data due to their composability, and demonstrate that the differentiability of NAMs allows them to train more complex interpretable models for COVID-19.",
      "citation_histogram": [
        [2020, 14],
        [2021, 60],
        [2022, 74]
      ],
      "detail_extracted": true
    },
    {
      "title": "Cvxnet: Learnable convex decomposition",
      "link": "http://openaccess.thecvf.com/content_CVPR_2020/html/Deng_CvxNet_Learnable_Convex_Decomposition_CVPR_2020_paper.html",
      "year": 2020,
      "cited_by": 150,
      "authors": [
        "Boyang Deng",
        "Kyle Genova",
        "Soroosh Yazdani",
        "Sofien Bouaziz",
        "Geoffrey Hinton",
        "Andrea Tagliasacchi"
      ],
      "description": "Any solid object can be decomposed into a collection of convex polytopes (in short, convexes). When a small number of convexes are used, such a decomposition can be thought of as a piece-wise approximation of the geometry. This decomposition is fundamental in computer graphics, where it provides one of the most common ways to approximate geometry, for example, in real-time physics simulation. A convex object also has the property of being simultaneously an explicit and implicit representation: one can interpret it explicitly as a mesh derived by computing the vertices of a convex hull, or implicitly as the collection of half-space constraints or support functions. Their implicit representation makes them particularly well suited for neural network training, as they abstract away from the topology of the geometry they need to represent. However, at testing time, convexes can also generate explicit representations-polygonal meshes-which can then be used in any downstream application. We introduce a network architecture to represent a low dimensional family of convexes. This family is automatically derived via an auto-encoding process. We investigate the applications of this architecture including automatic convex decomposition, image to 3D reconstruction, and part-based shape retrieval.",
      "citation_histogram": [
        [2019, 1],
        [2020, 35],
        [2021, 61],
        [2022, 51]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning symmetry groups with hidden units: Beyond the perceptron",
      "link": "https://www.sciencedirect.com/science/article/pii/0167278986902459",
      "year": 1986,
      "cited_by": 150,
      "authors": [
        "Terrence J Sejnowski",
        "Paul K Kienker",
        "Geoffrey E Hinton"
      ],
      "description": "Learning to recognize mirror, rotational and translational symmetries is a difficult problem for massively-parallel network models. These symmetries cannot be learned by first-order perceptrons or Hopfield networks, which have no means for incorporating additional adaptive units that are hidden from the input and output layers. We demonstrate that the Boltzmann learning algorithm is capable of finding sets of weights which turn hidden units into useful higher-order feature detectors capable of solving symmetry problems.",
      "citation_histogram": [
        [1986, 4],
        [1987, 6],
        [1988, 16],
        [1989, 13],
        [1990, 11],
        [1991, 8],
        [1992, 8],
        [1993, 4],
        [1994, 7],
        [1995, 2],
        [1996, 5],
        [1997, 2],
        [1998, 4],
        [1999, 3],
        [2000, 2],
        [2001, 3],
        [2002, 5],
        [2003, 3],
        [2004, 2],
        [2005, 1],
        [2006, 5],
        [2007, 2],
        [2008, 2],
        [2009, 1],
        [2010, 1],
        [2011, 3],
        [2012, 3],
        [2013, 3],
        [2014, 5],
        [2015, 1],
        [2016, 4],
        [2017, 3],
        [2018, 3],
        [2019, 5]
      ],
      "detail_extracted": true
    },
    {
      "title": "Modeling image patches with a directed hierarchy of Markov random fields",
      "link": "https://proceedings.neurips.cc/paper/2007/hash/9232fe81225bcaef853ae32870a2b0fe-Abstract.html",
      "year": 2007,
      "cited_by": 145,
      "authors": ["Simon Osindero", "Geoffrey E Hinton"],
      "description": "We describe an efficient learning procedure for multilayer generative models that combine the best aspects of Markov random fields and deep, directed belief nets. The generative models can be learned one layer at a time and when learning is complete they have a very fast inference procedure for computing a good approximation to the posterior distribution in all of the hidden layers. Each hidden layer has its own MRF whose energy function is modulated by the top-down directed connections from the layer above. To generate from the model, each layer in turn must settle to equilibrium given its top-down input. We show that this type of model is good at capturing the statistics of patches of natural images.",
      "citation_histogram": [
        [2006, 1],
        [2007, 1],
        [2008, 6],
        [2009, 13],
        [2010, 17],
        [2011, 12],
        [2012, 7],
        [2013, 8],
        [2014, 11],
        [2015, 15],
        [2016, 13],
        [2017, 9],
        [2018, 8],
        [2019, 9],
        [2020, 5],
        [2021, 7],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Relaxation and its role in vision",
      "link": "https://era.ed.ac.uk/handle/1842/8121",
      "year": 1978,
      "cited_by": 145,
      "authors": ["Geoffrey Everest Hinton"],
      "description": "It is argued that a visual system, especially one which handles imperfect data, needs a way of selecting the best consistent combination from among the many interrelated, locally plausible hypotheses about how parts or aspects of the visual input may be interpreted. A method is presented in which each hypothesis is given a supposition value between 0 and 1. A parallel relaxation I operator, based on the plausibilities of hypotheses and the logical relations between them, is then used to modify the supposition values, and the process is repeated until the best consistent set of hypotheses have supposition values of approximately 1, and the rest have values of approximately 0. The method is incorporated in a program which can interpret configurations of overlapping rectangles as puppets. For this task it is possible to formulate all the potentially relevant hypotheses before using relaxation to select the best consistent\u00a0\u2026",
      "citation_histogram": [
        [1983, 5],
        [1984, 4],
        [1985, 34],
        [1986, 3],
        [1987, 3],
        [1988, 8],
        [1989, 9],
        [1990, 6],
        [1991, 6],
        [1992, 3],
        [1993, 5],
        [1994, 3],
        [1995, 1],
        [1996, 2],
        [1997, 2],
        [1998, 1],
        [1999, 1],
        [2000, 1],
        [2001, 2],
        [2002, 3],
        [2003, 2],
        [2004, 3],
        [2005, 6],
        [2006, 4],
        [2007, 4],
        [2008, 1],
        [2009, 2],
        [2010, 1],
        [2011, 2],
        [2012, 1],
        [2013, 1],
        [2014, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Layer normalization. arXiv",
      "link": "https://scholar.google.com/scholar?cluster=9779596104069318346&hl=en&oi=scholarr",
      "year": 2016,
      "cited_by": 143,
      "authors": ["Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton"],
      "description": null,
      "citation_histogram": [
        [2017, 1],
        [2018, 7],
        [2019, 10],
        [2020, 11],
        [2021, 39],
        [2022, 74]
      ],
      "detail_extracted": true
    },
    {
      "title": "Two Distributed-State Models For Generating High-Dimensional Time Series.",
      "link": "https://www.jmlr.org/papers/volume12/taylor11a/taylor11a.pdf",
      "year": 2011,
      "cited_by": 143,
      "authors": ["Graham W Taylor", "Geoffrey E Hinton", "Sam T Roweis"],
      "description": "In this paper we develop a class of nonlinear generative models for high-dimensional time series. We first propose a model based on the restricted Boltzmann machine (RBM) that uses an undirected model with binary latent variables and real-valued \u201cvisible\u201d variables. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. This \u201cconditional\u201d RBM (CRBM) makes on-line inference efficient and allows us to use a simple approximate learning procedure. We demonstrate the power of our approach by synthesizing various sequences from a model trained on motion capture data and by performing on-line filling in of data lost during capture.We extend the CRBM in a way that preserves its most important computational properties and introduces multiplicative three-way interactions that allow the effective interaction weight between two variables to be modulated by the dynamic state of a third variable. We introduce a factoring of the implied three-way weight tensor to permit a more compact parameterization. The resulting model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve its ability to blend motion styles or to transition smoothly among them.",
      "citation_histogram": [
        [2011, 1],
        [2012, 6],
        [2013, 10],
        [2014, 15],
        [2015, 15],
        [2016, 15],
        [2017, 20],
        [2018, 15],
        [2019, 9],
        [2020, 19],
        [2021, 10],
        [2022, 6]
      ],
      "detail_extracted": true
    },
    {
      "title": "Neural networks: Tricks of the trade",
      "link": "https://scholar.google.com/scholar?cluster=6720193907524260336&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 138,
      "authors": [
        "Geoffrey E Hinton",
        "G Montavon",
        "GB Orr",
        "KR M\u00fcller"
      ],
      "description": null,
      "citation_histogram": [
        [2013, 1],
        [2014, 2],
        [2015, 4],
        [2016, 7],
        [2017, 17],
        [2018, 18],
        [2019, 24],
        [2020, 29],
        [2021, 22],
        [2022, 14]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning to represent visual input",
      "link": "https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2009.0200",
      "year": 2010,
      "cited_by": 138,
      "authors": ["Geoffrey E Hinton"],
      "description": "One of the central problems in computational neuroscience is to understand how the object-recognition pathway of the cortex learns a deep hierarchy of nonlinear feature detectors. Recent progress in machine learning shows that it is possible to learn deep hierarchies without requiring any labelled data. The feature detectors are learned one layer at a time and the goal of the learning procedure is to form a good generative model of images, not to predict the class of each image. The learning procedure only requires the pairwise correlations between the activations of neuron-like processing units in adjacent layers. The original version of the learning procedure is derived from a quadratic \u2018energy\u2019 function but it can be extended to allow third-order, multiplicative interactions in which neurons gate the pairwise interactions between other neurons. A technique for factoring the third-order interactions leads to a learning\u00a0\u2026",
      "citation_histogram": [
        [2010, 10],
        [2011, 11],
        [2012, 7],
        [2013, 49],
        [2014, 18],
        [2015, 6],
        [2016, 7],
        [2017, 1],
        [2018, 9],
        [2019, 3],
        [2020, 3],
        [2021, 4],
        [2022, 5]
      ],
      "detail_extracted": true
    },
    {
      "title": "Representing part-whole hierarchies in connectionist networks",
      "link": "http://www.csri.utoronto.ca/~hinton/absps/part-whole88.pdf",
      "year": 1988,
      "cited_by": 138,
      "authors": ["Geoffrey E Hinton"],
      "description": "One reason for scepticism about connectionist models that use distributed representations is that there are currently no convincing demonstrations of how these models can represent complex, articulated structures. Drew McDermott (personal communication) has suggested that the approach would be far more convincing if it could come up with a sensible scheme for representing the meaning of a sentence such as:\" She seems to be more at ease with her fellow students than with me, her adviser.\" This meaning \u00cc3 clearly composed of several major constituents with relationships between them, and each major constituent has its own, complex, internal structure. A representational scheme for dealing with meanings of this complexity must, at the very least, specify how the meanings of whole expressions are related to the meanings of their constituents and how it is possible, in some sense, to have the whole meaning\u00a0\u2026",
      "citation_histogram": [
        [1987, 25],
        [1988, 3],
        [1989, 8],
        [1990, 6],
        [1991, 11],
        [1992, 14],
        [1993, 7],
        [1994, 6],
        [1995, 1],
        [1996, 4],
        [1997, 1],
        [1998, 1],
        [1999, 2],
        [2000, 1],
        [2001, 2],
        [2002, 1],
        [2003, 21],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 3],
        [2008, 3],
        [2009, 2],
        [2010, 1],
        [2011, 3],
        [2012, 2],
        [2013, 1],
        [2014, 1],
        [2015, 1],
        [2016, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parallel computations for controlling an arm",
      "link": "https://www.tandfonline.com/doi/abs/10.1080/00222895.1984.10735317",
      "year": 1984,
      "cited_by": 131,
      "authors": ["Geoffrey Hinton"],
      "description": "In order to control a reaching movement of the arm and body, several different computational problems must be solved. Some parallel methods that could be implemented in networks of neuron-like processors are described. Each method solves a different part of the overall task. First, a method is described for finding the torques necessary to follow a desired trajectory. The methods is more economical and more versatile than table look-up and requires very few sequential steps. Then a way of generating an internal representation of a desired trajectory is described. This method shows the trajectory one piece at a time by applying a large set of heuristic rules to a \u201cmotion blackboard\u201e that represents the static and dynamic parameters of the state of the body at the current point in the trajectory. The computations are simplified by expressing the positions, orientations, and motions of parts of the body in terms of a single\u00a0\u2026",
      "citation_histogram": [
        [1984, 2],
        [1985, 4],
        [1986, 4],
        [1987, 3],
        [1988, 5],
        [1989, 7],
        [1990, 3],
        [1991, 4],
        [1992, 5],
        [1993, 11],
        [1994, 4],
        [1995, 5],
        [1996, 4],
        [1997, 3],
        [1998, 4],
        [1999, 3],
        [2000, 6],
        [2001, 4],
        [2002, 5],
        [2003, 2],
        [2004, 2],
        [2005, 2],
        [2006, 5],
        [2007, 2],
        [2008, 2],
        [2009, 2],
        [2010, 5],
        [2011, 5],
        [2012, 3],
        [2013, 2],
        [2014, 3],
        [2015, 1],
        [2016, 1],
        [2017, 1],
        [2018, 2],
        [2019, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Scene-based and viewer-centered representations for comparing shapes",
      "link": "https://www.sciencedirect.com/science/article/pii/0010027788900029",
      "year": 1988,
      "cited_by": 130,
      "authors": ["Geoffrey E Hinton", "Lawrence M Parsons"],
      "description": "Cinq exp\u00e9riences ont examin\u00e9 l'utilisation de deux types de repr\u00e9sentations (en coordonn\u00e9es fixes par rapport \u00e0 la sc\u00e8ne, ou en coordonn\u00e9es \u00e9gocentriques) dans la comparaison de formes sous diff\u00e9rentes orientations. Deux objets, soit les dessins utilis\u00e9s par Metzler et Shepard (1974), soit des h\u00e9lices, \u00e9taient plac\u00e9s sur une table. Les lignes de vis\u00e9e de l'observateur vers ces objets \u00e9taient s\u00e9par\u00e9s par 90 ou 150 degr\u00e9s. Chaque observateur devait d\u00e9cider si les formes des objets \u00e9taient identiques ou imag\u00e9s par un miroir, et devait tourner physiquement l'un des objets vers une orientation qui lui permettait de prendre une d\u00e9cision. Dans cette t\u00e2che, les observateurs tournent souvent l'objet jusqu'\u00e0 ce qu'il ait la m\u00eame relation \u00e0 la table que l'autre objet, utilisant donc un alignement par rapport \u00e0 la sc\u00e8ne ext\u00e9rieure bien que cela produise des images r\u00e9tiniennes tr\u00e8s diff\u00e9rentes pour les deux objets. Les r\u00e9ponses\u00a0\u2026",
      "citation_histogram": [
        [1987, 4],
        [1988, 1],
        [1989, 29],
        [1990, 1],
        [1991, 5],
        [1992, 3],
        [1993, 5],
        [1994, 4],
        [1995, 5],
        [1996, 2],
        [1997, 5],
        [1998, 8],
        [1999, 6],
        [2000, 5],
        [2001, 7],
        [2002, 3],
        [2003, 6],
        [2004, 1],
        [2005, 3],
        [2006, 2],
        [2007, 2],
        [2008, 3],
        [2009, 3],
        [2010, 2],
        [2011, 3],
        [2012, 4],
        [2013, 1],
        [2014, 2],
        [2015, 1],
        [2016, 2],
        [2017, 1],
        [2018, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Discovering binary codes for documents by learning deep generative models",
      "link": "https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1756-8765.2010.01109.x",
      "year": 2011,
      "cited_by": 129,
      "authors": ["Geoffrey Hinton", "Ruslan Salakhutdinov"],
      "description": " We describe a deep generative model in which the lowest layer represents the word\u2010count vector of a document and the top layer represents a learned binary code for that document. The top two layers of the generative model form an undirected associative memory and the remaining layers form a belief net with directed, top\u2010down connections. We present efficient learning and inference procedures for this type of generative model and show that it allows more accurate and much faster retrieval than latent semantic analysis. By using our method as a filter for a much slower method called TF\u2010IDF we achieve higher accuracy than TF\u2010IDF alone and save several orders of magnitude in retrieval time. By using short binary codes as addresses, we can perform retrieval on very large document sets in a time that is independent of the size of the document set using only one word of memory to describe each document.",
      "citation_histogram": [
        [2011, 2],
        [2012, 6],
        [2013, 8],
        [2014, 14],
        [2015, 19],
        [2016, 19],
        [2017, 10],
        [2018, 12],
        [2019, 12],
        [2020, 7],
        [2021, 13],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Phone recognition using restricted boltzmann machines",
      "link": "https://ieeexplore.ieee.org/abstract/document/5495651/",
      "year": 2010,
      "cited_by": 128,
      "authors": ["Abdel-rahman Mohamed", "Geoffrey Hinton"],
      "description": "For decades, Hidden Markov Models (HMMs) have been the state-of-the-art technique for acoustic modeling despite their unrealistic independence assumptions and the very limited representational capacity of their hidden states. Conditional Restricted Boltzmann Machines (CRBMs) have recently proved to be very effective for modeling motion capture sequences and this paper investigates the application of this more powerful type of generative model to acoustic modeling. On the standard TIMIT corpus, one type of CRBM outperforms HMMs and is comparable with the best other methods, achieving a phone error rate (PER) of 26.7% on the TIMIT core test set.",
      "citation_histogram": [
        [2010, 3],
        [2011, 10],
        [2012, 8],
        [2013, 5],
        [2014, 16],
        [2015, 15],
        [2016, 10],
        [2017, 20],
        [2018, 11],
        [2019, 6],
        [2020, 9],
        [2021, 8],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning distributed representations of concepts using linear relational embedding",
      "link": "https://ieeexplore.ieee.org/abstract/document/917563/",
      "year": 2001,
      "cited_by": 128,
      "authors": ["Alberto Paccanaro", "Geoffrey E.  Hinton"],
      "description": "We introduce linear relational embedding as a means of learning a distributed representation of concepts from data consisting of binary relations between these concepts. The key idea is to represent concepts as vectors, binary relations as matrices, and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept. A representation for concepts and relations is learned by maximizing an appropriate discriminative goodness function using gradient ascent. On a task involving family relationships, learning is fast and leads to good generalization.",
      "citation_histogram": [
        [2001, 2],
        [2002, 5],
        [2003, 2],
        [2004, 2],
        [2005, 3],
        [2006, 1],
        [2007, 2],
        [2008, 3],
        [2009, 5],
        [2010, 4],
        [2011, 7],
        [2012, 10],
        [2013, 17],
        [2014, 15],
        [2015, 11],
        [2016, 14],
        [2017, 12],
        [2018, 12]
      ],
      "detail_extracted": true
    },
    {
      "title": "Switching state-space models",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.2720&rep=rep1&type=pdf",
      "year": 1996,
      "cited_by": 127,
      "authors": ["Zoubin Ghahramani", "Geoffrey E Hinton"],
      "description": "We introduce a statistical model for non-linear time series which iteratively segments the data into regimes with approximately linear dynamics and learns the parameters of each of these linear regimes. This model combines and generalizes two of the most widely used stochastic time series models| the hidden Markov model and the linear dynamical system| and is related to models that are widely used in the control and econometrics literatures. It can also be derived by extending the mixture of experts neural network model (Jacobs et al., 1991) to its fully dynamical version, in which both expert and gating networks are recurrent. Inferring the posterior probabilities of the hidden states of this model is computationally intractable, and therefore the exact Expectation Maximization (EM) alogithm cannot be applied. However, we present a variational approximation which maximizes a lower bound on the log likelihood and makes use of both the forward {backward recursions for hidden Markov models and the Kalman lter recursions for linear dynamical systems.",
      "citation_histogram": [
        [1997, 1],
        [1998, 4],
        [1999, 11],
        [2000, 6],
        [2001, 3],
        [2002, 2],
        [2003, 10],
        [2004, 10],
        [2005, 10],
        [2006, 5],
        [2007, 1],
        [2008, 8],
        [2009, 7],
        [2010, 2],
        [2011, 1],
        [2012, 1],
        [2013, 6],
        [2014, 3],
        [2015, 4],
        [2016, 5],
        [2017, 3],
        [2018, 3],
        [2019, 3],
        [2020, 6],
        [2021, 5],
        [2022, 5]
      ],
      "detail_extracted": true
    },
    {
      "title": "Gated softmax classification",
      "link": "https://proceedings.neurips.cc/paper/2010/hash/5737c6ec2e0716f3d8a7a5c4e0de0d9a-Abstract.html",
      "year": 2010,
      "cited_by": 126,
      "authors": [
        "Roland Memisevic",
        "Christopher Zach",
        "Marc Pollefeys",
        "Geoffrey E Hinton"
      ],
      "description": "We describe a log-bilinear\" model that computes class probabilities by combining an input vector multiplicatively with a vector of binary latent variables. Even though the latent variables can take on exponentially many possible combinations of values, we can efficiently compute the exact probability of each class by marginalizing over the latent variables. This makes it possible to get the exact gradient of the log likelihood. The bilinear score-functions are defined using a three-dimensional weight tensor, and we show that factorizing this tensor allows the model to encode invariances inherent in a task by learning a dictionary of invariant basis functions. Experiments on a set of benchmark problems show that this fully probabilistic model can achieve classification performance that is competitive with (kernel) SVMs, backpropagation, and deep belief nets.\"",
      "citation_histogram": [
        [2011, 5],
        [2012, 6],
        [2013, 3],
        [2014, 8],
        [2015, 12],
        [2016, 6],
        [2017, 9],
        [2018, 12],
        [2019, 17],
        [2020, 17],
        [2021, 17],
        [2022, 11]
      ],
      "detail_extracted": true
    },
    {
      "title": "Topographic product models applied to natural scene statistics",
      "link": "https://ieeexplore.ieee.org/abstract/document/6790801/",
      "year": 2006,
      "cited_by": 126,
      "authors": ["Simon Osindero", "Max Welling", "Geoffrey E Hinton"],
      "description": "We present an energy-based model that uses a product of generalized Student-t distributions to capture the statistical structure in data sets. This model is inspired by and particularly applicable to \u201cnatural\u201d data sets such as images. We begin by providing the mathematical framework, where we discuss complete and overcomplete models and provide algorithms for training these models from data. Using patches of natural scenes, we demonstrate that our approach represents a viable alternative to independent component analysis as an interpretive model of biological visual systems. Although the two approaches are similar in flavor, there are also important differences, particularly when the representations are overcomplete. By constraining the interactions within our model, we are also able to study the topographic organization of Gabor-like receptive fields that our model learns. Finally, we discuss the relation of\u00a0\u2026",
      "citation_histogram": [
        [2006, 7],
        [2007, 14],
        [2008, 4],
        [2009, 15],
        [2010, 15],
        [2011, 10],
        [2012, 11],
        [2013, 11],
        [2014, 11],
        [2015, 3],
        [2016, 5],
        [2017, 4],
        [2018, 4],
        [2019, 4],
        [2020, 1],
        [2021, 5],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Nasa neural articulated shape approximation",
      "link": "https://link.springer.com/chapter/10.1007/978-3-030-58571-6_36",
      "year": 2020,
      "cited_by": 125,
      "authors": [
        "Boyang Deng",
        "John P Lewis",
        "Timothy Jeruzalski",
        "Gerard Pons-Moll",
        "Geoffrey Hinton",
        "Mohammad Norouzi",
        "Andrea Tagliasacchi"
      ],
      "description": " Efficient representation of articulated objects such as human bodies is an important problem in computer vision and graphics. To efficiently simulate deformation, existing approaches represent 3D objects using polygonal meshes and deform them using skinning techniques. This paper introduces neural articulated shape approximation (NASA), an alternative framework that enables representation of articulated deformable objects using neural indicator functions that are conditioned on pose. Occupancy testing using NASA is straightforward, circumventing the complexity of meshes and the issue of water-tightness. We demonstrate the effectiveness of NASA for 3D tracking applications, and discuss other potential extensions.",
      "citation_histogram": [
        [2020, 12],
        [2021, 58],
        [2022, 55]
      ],
      "detail_extracted": true
    },
    {
      "title": "Visualizing similarity data with a mixture of maps",
      "link": "http://proceedings.mlr.press/v2/cook07a.html",
      "year": 2007,
      "cited_by": 125,
      "authors": [
        "James Cook",
        "Ilya Sutskever",
        "Andriy Mnih",
        "Geoffrey Hinton"
      ],
      "description": "We show how to visualize a set of pairwise similarities between objects by using several different two-dimensional maps, each of which captures different aspects of the similarity structure. When the objects are ambiguous words, for example, different senses of a word occur in different maps, so \u201criver\u201d and \u201cloan\u201d can both be close to \u201cbank\u201d without being at all close to each other. Aspect maps resemble clustering because they model pair-wise similarities as a mixture of different types of similarity, but they also resemble local multi-dimensional scaling because they model each type of similarity by a twodimensional map. We demonstrate our method on a toy example, a database of human wordassociation data, a large set of images of handwritten digits, and a set of feature vectors that represent words.",
      "citation_histogram": [
        [2006, 1],
        [2007, 4],
        [2008, 3],
        [2009, 3],
        [2010, 3],
        [2011, 7],
        [2012, 9],
        [2013, 7],
        [2014, 11],
        [2015, 10],
        [2016, 16],
        [2017, 10],
        [2018, 9],
        [2019, 6],
        [2020, 16],
        [2021, 10]
      ],
      "detail_extracted": true
    },
    {
      "title": "Simulating brain damage",
      "link": "https://www.jstor.org/stable/24941651",
      "year": 1993,
      "cited_by": 125,
      "authors": ["Geoffrey E Hinton", "David C Plaut", "Tim Shallice"],
      "description": "An 1944 a young soldier su\u201d ered a bullet wound to the head. He survived the war with a strange disability: although he could read and comprehend some words with ease, many others gave him trouble. He read the word antique as\u2026 vase and uncle as\u2026 nephew. The injury was devastating to the patient, GR, but it provided invaluable information to researchers investigating the mechanisms by which the brain comprehends written language. A properly functioning system for converting letters on a page to spoken sounds reveals little of its inner structure, but when that system is disrupted, the peculiar pattern of the resulting dysfunction may o\u201d er essential clues to the original, undamaged architecture.During the past few years, computer simulations of brain function have advanced to the point where they can be used to model information-processing pathways. We have found that deliberate damage to arti \u2018cial\u00a0\u2026",
      "citation_histogram": [
        [1992, 1],
        [1993, 5],
        [1994, 13],
        [1995, 6],
        [1996, 5],
        [1997, 3],
        [1998, 5],
        [1999, 4],
        [2000, 3],
        [2001, 4],
        [2002, 23],
        [2003, 4],
        [2004, 2],
        [2005, 6],
        [2006, 5],
        [2007, 3],
        [2008, 2],
        [2009, 1],
        [2010, 2],
        [2011, 2],
        [2012, 3],
        [2013, 2],
        [2014, 2],
        [2015, 5],
        [2016, 2],
        [2017, 2],
        [2018, 2],
        [2019, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parallel distributed processing: Foundations",
      "link": "https://scholar.google.com/scholar?cluster=13668497064914549441&hl=en&oi=scholarr",
      "year": 1986,
      "cited_by": 123,
      "authors": ["DE Rumelhart", "JL McClelland", "PDP Research Group"],
      "description": null,
      "citation_histogram": [
        [1988, 2],
        [1989, 2],
        [1990, 5],
        [1991, 4],
        [1992, 2],
        [1993, 3],
        [1994, 4],
        [1995, 5],
        [1996, 7],
        [1997, 3],
        [1998, 5],
        [1999, 5],
        [2000, 2],
        [2001, 6],
        [2002, 5],
        [2003, 4],
        [2004, 4],
        [2005, 3],
        [2006, 5],
        [2007, 4],
        [2008, 5],
        [2009, 1],
        [2010, 2],
        [2011, 6],
        [2012, 5],
        [2013, 4],
        [2014, 3],
        [2015, 9],
        [2016, 3],
        [2017, 1],
        [2018, 1],
        [2019, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "How learning can guide evolution",
      "link": "https://books.google.com/books?hl=en&lr=&id=a0haDwAAQBAJ&oi=fnd&pg=PA447&dq=info:9L5TqlCpyIIJ:scholar.google.com&ots=GDMZ874Ik8&sig=bv4vmhpcdDA3LGzXcU7qKjxXN8Y",
      "year": 1996,
      "cited_by": 122,
      "authors": ["Geoffrey E Hinton", "Steven J Nowlan"],
      "description": "The assumption that acquired characteristics are not inherited is often taken to imply that the adaptations that an organism learns during its lifetime cannot guide the course of evolution. This inference is incorrect (Baldwin, 1896). Learning alters the shape of the search space in which evolution operates and thereby provides good evolutionary paths towards sets of co-adapted alleles. We demonstrate that this effect allows learning organisms to evolve much faster than their nonlearning equivalents, even though the characteristics acquired by the phenotype are not communi-cated to the genotype.",
      "citation_histogram": [
        [1996, 2],
        [1997, 1],
        [1998, 2],
        [1999, 3],
        [2000, 62],
        [2001, 3],
        [2002, 3],
        [2003, 2],
        [2004, 1],
        [2005, 4],
        [2006, 3],
        [2007, 1],
        [2008, 2],
        [2009, 4],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 1],
        [2014, 8],
        [2015, 3],
        [2016, 1],
        [2017, 6],
        [2018, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Shape recognition and illusory conjunctions.",
      "link": "http://www.csri.utoronto.ca/~hinton/absps/illusory.pdf",
      "year": 1985,
      "cited_by": 121,
      "authors": ["Geoffrey E Hinton", "Kevin J Lang"],
      "description": "One way to achieve viewpoint-invariant shape recognition is to impose a canonical, object-based frame of reference on a shape and to describe the positions, sizes and orientations of the shape's features relative to the imposed frame.'ITiis computation can be implemented in a parallel network of ncuron-likc processors, but the network has a tendency to make errors of a peculiar kind: When presented with several shapes it sometimes perceives one shape in the position of another. The parameters can be carefully tuned to avoid these\" illusory conjunctions\" in normal circumstances, but they reappear if the visual input is replaced by a random mask before the network has settled down. Treisman and Schmidt (1982) have shown that people make similar errors.",
      "citation_histogram": [
        [1986, 3],
        [1987, 23],
        [1988, 3],
        [1989, 33],
        [1990, 5],
        [1991, 4],
        [1992, 6],
        [1993, 3],
        [1994, 1],
        [1995, 4],
        [1996, 1],
        [1997, 1],
        [1998, 3],
        [1999, 2],
        [2000, 2],
        [2001, 2],
        [2002, 2],
        [2003, 3],
        [2004, 2],
        [2005, 1],
        [2006, 1],
        [2007, 3],
        [2008, 2],
        [2009, 1],
        [2010, 1],
        [2011, 1],
        [2012, 2],
        [2013, 1],
        [2014, 2],
        [2015, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "The EM algorithm for factor analyzers",
      "link": "https://scholar.google.com/scholar?cluster=2135090023154287457&hl=en&oi=scholarr",
      "year": 1997,
      "cited_by": 119,
      "authors": ["Z Ghahramani", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [1999, 1],
        [2000, 1],
        [2001, 2],
        [2002, 1],
        [2003, 1],
        [2004, 2],
        [2005, 1],
        [2006, 5],
        [2007, 6],
        [2008, 7],
        [2009, 12],
        [2010, 16],
        [2011, 5],
        [2012, 6],
        [2013, 9],
        [2014, 10],
        [2015, 9],
        [2016, 11],
        [2017, 8],
        [2018, 6]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning in parallel networks",
      "link": "http://www.cs.utoronto.ca/~hinton/absps/byte.pdf",
      "year": 1985,
      "cited_by": 117,
      "authors": ["Geoffrey Hinton"],
      "description": "THE BRA\u00ceN is an incredibly powerful computer. The cortex alone contains over 10'neurons, each connected to thousands of others. All of your knowledge is probably stored in the strengths of these connections, which somehow give you the effortless ability to understand English, to make sensible plans, to recall relevant facts from fragmentary cues, and to interpret the patterns of light and dark on the back of your eyeballs as r\u00e8a! three-dimensional scenes. By comparison, modern computers do these things very-slowly, if at all. They appear very smart when multiplying long numbers or storing millions of arbitrary facts, but they are remarkably bad at doing what any five-year-old can. One possible explanation is that we don't program computers suitably. Vfe are just so ignorant about what it takes to understand English or interpret visual images that we don't know the appropriate data structures and procedures to put\u00a0\u2026",
      "citation_histogram": [
        [1985, 5],
        [1986, 7],
        [1987, 25],
        [1988, 6],
        [1989, 4],
        [1990, 38],
        [1991, 4],
        [1992, 5],
        [1993, 3],
        [1994, 2],
        [1995, 1],
        [1996, 2],
        [1997, 1],
        [1998, 1],
        [1999, 1],
        [2000, 1],
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 4],
        [2005, 3],
        [2006, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning multiple layers of features from tiny images",
      "link": "https://scholar.google.com/scholar?cluster=2753684462029969015&hl=en&oi=scholarr",
      "year": 2009,
      "cited_by": 114,
      "authors": ["Krizhevsky Alex", "Geoffrey Hinton"],
      "description": null,
      "citation_histogram": [
        [2014, 1],
        [2015, 5],
        [2016, 1],
        [2017, 18],
        [2018, 6],
        [2019, 15],
        [2020, 16],
        [2021, 24],
        [2022, 28]
      ],
      "detail_extracted": true
    },
    {
      "title": "Connectionist symbol processing",
      "link": "https://dl.acm.org/doi/abs/10.5555/531837",
      "year": 1991,
      "cited_by": 114,
      "authors": ["Geoffrey E Hinton"],
      "description": "Connectionist Symbol Processing | Guide books ACM Digital Library home ACM home Google, \nInc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals \nMagazines Proceedings Books SIGs Conferences People More Search ACM Digital Library \nSearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse \nby TitleBooksConnectionist Symbol Processing ABSTRACT No abstract available. Comments \nLogin options Check if you have access through your login credentials or your institution to \nget full access on this article. Sign in Full Access Get this Publication Information Contributors \nPublished in Guide books cover image Connectionist Symbol Processing October 1991 272 \npages ISBN:026258106X Author: Geoffrey E. Hinton Copyright \u00a9 1991 Publisher MIT Press \nCambridge, MA, United States Publication History Online: 1 October 1991 Published: 1 \u2026",
      "citation_histogram": [
        [1991, 1],
        [1992, 3],
        [1993, 7],
        [1994, 8],
        [1995, 6],
        [1996, 7],
        [1997, 5],
        [1998, 5],
        [1999, 6],
        [2000, 7],
        [2001, 4],
        [2002, 4],
        [2003, 4],
        [2004, 4],
        [2005, 5],
        [2006, 3],
        [2007, 5],
        [2008, 3],
        [2009, 3],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 4],
        [2014, 3],
        [2015, 1],
        [2016, 1],
        [2017, 4],
        [2018, 1],
        [2019, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Implicit mixtures of restricted Boltzmann machines",
      "link": "https://proceedings.neurips.cc/paper/2008/hash/e820a45f1dfc7b95282d10b6087e11c0-Abstract.html",
      "year": 2008,
      "cited_by": 113,
      "authors": ["Vinod Nair", "Geoffrey E Hinton"],
      "description": "We present a mixture model whose components are Restricted Boltzmann Machines (RBMs). This possibility has not been considered before because computing the partition function of an RBM is intractable, which appears to make learning a mixture of RBMs intractable as well. Surprisingly, when formulated as a third-order Boltzmann machine, such a mixture model can be learned tractably using contrastive divergence. The energy function of the model captures three-way interactions among visible units, hidden units, and a single hidden multinomial unit that represents the cluster labels. The distinguishing feature of this model is that, unlike other mixture models, the mixing proportions are not explicitly parameterized. Instead, they are defined implicitly via the energy function and depend on all the parameters in the model. We present results for the MNIST and NORB datasets showing that the implicit mixture of RBMs learns clusters that reflect the class structure in the data.",
      "citation_histogram": [
        [2007, 1],
        [2008, 4],
        [2009, 11],
        [2010, 7],
        [2011, 8],
        [2012, 4],
        [2013, 14],
        [2014, 11],
        [2015, 5],
        [2016, 16],
        [2017, 7],
        [2018, 6],
        [2019, 8],
        [2020, 3],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "link": "https://scholar.google.com/scholar?cluster=13089196077609099286&hl=en&oi=scholarr",
      "year": 2014,
      "cited_by": 110,
      "authors": [
        "N Srivastavanitish",
        "Geoffrey Hinton",
        "A Krizhevskykriz",
        "I Sutskeverilya",
        "Ruslan Salakhutdinov"
      ],
      "description": null,
      "citation_histogram": [
        [2016, 2],
        [2017, 5],
        [2018, 11],
        [2019, 24],
        [2020, 33],
        [2021, 21],
        [2022, 14]
      ],
      "detail_extracted": true
    },
    {
      "title": "Does the wake-sleep algorithm produce good density estimators?",
      "link": "https://proceedings.neurips.cc/paper/1995/hash/55b1927fdafef39c48e5b73b5d61ea60-Abstract.html",
      "year": 1995,
      "cited_by": 110,
      "authors": ["Brendan J Frey", "Geoffrey E Hinton", "Peter Dayan"],
      "description": "The wake-sleep algorithm (Hinton, Dayan, Frey and Neal 1995) is a rel (cid: 173) atively efficient method of fitting a multilayer stochastic generative model to high-dimensional data. In addition to the top-down connec (cid: 173) tions in the generative model, it makes use of bottom-up connections for approximating the probability distribution over the hidden units given the data, and it trains these bottom-up connections using a simple delta rule. We use a variety of synthetic and real data sets to compare the per (cid: 173) formance of the wake-sleep algorithm with Monte Carlo and mean field methods for fitting the same generative model and also compare it with other models that are less powerful but easier to fit.",
      "citation_histogram": [
        [1996, 3],
        [1997, 7],
        [1998, 8],
        [1999, 5],
        [2000, 4],
        [2001, 2],
        [2002, 1],
        [2003, 2],
        [2004, 4],
        [2005, 1],
        [2006, 2],
        [2007, 1],
        [2008, 2],
        [2009, 2],
        [2010, 4],
        [2011, 1],
        [2012, 1],
        [2013, 3],
        [2014, 6],
        [2015, 5],
        [2016, 6],
        [2017, 11],
        [2018, 18],
        [2019, 9]
      ],
      "detail_extracted": true
    },
    {
      "title": "Imagenet classification with deep convolutional neural networks",
      "link": "https://scholar.google.com/scholar?cluster=11076218007429227424&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 108,
      "authors": ["A Kirzhevsky", "Ilya Sutskever", "Geoffrey E Hinton"],
      "description": null,
      "citation_histogram": [
        [2014, 1],
        [2015, 4],
        [2016, 5],
        [2017, 7],
        [2018, 23],
        [2019, 16],
        [2020, 20],
        [2021, 21],
        [2022, 10]
      ],
      "detail_extracted": true
    },
    {
      "title": "Split and merge EM algorithm for improving Gaussian mixture density estimates",
      "link": "https://link.springer.com/article/10.1023/A:1008155703044",
      "year": 2000,
      "cited_by": 108,
      "authors": [
        "Naonori Ueda",
        "Ryohei Nakano",
        "Zoubin Ghahramani",
        "Geoffrey E Hinton"
      ],
      "description": " The EM algorithm for Gaussian mixture models often gets caught in local maxima of the likelihood which involve having too many Gaussians in one part of the space and too few in another, widely separated part of the space. We present a new EM algorithm which performs split and merge operations on the Gaussians to escape from these configurations. This algorithm uses two novel criteria for efficiently selecting the split and merge candidates. Experimental results on synthetic and real data show the effectiveness of using the split and merge operations to improve the likelihood of both the training data and of held-out test data.",
      "citation_histogram": [
        [1999, 1],
        [2000, 1],
        [2001, 2],
        [2002, 1],
        [2003, 2],
        [2004, 3],
        [2005, 6],
        [2006, 1],
        [2007, 3],
        [2008, 7],
        [2009, 8],
        [2010, 7],
        [2011, 12],
        [2012, 8],
        [2013, 12],
        [2014, 4],
        [2015, 3],
        [2016, 5],
        [2017, 5],
        [2018, 3],
        [2019, 5],
        [2020, 5],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning in graphical models",
      "link": "https://scholar.google.com/scholar?cluster=13792682468181307505&hl=en&oi=scholarr",
      "year": 1998,
      "cited_by": 108,
      "authors": ["Radford M Neal", "Geoffrey E Hinton", "MI Jordan"],
      "description": null,
      "citation_histogram": [
        [2001, 1],
        [2002, 3],
        [2003, 1],
        [2004, 1],
        [2005, 8],
        [2006, 8],
        [2007, 4],
        [2008, 6],
        [2009, 6],
        [2010, 3],
        [2011, 7],
        [2012, 9],
        [2013, 6],
        [2014, 2],
        [2015, 7],
        [2016, 5],
        [2017, 10],
        [2018, 5],
        [2019, 9],
        [2020, 6]
      ],
      "detail_extracted": true
    },
    {
      "title": "Proceedings of the 7th International Joint Conference on Artificial Intelligence",
      "link": "https://scholar.google.com/scholar?cluster=6499627686456465905&hl=en&oi=scholarr",
      "year": 1981,
      "cited_by": 107,
      "authors": ["GE Hinton", "A Drina"],
      "description": null,
      "citation_histogram": [
        [1985, 14],
        [1986, 20],
        [1987, 53],
        [1988, 19],
        [1989, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Recognizing hand-written digits using hierarchical products of experts",
      "link": "https://proceedings.neurips.cc/paper/2000/hash/1f1baa5b8edac74eb4eaa329f14a0361-Abstract.html",
      "year": 2000,
      "cited_by": 104,
      "authors": ["Guy Mayraz", "Geoffrey E Hinton"],
      "description": "The product of experts learning procedure [1] can discover a set of stochastic binary features that constitute a non-linear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance, it is helpful to learn a hierarchy of separate models for each digit class. Each model in the hierarchy has one layer of hidden units and the nth level model is trained on data that consists of the activities of the hidden units in the already trained (n-l) th level model. After train (cid: 173) ing, each level produces a separate, unnormalized log probabilty score. With a three-level hierarchy for each of the 10 digit classes, a test image produces 30 scores which can be used as inputs to a supervised, logis (cid: 173) tic classification network that is trained on separate data. On the MNIST database, our system is comparable with current state-of-the-art discrimi (cid: 173) native methods, demonstrating that the product of experts learning proce (cid: 173) dure can produce effective generative models of high-dimensional data.",
      "citation_histogram": [
        [2001, 1],
        [2002, 4],
        [2003, 7],
        [2004, 4],
        [2005, 10],
        [2006, 14],
        [2007, 9],
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 5],
        [2012, 5],
        [2013, 3],
        [2014, 2],
        [2015, 5],
        [2016, 6],
        [2017, 5],
        [2018, 6],
        [2019, 3],
        [2020, 7],
        [2021, 3],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "A comparison of statistical learning methods on the GUSTO database",
      "link": "https://onlinelibrary.wiley.com/doi/abs/10.1002/(SICI)1097-0258(19981115)17:21%3C2501::AID-SIM938%3E3.0.CO;2-M",
      "year": 1998,
      "cited_by": 103,
      "authors": [
        "Marguerite Ennis",
        "Geoffrey Hinton",
        "David Naylor",
        "Mike Revow",
        "Robert Tibshirani"
      ],
      "description": " We apply a battery of modern, adaptive non\u2010linear learning methods to a large real database of cardiac patient data. We use each method to predict 30 day mortality from a large number of potential risk factors, and we compare their performances. We find that none of the methods could outperform a relatively simple logistic regression model previously developed for this problem. \u00a9 1998 John Wiley & Sons, Ltd.",
      "citation_histogram": [
        [1999, 4],
        [2000, 6],
        [2001, 5],
        [2002, 3],
        [2003, 5],
        [2004, 6],
        [2005, 8],
        [2006, 7],
        [2007, 6],
        [2008, 3],
        [2009, 5],
        [2010, 1],
        [2011, 3],
        [2012, 4],
        [2013, 2],
        [2014, 4],
        [2015, 7],
        [2016, 2],
        [2017, 1],
        [2018, 7],
        [2019, 4],
        [2020, 4],
        [2021, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Psychological and biological models",
      "link": "https://library.naist.jp/opac/volume/265",
      "year": 1986,
      "cited_by": 101,
      "authors": [
        "James L McClelland",
        "David E Rumelhart",
        "San Diego. PDP Research University of California",
        "Group"
      ],
      "description": "Rumelhart, David E.[Rumelhart, DE][\u30e9\u30e1\u30eb\u30cf\u30fc\u30c8, DE (\u30e9\u30e1\u30eb\u30cf\u30fc\u30c8, DE)][\u30eb\u30fc\u30e1\u30eb\u30cf\u30fc\u30c8, \u30c7\u30a3\u30d3\u30c3\u30c8 E.(\u30eb\u30fc\u30e1\u30eb\u30cf\u30fc\u30c8, \u30c7\u30a3\u30d3\u30c3\u30c8 E.)]",
      "citation_histogram": [
        [1988, 37],
        [1989, 2],
        [1990, 1],
        [1991, 32],
        [1992, 1],
        [1993, 1],
        [1994, 1],
        [1995, 1],
        [1996, 3],
        [1997, 1],
        [1998, 1],
        [1999, 2],
        [2000, 3],
        [2001, 1],
        [2002, 2],
        [2003, 1],
        [2004, 2],
        [2005, 3],
        [2006, 2],
        [2007, 2],
        [2008, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "The toronto face database",
      "link": "https://scholar.google.com/scholar?cluster=10957633046843900374&hl=en&oi=scholarr",
      "year": 2010,
      "cited_by": 100,
      "authors": ["Josh M Susskind", "Adam K Anderson", "Geoffrey E Hinton"],
      "description": null,
      "citation_histogram": [
        [2010, 1],
        [2011, 2],
        [2012, 4],
        [2013, 6],
        [2014, 13],
        [2015, 8],
        [2016, 7],
        [2017, 9],
        [2018, 9],
        [2019, 7],
        [2020, 16],
        [2021, 11],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "A mobile robot that learns its place",
      "link": "https://direct.mit.edu/neco/article-abstract/9/3/683/6076",
      "year": 1997,
      "cited_by": 100,
      "authors": ["Sageev Oore", "Geoffrey E Hinton", "Gregory Dudek"],
      "description": " We show how a neural network can be used to allow a mobile robot to derive an accurate estimate of its location from noisy sonar sensors and noisy motion information. The robot's model of its location is in the form of a probability distribution across a grid of possible locations. This distribution is updated using both the motion information and the predictions of a neural network that maps locations into likelihood distributions across possible sonar readings. By predicting sonar readings from locations, rather than vice versa, the robot can handle the very nongaussian noise in the sonar sensors. By using the constraint provided by the noisy motion information, the robot can use previous readings to improve its estimate of its current location. By treating the resulting estimates as if they were correct, the robot can learn the relationship between location and sonar readings without requiring an external supervision\u00a0\u2026",
      "citation_histogram": [
        [1997, 3],
        [1998, 9],
        [1999, 14],
        [2000, 6],
        [2001, 5],
        [2002, 4],
        [2003, 5],
        [2004, 1],
        [2005, 5],
        [2006, 12],
        [2007, 6],
        [2008, 2],
        [2009, 1],
        [2010, 4],
        [2011, 2],
        [2012, 1],
        [2013, 1],
        [2014, 4],
        [2015, 4],
        [2016, 4],
        [2017, 1],
        [2018, 3],
        [2019, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Generating more realistic images using gated mrf\u2019s",
      "link": "https://proceedings.neurips.cc/paper/2010/hash/6e7d2da6d3953058db75714ac400b584-Abstract.html",
      "year": 2010,
      "cited_by": 99,
      "authors": ["M Ranzato", "Volodymyr Mnih", "GE Hinton"],
      "description": "Probabilistic models of natural images are usually evaluated by measuring performance on rather indirect tasks, such as denoising and inpainting. A more direct way to evaluate a generative model is to draw samples from it and to check whether statistical properties of the samples match the statistics of natural images. This method is seldom used with high-resolution images, because current models produce samples that are very different from natural images, as assessed by even simple visual inspection. We investigate the reasons for this failure and we show that by augmenting existing models so that there are two sets of latent variables, one set modelling pixel intensities and the other set modelling image-specific pixel covariances, we are able to generate high-resolution images that look much more realistic than before. The overall model can be interpreted as a gated MRF where both pair-wise dependencies and mean intensities of pixels are modulated by the states of latent variables. Finally, we confirm that if we disallow weight-sharing between receptive fields that overlap each other, the gated MRF learns more efficient internal representations, as demonstrated in several recognition tasks.",
      "citation_histogram": [
        [2011, 12],
        [2012, 12],
        [2013, 13],
        [2014, 14],
        [2015, 3],
        [2016, 15],
        [2017, 6],
        [2018, 4],
        [2019, 5],
        [2020, 5],
        [2021, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "CIFAR-10 (canadian institute for advanced research).(2009)",
      "link": "https://scholar.google.com/scholar?cluster=13419785280548930294&hl=en&oi=scholarr",
      "year": 2009,
      "cited_by": 99,
      "authors": ["Alex Krizhevsky", "Vinod Nair", "Geoffrey Hinton"],
      "description": null,
      "citation_histogram": [
        [2016, 1],
        [2017, 4],
        [2018, 11],
        [2019, 14],
        [2020, 20],
        [2021, 27],
        [2022, 21]
      ],
      "detail_extracted": true
    },
    {
      "title": "Generative versus discriminative training of RBMs for classification of fMRI images",
      "link": "https://proceedings.neurips.cc/paper/2008/hash/3e77a14629775492504515dc4b23deda-Abstract.html",
      "year": 2008,
      "cited_by": 99,
      "authors": [
        "Tanya Schmah",
        "Geoffrey E Hinton",
        "Steven Small",
        "Stephen Strother",
        "Richard Zemel"
      ],
      "description": "Neuroimaging datasets often have a very large number of voxels and a very small number of training cases, which means that overfitting of models for this data can become a very serious problem. Working with a set of fMRI images from a study on stroke recovery, we consider a classification task for which logistic regression performs poorly, even when L1-or L2-regularized. We show that much better discrimination can be achieved by fitting a generative model to each separate condition and then seeing which model is most likely to have generated the data. We compare discriminative training of exactly the same set of models, and we also consider convex blends of generative and discriminative training.",
      "citation_histogram": [
        [2009, 1],
        [2010, 3],
        [2011, 8],
        [2012, 3],
        [2013, 5],
        [2014, 15],
        [2015, 11],
        [2016, 12],
        [2017, 8],
        [2018, 10],
        [2019, 6],
        [2020, 8],
        [2021, 4],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parallel models of associative memory",
      "link": "https://scholar.google.com/scholar?cluster=13613300985564310122&hl=en&oi=scholarr",
      "year": 1981,
      "cited_by": 99,
      "authors": ["JA Feldman", "GE Hinton", "JA Anderson"],
      "description": null,
      "citation_histogram": [
        [1983, 1],
        [1984, 14],
        [1985, 20],
        [1986, 24],
        [1987, 37],
        [1988, 1],
        [1989, 1],
        [1990, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "How to represent part-whole hierarchies in a neural network",
      "link": "https://arxiv.org/abs/2102.12627",
      "year": 2021,
      "cited_by": 97,
      "authors": ["Geoffrey Hinton"],
      "description": "This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language",
      "citation_histogram": [
        [2020, 1],
        [2021, 44],
        [2022, 52]
      ],
      "detail_extracted": true
    },
    {
      "title": "Divide the gradient by a running average of its recent magnitude. coursera: Neural networks for machine learning",
      "link": "https://scholar.google.com/scholar?cluster=17777770849597044109&hl=en&oi=scholarr",
      "year": 2017,
      "cited_by": 97,
      "authors": ["T Tieleman", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2017, 2],
        [2018, 11],
        [2019, 21],
        [2020, 17],
        [2021, 27],
        [2022, 19]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep lambertian networks",
      "link": "https://arxiv.org/abs/1206.6445",
      "year": 2012,
      "cited_by": 97,
      "authors": ["Yichuan Tang", "Ruslan Salakhutdinov", "Geoffrey Hinton"],
      "description": "Visual perception is a challenging problem in part due to illumination variations. A possible solution is to first estimate an illumination invariant representation before using it for recognition. The object albedo and surface normals are examples of such representations. In this paper, we introduce a multilayer generative model where the latent variables include the albedo, surface normals, and the light source. Combining Deep Belief Nets with the Lambertian reflectance assumption, our model can learn good priors over the albedo from 2D images. Illumination variations can be explained by changing only the lighting latent variable in our model. By transferring learned knowledge from similar objects, albedo and surface normals estimation from a single image is possible in our model. Experiments demonstrate that our model is able to generalize as well as improve over standard baselines in one-shot face recognition.",
      "citation_histogram": [
        [2000, 1],
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 1],
        [2006, 2],
        [2007, 9],
        [2008, 7],
        [2009, 12],
        [2010, 18],
        [2011, 14],
        [2012, 11],
        [2013, 6],
        [2014, 7]
      ],
      "detail_extracted": true
    },
    {
      "title": "Where do features come from?",
      "link": "https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12049",
      "year": 2014,
      "cited_by": 96,
      "authors": ["Geoffrey Hinton"],
      "description": " It is possible to learn multiple layers of non\u2010linear features by backpropagating error derivatives through a feedforward neural network. This is a very effective learning procedure when there is a huge amount of labeled training data, but for many learning tasks very few labeled examples are available. In an effort to overcome the need for labeled data, several different generative models were developed that learned interesting features by modeling the higher order statistical structure of a set of input vectors. One of these generative models, the restricted Boltzmann machine (RBM), has no connections between its hidden units and this makes perceptual inference and learning much simpler. More significantly, after a layer of hidden features has been learned, the activities of these features can be used as training data for another RBM. By applying this idea recursively, it is possible to learn a deep hierarchy of\u00a0\u2026",
      "citation_histogram": [
        [2013, 3],
        [2014, 4],
        [2015, 2],
        [2016, 16],
        [2017, 15],
        [2018, 15],
        [2019, 10],
        [2020, 9],
        [2021, 13],
        [2022, 5]
      ],
      "detail_extracted": true
    },
    {
      "title": "Yee-Whye",
      "link": "https://scholar.google.com/scholar?cluster=17330177180122454852&hl=en&oi=scholarr",
      "year": 2006,
      "cited_by": 96,
      "authors": ["E Hinton", "Osindero Geoffrey", "Teh Simon"],
      "description": null,
      "citation_histogram": [
        [2010, 1],
        [2011, 2],
        [2012, 4],
        [2013, 10],
        [2014, 6],
        [2015, 9],
        [2016, 16],
        [2017, 10],
        [2018, 10],
        [2019, 13],
        [2020, 9],
        [2021, 6]
      ],
      "detail_extracted": true
    },
    {
      "title": "Glove-TalkII: an adaptive gesture-to-formant interface",
      "link": "https://dl.acm.org/doi/fullHtml/10.1145/223904.223966",
      "year": 1995,
      "cited_by": 96,
      "authors": ["Sidney Fels", "Geoffrey Hinton"],
      "description": "Glove-TalkII is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped continuously to 10 control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary, multiple languages in addition to direct control of fundamental frequency and volume. Currently, the best version of Glove-TalkII uses several input devices (including a Cyberglove, a ContactGlove, a polhemus sensor, and a foot-pedal), a parallel formant speech synthesizer and 3 neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a\u00a0\u2026",
      "citation_histogram": [
        [1994, 2],
        [1995, 2],
        [1996, 2],
        [1997, 5],
        [1998, 5],
        [1999, 9],
        [2000, 4],
        [2001, 8],
        [2002, 7],
        [2003, 3],
        [2004, 4],
        [2005, 4],
        [2006, 2],
        [2007, 2],
        [2008, 9],
        [2009, 3],
        [2010, 2],
        [2011, 4],
        [2012, 2],
        [2013, 3],
        [2014, 2],
        [2015, 2],
        [2016, 2],
        [2017, 3],
        [2018, 1],
        [2019, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "The DELVE manual",
      "link": "http://mlg.eng.cam.ac.uk/pub/pdf/RasNeaHinetal96.pdf",
      "year": 1996,
      "cited_by": 95,
      "authors": [
        "Carl E Rasmussen",
        "Radford M Neal",
        "Geoffrey E Hinton",
        "Drew van Camp",
        "Michael Revow",
        "Zoubin Ghahramani",
        "R Kustra",
        "Robert Tibshirani"
      ],
      "description": "DELVE| Data for Evaluating Learning in Valid Experiments| is a collection of datasets from many sources, an environment within which this data can be used to assess the performance of methods for learning relationships from data, and a repository for the results of such assessments.Many methods for learning relationships from empirical data have been developed by researchers in statistics, pattern recognition, arti cial intelligence, neural networks, and other elds. Methods in common use include simple linear models, nearest neighbor methods, decision trees, multilayer perceptron networks, and many others of varying degrees of complexity. Properly comparing the performance of these learning methods in realistic contexts is a surprisingly di cult task, requiring both an extensive collection of real-world data, and a carefully-designed scheme for performing experiments.",
      "citation_histogram": [
        [1996, 1],
        [1997, 1],
        [1998, 3],
        [1999, 3],
        [2000, 5],
        [2001, 5],
        [2002, 6],
        [2003, 2],
        [2004, 4],
        [2005, 1],
        [2006, 1],
        [2007, 7],
        [2008, 5],
        [2009, 3],
        [2010, 2],
        [2011, 7],
        [2012, 5],
        [2013, 1],
        [2014, 7],
        [2015, 3],
        [2016, 2],
        [2017, 3],
        [2018, 4],
        [2019, 1],
        [2020, 6],
        [2021, 2],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "A soft decision-directed LMS algorithm for blind equalization",
      "link": "https://ieeexplore.ieee.org/abstract/document/216497/",
      "year": 1993,
      "cited_by": 93,
      "authors": ["Steven J Nowlan", "Geoffrey E Hinton"],
      "description": "An adaptation algorithm for equalizers operating on very distorted channels is presented. The algorithm is based on the idea of adjusting the equalizer tap gains to maximize the likelihood that the equalizer outputs would be generated by a mixture of two Gaussians with known means. The decision-directed least-mean-square algorithm is shown to be an approximation to maximizing the likelihood that the equalizer outputs come from such an independently and identically distributed source. The algorithm is developed in the context of a binary pulse-amplitude-modulation channel, and simulations demonstrate that the algorithm converges in channels for which the decision-directed LMS algorithms does not converge.< >",
      "citation_histogram": [
        [1994, 1],
        [1995, 4],
        [1996, 13],
        [1997, 5],
        [1998, 7],
        [1999, 1],
        [2000, 2],
        [2001, 6],
        [2002, 4],
        [2003, 5],
        [2004, 1],
        [2005, 4],
        [2006, 2],
        [2007, 3],
        [2008, 3],
        [2009, 5],
        [2010, 2],
        [2011, 4],
        [2012, 2],
        [2013, 4],
        [2014, 1],
        [2015, 4],
        [2016, 2],
        [2017, 1],
        [2018, 2],
        [2019, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Separating figure from ground with a Boltzmann machine",
      "link": "https://dl.acm.org/doi/abs/10.5555/93776.93800",
      "year": 1990,
      "cited_by": 93,
      "authors": ["Terrence J Sejnowski", "Geoffrey E Hinton"],
      "description": "Separating figure from ground with a Boltzmann machine | Vision, brain, and cooperative \ncomputation ACM Digital Library home ACM home Google, Inc. (search) Advanced Search \nBrowse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs \nConferences People More Search ACM Digital Library SearchSearch Advanced Search \nBrowse Browse Digital Library Collections More HomeBrowse by TitleBooksVision, brain, and \ncooperative computationSeparating figure from ground with a Boltzmann machine chapter \nShare on Separating figure from ground with a Boltzmann machine Authors: Terrence J. \nSejnowski View Profile , Geoffrey E. Hinton View Profile Authors Info & Claims Vision, brain, and \ncooperative computationFebruary 1990 Pages 703\u2013724 Online:01 February 1990Publication \nHistory 1citation 0 Downloads Metrics Total Citations1 Total Downloads0 Last 12 Months0 Last \u2026",
      "citation_histogram": [
        [1986, 7],
        [1987, 2],
        [1988, 2],
        [1989, 5],
        [1990, 2],
        [1991, 4],
        [1992, 6],
        [1993, 4],
        [1994, 1],
        [1995, 2],
        [1996, 2],
        [1997, 3],
        [1998, 3],
        [1999, 2],
        [2000, 3],
        [2001, 2],
        [2002, 1],
        [2003, 3],
        [2004, 3],
        [2005, 6],
        [2006, 2],
        [2007, 1],
        [2008, 2],
        [2009, 3],
        [2010, 2],
        [2011, 4],
        [2012, 3],
        [2013, 4],
        [2014, 3],
        [2015, 3],
        [2016, 1],
        [2017, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Distinguishing text from graphics in on-line handwritten ink",
      "link": "https://ieeexplore.ieee.org/abstract/document/1363901/",
      "year": 2004,
      "cited_by": 90,
      "authors": [
        "Christopher M Bishop",
        "Markus Svensen",
        "Goeffrey E Hinton"
      ],
      "description": "We present a system that separates text from graphics strokes in handwritten digital ink. It utilizes not just the characteristics of the strokes, but also the information provided by the gaps between the strokes, as well as the temporal characteristics of the stroke sequence. It is built using machine learning techniques that infer the internal parameters of the system from real digital ink, collected using a tablet PC.",
      "citation_histogram": [
        [2005, 2],
        [2006, 2],
        [2007, 8],
        [2008, 2],
        [2009, 6],
        [2010, 10],
        [2011, 8],
        [2012, 5],
        [2013, 10],
        [2014, 7],
        [2015, 2],
        [2016, 4],
        [2017, 5],
        [2018, 2],
        [2019, 3],
        [2020, 6],
        [2021, 3],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Variational learning in nonlinear Gaussian belief networks",
      "link": "https://ieeexplore.ieee.org/abstract/document/6790581/",
      "year": 1999,
      "cited_by": 90,
      "authors": ["Brendan J Frey", "Geoffrey E Hinton"],
      "description": "We view perceptual tasks such as vision and speech recognition as inference problems where the goal is to estimate the posterior distribution over latent variables (e.g., depth in stereo vision) given the sensory input. The recent flurry of research in independent component analysis exemplifies the importance of inferring the continuous-valued latent variables of input data. The latent variables found by this method are linearly related to the input, but perception requires nonlinear inferences such as classification and depth estimation. In this article, we present a unifying framework for stochastic neural networks with nonlinear latent variables. Nonlinear units are obtained by passing the outputs of linear gaussian units through various nonlinearities. We present a general variational method that maximizes a lower bound on the likelihood of a training set and give results on two visual feature extraction problems. We also\u00a0\u2026",
      "citation_histogram": [
        [1998, 2],
        [1999, 3],
        [2000, 2],
        [2001, 3],
        [2002, 2],
        [2003, 3],
        [2004, 3],
        [2005, 2],
        [2006, 5],
        [2007, 2],
        [2008, 2],
        [2009, 1],
        [2010, 3],
        [2011, 1],
        [2012, 6],
        [2013, 6],
        [2014, 5],
        [2015, 3],
        [2016, 6],
        [2017, 4],
        [2018, 11],
        [2019, 13],
        [2020, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Layer normalization. 2016",
      "link": "https://scholar.google.com/scholar?cluster=6081170003746934374&hl=en&oi=scholarr",
      "year": 2016,
      "cited_by": 89,
      "authors": ["Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton"],
      "description": null,
      "citation_histogram": [
        [2017, 1],
        [2018, 5],
        [2019, 4],
        [2020, 10],
        [2021, 21],
        [2022, 48]
      ],
      "detail_extracted": true
    },
    {
      "title": "Inferring motor programs from images of handwritten digits",
      "link": "https://proceedings.neurips.cc/paper/2005/hash/8fc687aa152e8199fe9e73304d407bca-Abstract.html",
      "year": 2005,
      "cited_by": 87,
      "authors": ["Vinod Nair", "Geoffrey E Hinton"],
      "description": "We describe a generative model for handwritten digits that uses two pairs of opposing springs whose stiffnesses are controlled by a motor program. We show how neural networks can be trained to infer the motor programs required to accurately reconstruct the MNIST digits. The inferred motor programs can be used directly for digit classification, but they can also be used in other ways. By adding noise to the motor program inferred from an MNIST image we can generate a large set of very different images of the same class, thus enlarging the training set available to other methods. We can also use the motor programs as additional, highly informative outputs which reduce overfitting when training a feed-forward classifier.",
      "citation_histogram": [
        [2006, 6],
        [2007, 3],
        [2008, 3],
        [2009, 4],
        [2010, 4],
        [2011, 1],
        [2012, 33],
        [2013, 2],
        [2014, 4],
        [2015, 6],
        [2016, 1],
        [2017, 3],
        [2018, 6],
        [2019, 4],
        [2020, 5],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Introduction to the special section on deep learning for speech and language processing",
      "link": "https://ieeexplore.ieee.org/abstract/document/6060895/",
      "year": 2011,
      "cited_by": 86,
      "authors": [
        "Dong Yu",
        "Geoffrey Hinton",
        "Nelson Morgan",
        "Jen-Tzung Chien",
        "Shigeki Sagayama"
      ],
      "description": "Current speech recognition systems, for example, typically use Gaussian mixture models (GMMs), to estimate the observation (or emission) probabilities of hidden Markov models (HMMs), and GMMs are generative models that have only one layer of latent variables. Instead of developing more powerful models, most of the research effort has gone into finding better ways of estimating the GMM parameters so that error rates are decreased or the margin between different classes is increased. The same observation holds for natural language processing (NLP) in which maximum entropy (MaxEnt) models and conditional random fields (CRFs) have been popular for the last decade. Both of these approaches use shallow models whose success largely depends on the use of carefully handcrafted features.",
      "citation_histogram": [
        [2012, 1],
        [2013, 1],
        [2014, 3],
        [2015, 6],
        [2016, 11],
        [2017, 2],
        [2018, 9],
        [2019, 12],
        [2020, 16],
        [2021, 15],
        [2022, 9]
      ],
      "detail_extracted": true
    },
    {
      "title": "Developing population codes by minimizing description length",
      "link": "https://proceedings.neurips.cc/paper/1993/hash/b86e8d03fe992d1b0e19656875ee557c-Abstract.html",
      "year": 1993,
      "cited_by": 86,
      "authors": ["Richard Zemel", "Geoffrey E Hinton"],
      "description": "The Minimum Description Length principle (MDL) can be used to train the hidden units of a neural network to extract a representa (cid: 173) tion that is cheap to describe but nonetheless allows the input to be reconstructed accurately. We show how MDL can be used to develop highly redundant population codes. Each hidden unit has a location in a low-dimensional implicit space. If the hidden unit activities form a bump of a standard shape in this space, they can be cheaply encoded by the center ofthis bump. So the weights from the input units to the hidden units in an autoencoder are trained to make the activities form a standard bump. The coordinates of the hidden units in the implicit space are also learned, thus allow (cid: 173) ing flexibility, as the network develops a discontinuous topography when presented with different input classes. Population-coding in a space other than the input enables a network to extract nonlinear higher-order properties of the inputs.",
      "citation_histogram": [
        [1993, 1],
        [1994, 3],
        [1995, 1],
        [1996, 6],
        [1997, 6],
        [1998, 7],
        [1999, 7],
        [2000, 1],
        [2001, 2],
        [2002, 5],
        [2003, 6],
        [2004, 2],
        [2005, 4],
        [2006, 4],
        [2007, 3],
        [2008, 1],
        [2009, 2],
        [2010, 2],
        [2011, 2],
        [2012, 2],
        [2013, 1],
        [2014, 2],
        [2015, 1],
        [2016, 1],
        [2017, 1],
        [2018, 5],
        [2019, 2],
        [2020, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning representations by back-propagating errors",
      "link": "https://scholar.google.com/scholar?cluster=5686775081508066012&hl=en&oi=scholarr",
      "year": 1986,
      "cited_by": 86,
      "authors": ["David E Rumlhart", "Geoffrey E Hinton", "Ronald J Williams"],
      "description": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ubility to create useful new features distin-guishes back-propagation from earlier, simpier methods such as the perceptromconvergence procedure \u2018. There have been many attempts to design self-organizing neural networkst The aim is to find a powerful synaptic modi\ufb01cation rule that will aliow an arbitrarily connected neural network to develop an internal structure that is appropriate for a\u00a0\u2026",
      "citation_histogram": [
        [1989, 1],
        [1990, 4],
        [1991, 2],
        [1992, 3],
        [1993, 2],
        [1994, 2],
        [1995, 3],
        [1996, 4],
        [1997, 4],
        [1998, 1],
        [1999, 2],
        [2000, 3],
        [2001, 4],
        [2002, 5],
        [2003, 4],
        [2004, 3],
        [2005, 4],
        [2006, 1],
        [2007, 2],
        [2008, 7],
        [2009, 2],
        [2010, 1],
        [2011, 3],
        [2012, 1],
        [2013, 5],
        [2014, 3],
        [2015, 3],
        [2016, 1],
        [2017, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep mixtures of factor analysers",
      "link": "https://arxiv.org/abs/1206.4635",
      "year": 2012,
      "cited_by": 85,
      "authors": ["Yichuan Tang", "Ruslan Salakhutdinov", "Geoffrey Hinton"],
      "description": "An efficient way to learn deep density models that have many layers of latent variables is to learn one layer at a time using a model that has only one layer of latent variables. After learning each layer, samples from the posterior distributions for that layer are used as training data for learning the next layer. This approach is commonly used with Restricted Boltzmann Machines, which are undirected graphical models with a single hidden layer, but it can also be used with Mixtures of Factor Analysers (MFAs) which are directed graphical models. In this paper, we present a greedy layer-wise learning algorithm for Deep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be converted to an equivalent shallow MFA by multiplying together the factor loading matrices at different levels, learning and inference are much more efficient in a DMFA and the sharing of each lower-level factor loading matrix by many different higher level MFAs prevents overfitting. We demonstrate empirically that DMFAs learn better density models than both MFAs and two types of Restricted Boltzmann Machine on a wide variety of datasets.",
      "citation_histogram": [
        [2012, 1],
        [2013, 5],
        [2014, 6],
        [2015, 6],
        [2016, 11],
        [2017, 5],
        [2018, 11],
        [2019, 19],
        [2020, 8],
        [2021, 6],
        [2022, 5]
      ],
      "detail_extracted": true
    },
    {
      "title": "Preface to the special issue on connectionist symbol processing",
      "link": "http://www.csri.utoronto.ca/~hinton/absps/connectionist.pdf",
      "year": 1990,
      "cited_by": 85,
      "authors": ["Geoffrey E Hinton"],
      "description": "Connectionist networks are composed of relatively simple, neuron-like processing elements that store all their long-term knowledge in the strengths of the connections between processors. In the last decade there has been considerable progress in developing learning procedures for these networks that allow them to automatically construct their own internal representations [6-8, 10]. The learning procedures are typically applied in networks that map input vectors to output vectors via a few layers of\" hidden\" units. The network learns to dedicate particular hidden units to particular pieces or aspects of the input vector that are relevant in determining the output. The network generally learns to use distributed representations [5] in which each input vector is represented by activity in many different hidden units, and each hidden unit is involved in representing many different input vectors. Within the connectionist\u00a0\u2026",
      "citation_histogram": [
        [1990, 1],
        [1991, 7],
        [1992, 7],
        [1993, 4],
        [1994, 9],
        [1995, 6],
        [1996, 6],
        [1997, 4],
        [1998, 3],
        [1999, 3],
        [2000, 4],
        [2001, 1],
        [2002, 3],
        [2003, 2],
        [2004, 1],
        [2005, 2],
        [2006, 1],
        [2007, 1],
        [2008, 4],
        [2009, 3],
        [2010, 1],
        [2011, 1],
        [2012, 3],
        [2013, 3],
        [2014, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Proceedings of the national conference on artificial intelligence",
      "link": "https://scholar.google.com/scholar?cluster=3684552335028918034&hl=en&oi=scholarr",
      "year": 1983,
      "cited_by": 84,
      "authors": ["SE Fahlman", "GE Hinton", "TJ Sejnowski"],
      "description": null,
      "citation_histogram": [
        [1985, 27],
        [1986, 24],
        [1987, 32],
        [1988, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Analyzing and improving representations with the soft nearest neighbor loss",
      "link": "https://proceedings.mlr.press/v97/frosst19a.html",
      "year": 2019,
      "cited_by": 81,
      "authors": ["Nicholas Frosst", "Nicolas Papernot", "Geoffrey Hinton"],
      "description": "We explore and expand the Soft Nearest Neighbor Loss to measure the entanglement of class manifolds in representation space: ie, how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool, it provides insights into the evolution of class similarity structures during learning. Surprisingly, we find that maximizing the entanglement of representations of different classes in the hidden layers is beneficial for discrimination in the final layer, possibly because it encourages representations to identify class-independent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to better-calibrated estimates of uncertainty on outlier data but also marginally improved generalization. Data that is not from the training distribution can be recognized by observing that in the hidden layers, it has fewer than the normal number of neighbors from the predicted class.",
      "citation_histogram": [
        [2019, 8],
        [2020, 21],
        [2021, 25],
        [2022, 27]
      ],
      "detail_extracted": true
    },
    {
      "title": "Visualizing high-dimensional data using t-sne. journal of machine learning research",
      "link": "https://scholar.google.com/scholar?cluster=3697215906963383389&hl=en&oi=scholarr",
      "year": 2008,
      "cited_by": 81,
      "authors": ["Laurens Van Der Maaten", "Geoffrey Hinton"],
      "description": null,
      "citation_histogram": [
        [2016, 4],
        [2017, 2],
        [2018, 9],
        [2019, 23],
        [2020, 20],
        [2021, 15],
        [2022, 8]
      ],
      "detail_extracted": true
    },
    {
      "title": "GTM through time",
      "link": "https://digital-library.theiet.org/content/conferences/10.1049/cp_19970711",
      "year": 1997,
      "cited_by": 81,
      "authors": [
        "Christopher M Bishop",
        "Geoffrey E Hinton",
        "Iain GD Strachan"
      ],
      "description": "The standard GTM (generative topographic mapping) algorithm assumes that the data on which it is trained consists of independent, identically distributed (i.i.d.) vectors. For time series, however, the i.i.d. assumption is a poor approximation. In this paper we show how the GTM algorithm can be extended to model time series by incorporating it as the emission density in a hidden Markov model. Since GTM has discrete hidden states we are able to find a tractable EM algorithm, based on the forward-backward algorithm, to train the model. We illustrate the performance of GTM through time using flight recorder data from a helicopter.",
      "citation_histogram": [
        [1997, 1],
        [1998, 4],
        [1999, 1],
        [2000, 1],
        [2001, 1],
        [2002, 4],
        [2003, 5],
        [2004, 4],
        [2005, 5],
        [2006, 9],
        [2007, 1],
        [2008, 3],
        [2009, 3],
        [2010, 5],
        [2011, 3],
        [2012, 7],
        [2013, 4],
        [2014, 5],
        [2015, 2],
        [2016, 2],
        [2017, 1],
        [2018, 1],
        [2019, 2],
        [2020, 2],
        [2021, 3],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning multiple layers of features from tiny images. Vol. 1",
      "link": "https://scholar.google.com/scholar?cluster=744582818248305376&hl=en&oi=scholarr",
      "year": 2009,
      "cited_by": 78,
      "authors": ["Alex Krizhevsky", "Geoffrey Hinton"],
      "description": null,
      "citation_histogram": [
        [2015, 1],
        [2016, 1],
        [2017, 1],
        [2018, 12],
        [2019, 31],
        [2020, 14],
        [2021, 12],
        [2022, 6]
      ],
      "detail_extracted": true
    },
    {
      "title": "What kind of graphical model is the brain?",
      "link": "http://www.cs.toronto.edu/~hinton/absps/ijcai05.pdf",
      "year": 2005,
      "cited_by": 75,
      "authors": ["Geoffrey E Hinton"],
      "description": "If neurons are treated as latent variables, our visual systems are non-linear, densely-connected graphical models containing billions of variables and thousands of billions of parameters. Current algorithms would have difficulty learning a graphical model of this scale. Starting with an algorithm that has difficulty learning more than a few thousand parameters, I describe a series of progressively better learning algorithms all of which are designed to run on neuron-like hardware. The latest member of this series can learn deep, multi-layer belief nets quite rapidly. It turns a generic network with three hidden layers and 1.7 million connections into a very good generative model of handwritten digits. After learning, the model gives classification performance that is comparable to the best discriminative methods.",
      "citation_histogram": [
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 7],
        [2009, 1],
        [2010, 3],
        [2011, 3],
        [2012, 2],
        [2013, 5],
        [2014, 2],
        [2015, 7],
        [2016, 9],
        [2017, 5],
        [2018, 9],
        [2019, 10],
        [2020, 7]
      ],
      "detail_extracted": true
    },
    {
      "title": "Imputer: Sequence modelling via imputation and dynamic programming",
      "link": "http://proceedings.mlr.press/v119/chan20b.html",
      "year": 2020,
      "cited_by": 73,
      "authors": [
        "William Chan",
        "Chitwan Saharia",
        "Geoffrey Hinton",
        "Mohammad Norouzi",
        "Navdeep Jaitly"
      ],
      "description": "This paper presents the Imputer, a neural sequence model that generates output sequences iteratively via imputations. The Imputer is an iterative generation model, requiring only a constant number of generation steps independent of the number of input or output tokens. The Imputer can be trained to approximately marginalize over all possible alignments between the input and output sequences, and all possible generation orders. We present a tractable dynamic programming training algorithm, which yields a lower bound on the log marginal likelihood. When applied to end-to-end speech recognition, the Imputer outperforms prior non-autoregressive models and achieves competitive results to autoregressive models. On LibriSpeech test-other, the Imputer achieves 11.1 WER, outperforming CTC at 13.0 WER and seq2seq at 12.5 WER.",
      "citation_histogram": [
        [2020, 13],
        [2021, 36],
        [2022, 23]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning sparse networks using targeted dropout",
      "link": "https://arxiv.org/abs/1905.13678",
      "year": 2019,
      "cited_by": 73,
      "authors": [
        "Aidan N Gomez",
        "Ivan Zhang",
        "Siddhartha Rao Kamalakara",
        "Divyam Madaan",
        "Kevin Swersky",
        "Yarin Gal",
        "Geoffrey E Hinton"
      ],
      "description": "Neural networks are easier to optimise when they have many more weights than are required for modelling the mapping from inputs to outputs. This suggests a two-stage learning procedure that first learns a large net and then prunes away connections or hidden units. But standard training does not necessarily encourage nets to be amenable to pruning. We introduce targeted dropout, a method for training a neural network so that it is robust to subsequent pruning. Before computing the gradients for each weight update, targeted dropout stochastically selects a set of units or weights to be dropped using a simple self-reinforcing sparsity criterion and then computes the gradients for the remaining weights. The resulting network is robust to post hoc pruning of weights or units that frequently occur in the dropped sets. The method improves upon more complicated sparsifying regularisers while being simple to implement and easy to tune.",
      "citation_histogram": [
        [2019, 5],
        [2020, 21],
        [2021, 32],
        [2022, 15]
      ],
      "detail_extracted": true
    },
    {
      "title": "System and method for addressing overfitting in a neural network",
      "link": "https://patents.google.com/patent/US9406017B2/en",
      "year": 2016,
      "cited_by": 73,
      "authors": [
        "Geoffrey E Hinton",
        "Alexander Krizhevsky",
        "Ilya Sutskever",
        "Nitish Srivastva"
      ],
      "description": "A system for training a neural network. A switch is linked to feature detectors in at least some of the layers of the neural network. For each training case, the switch randomly selectively disables each of the feature detectors in accordance with a preconfigured probability. The weights from each training case are then normalized for applying the neural network to test data.",
      "citation_histogram": [
        [2017, 2],
        [2018, 2],
        [2019, 17],
        [2020, 24],
        [2021, 21],
        [2022, 7]
      ],
      "detail_extracted": true
    },
    {
      "title": "2012 AlexNet",
      "link": "https://scholar.google.com/scholar?cluster=14668977561691487040&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 73,
      "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"],
      "description": null,
      "citation_histogram": [
        [2012, 1],
        [2013, 1],
        [2014, 5],
        [2015, 8],
        [2016, 10],
        [2017, 16],
        [2018, 23],
        [2019, 8]
      ],
      "detail_extracted": true
    },
    {
      "title": "Rectified linear units improve restricted Boltzmann machines. 2010",
      "link": "https://scholar.google.com/scholar?cluster=12028798998732509317&hl=en&oi=scholarr",
      "year": 2010,
      "cited_by": 73,
      "authors": ["Vinod Nair", "Geoffrey E Hinton"],
      "description": null,
      "citation_histogram": [
        [2017, 2],
        [2018, 4],
        [2019, 2],
        [2020, 4],
        [2021, 4],
        [2022, 57]
      ],
      "detail_extracted": true
    },
    {
      "title": "Modeling natural images using gated MRFs",
      "link": "https://ieeexplore.ieee.org/abstract/document/6420839/",
      "year": 2013,
      "cited_by": 72,
      "authors": [
        "Marc'Aurelio Ranzato",
        "Volodymyr Mnih",
        "Joshua M Susskind",
        "Geoffrey E Hinton"
      ],
      "description": "This paper describes a Markov Random Field for real-valued image modeling that has two sets of latent variables. One set is used to gate the interactions between all pairs of pixels, while the second set determines the mean intensities of each pixel. This is a powerful model with a conditional distribution over the input that is Gaussian, with both mean and covariance determined by the configuration of latent variables, which is unlike previous models that were restricted to using Gaussians with either a fixed mean or a diagonal covariance matrix. Thanks to the increased flexibility, this gated MRF can generate more realistic samples after training on an unconstrained distribution of high-resolution natural images. Furthermore, the latent variables of the model can be inferred efficiently and can be used as very effective descriptors in recognition tasks. Both generation and discrimination drastically improve as layers of\u00a0\u2026",
      "citation_histogram": [
        [2013, 1],
        [2014, 8],
        [2015, 11],
        [2016, 9],
        [2017, 8],
        [2018, 7],
        [2019, 9],
        [2020, 5],
        [2021, 7],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "A desktop input device and interface for interactive 3d character animation",
      "link": "http://www.graphicsinterface.org/wp-content/uploads/gi2002-16.pdf",
      "year": 2002,
      "cited_by": 72,
      "authors": ["Sageev Oore", "Demetri Terzopoulos", "Geoffrey Hinton"],
      "description": "We present a novel input device and interface for interactively controlling the animation of graphical human character from a desktop environment. The trackers are embedded in a new physical design, which is both simple yet also provides significant benefits, and establishes a tangible interface with coordinate frames inherent to the character. A layered kinematic motion recording strategy accesses subsets of the total degrees of freedom of the character. We present the experiences of three novice users with the system, and that of a long-term user who has prior experience with other complex continuous interfaces.",
      "citation_histogram": [
        [2003, 3],
        [2004, 4],
        [2005, 8],
        [2006, 5],
        [2007, 6],
        [2008, 4],
        [2009, 2],
        [2010, 2],
        [2011, 3],
        [2012, 5],
        [2013, 5],
        [2014, 7],
        [2015, 3],
        [2016, 1],
        [2017, 6],
        [2018, 3],
        [2019, 3],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "The CIFAR-10 dataset (2014)",
      "link": "https://scholar.google.com/scholar?cluster=10149838758828836963&hl=en&oi=scholarr",
      "year": 2020,
      "cited_by": 71,
      "authors": ["Alex Krizhevsky", "Vinod Nair", "Geoffrey Hinton"],
      "description": null,
      "citation_histogram": [
        [2015, 1],
        [2016, 2],
        [2017, 1],
        [2018, 12],
        [2019, 8],
        [2020, 16],
        [2021, 20],
        [2022, 11]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parallelizing neural networks during training",
      "link": "https://patents.google.com/patent/US9811775B2/en",
      "year": 2017,
      "cited_by": 71,
      "authors": [
        "Alexander Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey E Hinton"
      ],
      "description": "A parallel convolutional neural network is provided. The CNN is implemented by a plurality of convolutional neural networks each on a respective processing node. Each CNN has a plurality of layers. A subset of the layers are interconnected between processing nodes such that activations are fed forward across nodes. The remaining subset is not so interconnected.",
      "citation_histogram": [
        [2017, 10],
        [2018, 8],
        [2019, 10],
        [2020, 19],
        [2021, 14],
        [2022, 10]
      ],
      "detail_extracted": true
    },
    {
      "title": "Modeling the joint density of two images under a variety of transformations",
      "link": "https://ieeexplore.ieee.org/abstract/document/5995541/",
      "year": 2011,
      "cited_by": 71,
      "authors": [
        "Joshua Susskind",
        "Geoffrey Hinton",
        "Roland Memisevic",
        "Marc Pollefeys"
      ],
      "description": "We describe a generative model of the relationship between two images. The model is defined as a factored three-way Boltzmann machine, in which hidden variables collaborate to define the joint correlation matrix for image pairs. Modeling the joint distribution over pairs makes it possible to efficiently match images that are the same according to a learned measure of similarity. We apply the model to several face matching tasks, and show that it learns to represent the input images using task-specific basis functions. Matching performance is superior to previous similar generative models, including recent conditional models of transformations. We also show that the model can be used as a plug-in matching score to perform invariant classification.",
      "citation_histogram": [
        [2011, 3],
        [2012, 6],
        [2013, 9],
        [2014, 8],
        [2015, 10],
        [2016, 12],
        [2017, 8],
        [2018, 3],
        [2019, 2],
        [2020, 2],
        [2021, 2],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Building adaptive interfaces with neural networks: The glove-talk pilot study",
      "link": "https://dl.acm.org/doi/abs/10.5555/647402.725595",
      "year": 1990,
      "cited_by": 71,
      "authors": ["Sidney Fels", "Geoffrey E Hinton"],
      "description": "Building adaptive interfaces with neural networks | Proceedings of the IFIP TC13 Third \nInterational Conference on Human-Computer Interaction ACM Digital Library home ACM home \nGoogle, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search \nJournals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital \nLibrary SearchSearch Advanced Search Browse Browse Digital Library Collections More \nHomeBrowse by TitleProceedingsINTERACT '90Building adaptive interfaces with neural \nnetworks: The glove-talk pilot study Article Share on Building adaptive interfaces with neural \nnetworks: The glove-talk pilot study Authors: Sidney Fels View Profile , Geoffrey E. Hinton View \nProfile Authors Info & Claims INTERACT '90: Proceedings of the IFIP TC13 Third Interational \nConference on Human-Computer InteractionAugust 1990 Pages 683\u2013688 Online:27 August 1990\u2026",
      "citation_histogram": [
        [1990, 1],
        [1991, 3],
        [1992, 2],
        [1993, 14],
        [1994, 10],
        [1995, 6],
        [1996, 6],
        [1997, 9],
        [1998, 5],
        [1999, 1],
        [2000, 1],
        [2001, 4],
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 1],
        [2006, 2],
        [2007, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1",
      "link": "https://scholar.google.com/scholar?cluster=10641706585958235704&hl=en&oi=scholarr",
      "year": 1986,
      "cited_by": 69,
      "authors": ["Geoffrey E Hinton", "JL McClelland", "DE Rumelhart"],
      "description": null,
      "citation_histogram": [
        [1989, 1],
        [1990, 1],
        [1991, 2],
        [1992, 1],
        [1993, 1],
        [1994, 1],
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 1],
        [1999, 1],
        [2000, 1],
        [2001, 4],
        [2002, 1],
        [2003, 6],
        [2004, 7],
        [2005, 14],
        [2006, 13],
        [2007, 5],
        [2008, 4],
        [2009, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Matrix capsules with EM routing",
      "link": "https://hci.iwr.uni-heidelberg.de/system/files/private/downloads/491388785/carsten_lueth_em_routing_slides.pdf",
      "year": 2018,
      "cited_by": 67,
      "authors": ["Sara Sabour", "Nicholas Frosst", "Geoffrey Hinton"],
      "description": "Capsule networks\u2019 core idea is to break up the neural net into chunks, or capsules, that work in teams and are assigned a portion of a problem; each capsule is pre-loaded with basic training on its portion of the object or process it will examine. The individual capsules work cooperatively, sharing their findings and contributing to solving the problem as a whole.",
      "citation_histogram": [
        [2018, 4],
        [2019, 9],
        [2020, 29],
        [2021, 17],
        [2022, 8]
      ],
      "detail_extracted": true
    },
    {
      "title": "Tensor analyzers",
      "link": "https://proceedings.mlr.press/v28/tang13.html",
      "year": 2013,
      "cited_by": 65,
      "authors": ["Yichuan Tang", "Ruslan Salakhutdinov", "Geoffrey Hinton"],
      "description": "Factor Analysis is a statistical method that seeks to explain linear variations in data by using unobserved latent variables. Due to its additive nature, it is not suitable for modeling data that is generated by multiple groups of latent factors which interact multiplicatively. In this paper, we introduce Tensor Analyzers which are a multilinear generalization of Factor Analyzers. We describe an efficient way of sampling from the posterior distribution over factor values and we demonstrate that these samples can be used in the EM algorithm for learning interesting mixture models of natural image patches. Tensor Analyzers can also accurately recognize a face under significant pose and illumination variations when given only one previous image of that face. We also show that Tensor Analyzers can be trained in an unsupervised, semi-supervised, or fully supervised settings.",
      "citation_histogram": [
        [2014, 6],
        [2015, 8],
        [2016, 14],
        [2017, 4],
        [2018, 6],
        [2019, 8],
        [2020, 7],
        [2021, 8],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning to parse images",
      "link": "https://proceedings.neurips.cc/paper/1999/hash/5a142a55461d5fef016acfb927fee0bd-Abstract.html",
      "year": 1999,
      "cited_by": 65,
      "authors": ["Geoffrey E Hinton", "Zoubin Ghahramani", "Yee Whye Teh"],
      "description": "We describe a class of probabilistic models that we call credibility networks. Using parse trees as internal representations of images, credibility networks are able to perform segmentation and recog (cid: 173) nition simultaneously, removing the need for ad hoc segmentation heuristics. Promising results in the problem of segmenting hand (cid: 173) written digits were obtained.",
      "citation_histogram": [
        [2000, 1],
        [2001, 3],
        [2002, 2],
        [2003, 5],
        [2004, 1],
        [2005, 2],
        [2006, 8],
        [2007, 6],
        [2008, 1],
        [2009, 3],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 1],
        [2014, 2],
        [2015, 2],
        [2016, 6],
        [2017, 7],
        [2018, 9],
        [2019, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "G\u2010maximization: an unsupervised learning procedure for discovering regularities",
      "link": "https://aip.scitation.org/doi/abs/10.1063/1.36234",
      "year": 1986,
      "cited_by": 63,
      "authors": ["Barak A Pearlmutter", "Geoffrey E Hinton"],
      "description": "Hill climbing is used to maximize an information theoretic measure of the difference betwen the actual behavior of a unit and the behavior that would be predicted by a statistician who knew the first order statistics of the inputs but believed them to be independent. This causes the unit to detect higher order correlations among its inputs. Initial simulations are presented, and seem encouraging. We describe an extension of the basic idea which makes it resemble competitive learning and which causes members of a population of these units to differentiate, each extracting different structure from the input.",
      "citation_histogram": [
        [1987, 4],
        [1988, 1],
        [1989, 3],
        [1990, 7],
        [1991, 5],
        [1992, 7],
        [1993, 5],
        [1994, 3],
        [1995, 3],
        [1996, 1],
        [1997, 1],
        [1998, 1],
        [1999, 1],
        [2000, 2],
        [2001, 2],
        [2002, 3],
        [2003, 2],
        [2004, 2],
        [2005, 2],
        [2006, 1],
        [2007, 2],
        [2008, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "The bootstrap Widrow-Hoff rule as a cluster-formation algorithm",
      "link": "https://ieeexplore.ieee.org/abstract/document/6796409/",
      "year": 1990,
      "cited_by": 62,
      "authors": ["Geoffrey E Hinton", "Steven J Nowlan"],
      "description": "An algorithm that is widely used for adaptive equalization in current modems is the \u201cbootstrap\u201d or \u201cdecision-directed\u201d version of the Widrow-Hoff rule. We show that this algorithm can be viewed as an unsupervised clustering algorithm in which the data points are transformed so that they form two clusters that are as tight as possible. The standard algorithm performs gradient ascent in a crude model of the log likelihood of generating the transformed data points from two gaussian distributions with fixed centers. Better convergence is achieved by using the exact gradient of the log likelihood.",
      "citation_histogram": [
        [1990, 2],
        [1991, 2],
        [1992, 6],
        [1993, 6],
        [1994, 2],
        [1995, 7],
        [1996, 3],
        [1997, 5],
        [1998, 3],
        [1999, 1],
        [2000, 1],
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 1],
        [2006, 4],
        [2007, 2],
        [2008, 2],
        [2009, 1],
        [2010, 2],
        [2011, 5]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep learning",
      "link": "https://scholar.google.com/scholar?cluster=11688801935440556946&hl=en&oi=scholarr",
      "year": 2015,
      "cited_by": 60,
      "authors": ["B Yoshua", "L Yann", "H Geoffrey"],
      "description": null,
      "citation_histogram": [
        [2014, 1],
        [2015, 14],
        [2016, 3],
        [2017, 7],
        [2018, 9],
        [2019, 10],
        [2020, 8],
        [2021, 8]
      ],
      "detail_extracted": true
    },
    {
      "title": "Speech recognition with deep recurrent neural networks IEEE International Conference on Acoustics",
      "link": "https://scholar.google.com/scholar?cluster=789948345631245743&hl=en&oi=scholarr",
      "year": 2013,
      "cited_by": 60,
      "authors": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"],
      "description": null,
      "citation_histogram": [
        [2016, 3],
        [2017, 6],
        [2018, 17],
        [2019, 9],
        [2020, 6],
        [2021, 13],
        [2022, 6]
      ],
      "detail_extracted": true
    },
    {
      "title": "Cifar-10",
      "link": "https://scholar.google.com/scholar?cluster=977027640734179256&hl=en&oi=scholarr",
      "year": 2009,
      "cited_by": 60,
      "authors": ["Alex Krizhevsky", "Vinod Nair", "Geoffrey Hinton"],
      "description": null,
      "citation_histogram": [
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 2],
        [2014, 6],
        [2015, 10],
        [2016, 12],
        [2017, 16],
        [2018, 11]
      ],
      "detail_extracted": true
    },
    {
      "title": "Proceedings of the 24th international conference on Machine learning",
      "link": "https://scholar.google.com/scholar?cluster=17989377543595634210&hl=en&oi=scholarr",
      "year": 2007,
      "cited_by": 60,
      "authors": ["Ruslan Salakhutdinov", "Andriy Mnih", "Geoffrey Hinton"],
      "description": null,
      "citation_histogram": [
        [2011, 2],
        [2012, 1],
        [2013, 1],
        [2014, 3],
        [2015, 1],
        [2016, 11],
        [2017, 10],
        [2018, 6],
        [2019, 13],
        [2020, 7],
        [2021, 5]
      ],
      "detail_extracted": true
    },
    {
      "title": "Self supervised boosting",
      "link": "https://proceedings.neurips.cc/paper/2002/hash/cd0cbcc668fe4bc58e0af3cc7e0a653d-Abstract.html",
      "year": 2002,
      "cited_by": 60,
      "authors": ["Max Welling", "Richard Zemel", "Geoffrey E Hinton"],
      "description": "Boosting algorithms and successful applications thereof abound for classification and regression learning problems, but not for unsupervised learning. We propose a sequential approach to adding features to a random field model by training them to improve classification performance between the data and an equal-sized sample of \u201cnegative examples\u201d generated from the model\u2019s current estimate of the data density. Training in each boosting round proceeds in three stages: first we sample negative examples from the model\u2019s current Boltzmann distribution. Next, a feature is trained to improve classification performance between data and negative examples. Finally, a coefficient is learned which determines the importance of this feature relative to ones already in the pool. Negative examples only need to be generated once to learn each new feature. The validity of the approach is demonstrated on binary digits and continuous synthetic data.",
      "citation_histogram": [
        [2003, 1],
        [2004, 2],
        [2005, 4],
        [2006, 3],
        [2007, 5],
        [2008, 3],
        [2009, 1],
        [2010, 4],
        [2011, 1],
        [2012, 1],
        [2013, 9],
        [2014, 9],
        [2015, 6],
        [2016, 1],
        [2017, 2],
        [2018, 6],
        [2019, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "others. 1988",
      "link": "https://scholar.google.com/scholar?cluster=13359969944601285393&hl=en&oi=scholarr",
      "year": 1988,
      "cited_by": 60,
      "authors": [
        "David E Rumelhart",
        "Geoffrey E Hinton",
        "Ronald J Williams"
      ],
      "description": null,
      "citation_histogram": [
        [2002, 2],
        [2003, 1],
        [2004, 1],
        [2005, 10],
        [2006, 12],
        [2007, 16],
        [2008, 12],
        [2009, 6]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep Belief Nets.",
      "link": "https://www.cs.toronto.edu/~hinton/ucltutorial.pdf",
      "year": 2010,
      "cited_by": 59,
      "authors": ["Geoffrey E Hinton"],
      "description": "\u2022 High-dimensional data (eg more than 100 dimensions)\u2022 The noise is not sufficient to obscure the structure in the data if we process it right.\u2022 There is a huge amount of structure in the data, but the structure is too complicated to be represented by a simple model.",
      "citation_histogram": [
        [2013, 2],
        [2014, 2],
        [2015, 2],
        [2016, 4],
        [2017, 6],
        [2018, 9],
        [2019, 12],
        [2020, 9],
        [2021, 9],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Mean field networks that learn to discriminate temporally distorted strings",
      "link": "https://www.sciencedirect.com/science/article/pii/B9781483214481500081",
      "year": 1991,
      "cited_by": 59,
      "authors": ["Christopher KI Williams", "Geoffrey E Hinton"],
      "description": "Neural networks can be used to discriminate between very similar phonemes and they can handle the variability in time of occurrence by using a time-delay architecture followed by a temporal integration (Lang, Hinton and Waibel, 1990). So far, however, neural networks have been less successful at handling longer duration events that require something equivalent to \u201ctime warping\u201d in order to match stored knowledge to the data. We present a type of mean field network (MFN) with tied weights that is capable of approximating the recognizer for a hidden markov model (HMM). In the process of settling to a stable state, the MFN finds a blend of likely ways of generating the input string given its internal model of the probabilities of transitions between hidden states and the probabilities of input symbols given a hidden state. This blend is a heuristic approximation to the full set of path probabilities that is implicitly\u00a0\u2026",
      "citation_histogram": [
        [1991, 1],
        [1992, 1],
        [1993, 1],
        [1994, 4],
        [1995, 4],
        [1996, 5],
        [1997, 5],
        [1998, 5],
        [1999, 4],
        [2000, 2],
        [2001, 2],
        [2002, 5],
        [2003, 2],
        [2004, 1],
        [2005, 2],
        [2006, 1],
        [2007, 1],
        [2008, 2],
        [2009, 2],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 1],
        [2014, 2],
        [2015, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "A hierarchical community of experts",
      "link": "https://link.springer.com/chapter/10.1007/978-94-011-5014-9_17",
      "year": 1998,
      "cited_by": 57,
      "authors": ["Geoffrey E Hinton", "Brian Sallans", "Zoubin Ghahramani"],
      "description": " We describe a directed acyclic graphical model that contains a hierarchy of linear units and a mechanism for dynamically selecting an appropriate subset of these units to model each observation. The non-linear selection mechanism is a hierarchy of binary units each of which gates the output of one of the linear units. There are no connections from linear units to binary units, so the generative model can be viewed as a logistic belief net (Neal 1992) which selects a skeleton linear model from among the available linear units. We show that Gibbs sampling can be used to learn the parameters of the linear and binary units even when the sampling is so brief that the Markov chain is far from equilibrium.",
      "citation_histogram": [
        [1998, 2],
        [1999, 3],
        [2000, 4],
        [2001, 6],
        [2002, 3],
        [2003, 4],
        [2004, 3],
        [2005, 1],
        [2006, 4],
        [2007, 3],
        [2008, 1],
        [2009, 2],
        [2010, 4],
        [2011, 4],
        [2012, 1],
        [2013, 1],
        [2014, 2],
        [2015, 1],
        [2016, 3],
        [2017, 2],
        [2018, 1],
        [2019, 1],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "The Toronto face dataset",
      "link": "https://scholar.google.com/scholar?cluster=17396760066458691900&hl=en&oi=scholarr",
      "year": 2010,
      "cited_by": 56,
      "authors": ["Joshua Susskind", "Adam Anderson", "Geoffrey E Hinton"],
      "description": null,
      "citation_histogram": [
        [2012, 2],
        [2013, 5],
        [2014, 9],
        [2015, 6],
        [2016, 3],
        [2017, 2],
        [2018, 2],
        [2019, 5],
        [2020, 8],
        [2021, 10],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning mixture models of spatial coherence",
      "link": "https://direct.mit.edu/neco/article-abstract/5/2/267/5709",
      "year": 1993,
      "cited_by": 56,
      "authors": ["Suzanna Becker", "Geoffrey E Hinton"],
      "description": " We have previously described an unsupervised learning procedure that discovers spatially coherent properties of the world by maximizing the information that parameters extracted from different parts of the sensory input convey about some common underlying cause. When given random dot stereograms of curved surfaces, this procedure learns to extract surface depth because that is the property that is coherent across space. It also learns how to interpolate the depth at one location from the depths at nearby locations (Becker and Hinton 1992b). In this paper, we propose two new models that handle surfaces with discontinuities. The first model attempts to detect cases of discontinuities and reject them. The second model develops a mixture of expert interpolators. It learns to detect the locations of discontinuities and to invoke specialized, asymmetric interpolators that do not cross the discontinuities.",
      "citation_histogram": [
        [1994, 1],
        [1995, 1],
        [1996, 9],
        [1997, 3],
        [1998, 2],
        [1999, 1],
        [2000, 5],
        [2001, 5],
        [2002, 8],
        [2003, 3],
        [2004, 3],
        [2005, 2],
        [2006, 1],
        [2007, 2],
        [2008, 2],
        [2009, 1],
        [2010, 3],
        [2011, 1],
        [2012, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Detecting and diagnosing adversarial images with class-conditional capsule reconstructions",
      "link": "https://arxiv.org/abs/1907.02957",
      "year": 2019,
      "cited_by": 54,
      "authors": [
        "Yao Qin",
        "Nicholas Frosst",
        "Sara Sabour",
        "Colin Raffel",
        "Garrison Cottrell",
        "Geoffrey Hinton"
      ],
      "description": "Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples.",
      "citation_histogram": [
        [2018, 2],
        [2019, 18],
        [2020, 27],
        [2021, 7]
      ],
      "detail_extracted": true
    },
    {
      "title": "Dynamic routing between capsules",
      "link": "https://scholar.google.com/scholar?cluster=6221172228025714915&hl=en&oi=scholarr",
      "year": 2017,
      "cited_by": 53,
      "authors": ["Sabour Sara", "Frosst Nicholas", "E Hinton Geoffrey"],
      "description": null,
      "citation_histogram": [
        [2018, 6],
        [2019, 9],
        [2020, 13],
        [2021, 14],
        [2022, 8]
      ],
      "detail_extracted": true
    },
    {
      "title": "Distilling the knowledge in a neural network. arXiv preprint arXiv: 150302531",
      "link": "https://scholar.google.com/scholar?cluster=15700055920826360076&hl=en&oi=scholarr",
      "year": 2015,
      "cited_by": 52,
      "authors": ["G Hinton", "Oriol Vinyals", "Jeffrey Dean"],
      "description": null,
      "citation_histogram": [
        [2017, 1],
        [2018, 1],
        [2019, 4],
        [2020, 10],
        [2021, 12],
        [2022, 23]
      ],
      "detail_extracted": true
    },
    {
      "title": "Temporal-kernel recurrent neural networks",
      "link": "https://www.sciencedirect.com/science/article/pii/S0893608009002664",
      "year": 2010,
      "cited_by": 52,
      "authors": ["Ilya Sutskever", "Geoffrey Hinton"],
      "description": "A Recurrent Neural Network (RNN) is a powerful connectionist model that can be applied to many challenging sequential problems, including problems that naturally arise in language and speech. However, RNNs are extremely hard to train on problems that have long-term dependencies, where it is necessary to remember events for many timesteps before using them to make a prediction.In this paper we consider the problem of training RNNs to predict sequences that exhibit significant long-term dependencies, focusing on a serial recall task where the RNN needs to remember a sequence of characters for a large number of steps before reconstructing it. We introduce the Temporal-Kernel Recurrent Neural Network (TKRNN), which is a variant of the RNN that can cope with long-term dependencies much more easily than a standard RNN, and show that the TKRNN develops short-term memory that successfully\u00a0\u2026",
      "citation_histogram": [
        [2010, 1],
        [2011, 6],
        [2012, 6],
        [2013, 7],
        [2014, 4],
        [2015, 2],
        [2016, 8],
        [2017, 6],
        [2018, 4],
        [2019, 7],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Neural Networks for Machine Learning, Lecture 15a",
      "link": "https://www.cs.toronto.edu/~hinton/coursera/lecture15/lec15.pdf",
      "year": 2019,
      "cited_by": 51,
      "authors": ["GE Hinton"],
      "description": "\u2022 When we train the first RBM in the stack we use the same trick.\u2013We treat the word counts as probabilities, but we make the visible to hidden weights N times bigger than the hidden to visible because we have N observations from the probability distribution.",
      "citation_histogram": [
        [2015, 1],
        [2016, 2],
        [2017, 2],
        [2018, 7],
        [2019, 5],
        [2020, 6],
        [2021, 8],
        [2022, 19]
      ],
      "detail_extracted": true
    },
    {
      "title": "Layer normalization. CoRR abs/1607.06450 (2016)",
      "link": "https://scholar.google.com/scholar?cluster=509433928898618251&hl=en&oi=scholarr",
      "year": 2016,
      "cited_by": 51,
      "authors": ["Lei Jimmy Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton"],
      "description": null,
      "citation_histogram": [
        [2018, 5],
        [2019, 7],
        [2020, 6],
        [2021, 16],
        [2022, 16]
      ],
      "detail_extracted": true
    },
    {
      "title": "Analysis-by-synthesis by learning to invert generative black boxes",
      "link": "https://link.springer.com/chapter/10.1007/978-3-540-87536-9_99",
      "year": 2008,
      "cited_by": 49,
      "authors": ["Vinod Nair", "Josh Susskind", "Geoffrey E Hinton"],
      "description": " For learning meaningful representations of data, a rich source of prior knowledge may come in the form of a generative black box, e.g. a graphics program that generates realistic facial images. We consider the problem of learning the inverse of a given generative model from data. The problem is non-trivial because it is difficult to create labelled training cases by hand, and the generative mapping is a black box in the sense that there is no analytic expression for its gradient. We describe a way of training a feedforward neural network that starts with just one labelled training example and uses the generative black box to \u201cbreed\u201d more training data. As learning proceeds, the training set evolves and the labels that the network assigns to unlabelled training data converge to their correct values. We demonstrate our approach by learning to invert a generative model of eyes and an active appearance model of faces.",
      "citation_histogram": [
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 1],
        [2012, 3],
        [2013, 5],
        [2014, 3],
        [2015, 3],
        [2016, 11],
        [2017, 5],
        [2018, 7],
        [2019, 3],
        [2020, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Spiking boltzmann machines",
      "link": "https://proceedings.neurips.cc/paper/1999/hash/62889e73828c756c961c5a6d6c01a463-Abstract.html",
      "year": 2000,
      "cited_by": 49,
      "authors": ["Geoffrey E Hinton", "Andrew Brown"],
      "description": "We first show how to represent sharp posterior probability distribu (cid: 173) tions using real valued coefficients on broadly-tuned basis functions. Then we show how the precise times of spikes can be used to con (cid: 173) vey the real-valued coefficients on the basis functions quickly and accurately. Finally we describe a simple simulation in which spik (cid: 173) ing neurons learn to model an image sequence by fitting a dynamic generative model.",
      "citation_histogram": [
        [2002, 1],
        [2003, 1],
        [2004, 3],
        [2005, 2],
        [2006, 2],
        [2007, 4],
        [2008, 1],
        [2009, 1],
        [2010, 3],
        [2011, 7],
        [2012, 2],
        [2013, 1],
        [2014, 3],
        [2015, 4],
        [2016, 5],
        [2017, 2],
        [2018, 2],
        [2019, 1],
        [2020, 2],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Adaptive soft weight tying using gaussian mixtures",
      "link": "https://proceedings.neurips.cc/paper/1991/hash/05f971b5ec196b8c65b75d2ef8267331-Abstract.html",
      "year": 1991,
      "cited_by": 49,
      "authors": ["Steven Nowlan", "Geoffrey E Hinton"],
      "description": "One way of simplifying neural networks so they generalize better is to add an extra t. erm 10 the error fUll ction that will penalize complexit. y.\\Ve propose a new penalt. y t. erm in which the dist rihution of weight values is modelled as a mixture of multiple gaussians. C nder this model, a set of weights is simple if the weights can be clustered into subsets so that the weights in each cluster have similar values. We allow the parameters of the mixture model to adapt at t. he same time as t. he network learns. Simulations demonstrate that this complexity term is more effective than previous complexity terms.",
      "citation_histogram": [
        [1992, 1],
        [1993, 6],
        [1994, 8],
        [1995, 10],
        [1996, 7],
        [1997, 2],
        [1998, 1],
        [1999, 2],
        [2000, 1],
        [2001, 1],
        [2002, 1],
        [2003, 3],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Dimensionality reduction and prior knowledge in e-set recognition",
      "link": "https://proceedings.neurips.cc/paper/1989/hash/289dff07669d7a23de0ef88d2f7129e7-Abstract.html",
      "year": 1989,
      "cited_by": 49,
      "authors": ["Kevin Lang", "Geoffrey E Hinton"],
      "description": "It is well known that when an automatic learning algorithm is applied to a fixed corpus of data, the size of the corpus places an upper bound on the number of degrees of freedom that the model can contain if it is to generalize well. Because the amount of hardware in a neural network typically increases with the dimensionality of its inputs, it can be challenging to build a high-performance network for classifying large input patterns. In this paper, several techniques for addressing this problem are discussed in the context of an isolated word recognition task.",
      "citation_histogram": [
        [1990, 1],
        [1991, 2],
        [1992, 4],
        [1993, 2],
        [1994, 2],
        [1995, 2],
        [1996, 5],
        [1997, 3],
        [1998, 1],
        [1999, 3],
        [2000, 3],
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 3],
        [2010, 1],
        [2011, 5],
        [2012, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Comparing classification methods for longitudinal fMRI studies",
      "link": "https://direct.mit.edu/neco/article-abstract/22/11/2729/7587",
      "year": 2010,
      "cited_by": 48,
      "authors": [
        "Tanya Schmah",
        "Grigori Yourganov",
        "Richard S Zemel",
        "Geoffrey E Hinton",
        "Steven L Small",
        "Stephen C Strother"
      ],
      "description": " We compare 10 methods of classifying fMRI volumes by applying them to data from a                     longitudinal study of stroke recovery: adaptive Fisher's linear and quadratic                     discriminant; gaussian naive Bayes; support vector machines with linear,                     quadratic, and radial basis function (RBF) kernels; logistic regression; two                     novel methods based on pairs of restricted Boltzmann machines (RBM); and                     K-nearest neighbors. All methods were tested on three binary classification                     tasks, and their out-of-sample classification accuracies are compared. The                     relative performance of the methods varies considerably across subjects and                     classification tasks. The best overall performers were adaptive quadratic                     discriminant, support vector machines with RBF kernels, and generatively trained                     pairs of RBMs.",
      "citation_histogram": [
        [2010, 1],
        [2011, 5],
        [2012, 6],
        [2013, 8],
        [2014, 6],
        [2015, 5],
        [2016, 2],
        [2017, 3],
        [2018, 2],
        [2019, 4],
        [2020, 4],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "How to do backpropagation in a brain",
      "link": "http://www.cs.toronto.edu/~hinton/backpropincortex2007.pdf",
      "year": 2007,
      "cited_by": 47,
      "authors": ["Geoffrey Hinton"],
      "description": "How to do backpropagation in a brain Page 1 How to do backpropagation in a brain Geoffrey \nHinton Canadian Institute for Advanced Research & University of Toronto 1 Page 2 What is \nwrong with back-propagation? \u2022 It requires labeled training data. (fixed) \u2013 Almost all real data \nis unlabeled. \u2013 The brain needs to fit about 10^14 connection weights in only about 10^9 \nseconds. Labels cannot possibly provide enough information. \u2022 The learning time does not \nscale well for many hidden layers. (fixed) \u2022 The neurons need to send two different types of \nsignal \u2013 Forward pass: signal = activity = y \u2013 Backward pass: signal = dE/dy Page 3 Training a \ndeep network \u2022 First train a layer of features that receive input directly from the pixels. \u2022 Then \ntreat the activations of the trained features as if they were pixels and learn features of features \nin a second hidden layer. \u2022 Each time we add another layer of features we improve a bound on \u2026",
      "citation_histogram": [
        [2014, 1],
        [2015, 5],
        [2016, 4],
        [2017, 5],
        [2018, 6],
        [2019, 6],
        [2020, 10],
        [2021, 9],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Neighbourhood component analysis",
      "link": "http://www.iro.umontreal.ca/~lisa/workshop2004/abstract_roweis.html",
      "year": 2004,
      "cited_by": 47,
      "authors": ["Sam Roweis", "Geoffrey Hinton", "Ruslan Salakhutdinov"],
      "description": "Say I give you a dataset of N points {x_n} in D dimensions. Can you find for me (up to rotation and isotropic scaling) a projection matrix A (of size d by D) such that when you apply nearest neighbour classification to the point set {y_n= A x_n} you get the best possible performance? Nearest neighbour classification is a very simple nonparametric method for supervised learning, and has several appealing properties: the decision surfaces are nonlinear, the quality of the predictions automatically improve as the amount of training data increases, and there is only a single hyperparameter to be tuned.However, there are two significant problems. First, we must define what we mean by\" nearest\", in other words we must specify a metric on the input space. Second, the computational load of the classifier is quite high at test time since we must store and search through the entire training set to find the neighbours of a query\u00a0\u2026",
      "citation_histogram": [
        [2006, 1],
        [2007, 2],
        [2008, 1],
        [2009, 1],
        [2010, 2],
        [2011, 7],
        [2012, 7],
        [2013, 2],
        [2014, 6],
        [2015, 3],
        [2016, 7],
        [2017, 6]
      ],
      "detail_extracted": true
    },
    {
      "title": "Pix2seq: A language modeling framework for object detection",
      "link": "https://arxiv.org/abs/2109.10852",
      "year": 2021,
      "cited_by": 46,
      "authors": [
        "Ting Chen",
        "Saurabh Saxena",
        "Lala Li",
        "David J Fleet",
        "Geoffrey Hinton"
      ],
      "description": "This paper presents Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we simply cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural net to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural net knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.",
      "citation_histogram": [
        [2021, 4],
        [2022, 42]
      ],
      "detail_extracted": true
    },
    {
      "title": "Spatial Coherence as an Internal Teacher for a Neural Network",
      "link": null,
      "year": 1995,
      "cited_by": 46,
      "authors": ["Geoffrey E Hinton"],
      "description": null,
      "citation_histogram": [
        [1989, 2],
        [1990, 1],
        [1991, 6],
        [1992, 3],
        [1993, 6],
        [1994, 3],
        [1995, 2],
        [1996, 2],
        [1997, 1],
        [1998, 2],
        [1999, 1],
        [2000, 2],
        [2001, 3],
        [2002, 6],
        [2003, 1],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Darccc: Detecting adversaries by reconstruction from class conditional capsules",
      "link": "https://arxiv.org/abs/1811.06969",
      "year": 2018,
      "cited_by": 45,
      "authors": ["Nicholas Frosst", "Sara Sabour", "Geoffrey Hinton"],
      "description": "We present a simple technique that allows capsule models to detect adversarial images. In addition to being trained to classify images, the capsule model is trained to reconstruct the images from the pose parameters and identity of the correct top-level capsule. Adversarial images do not look like a typical member of the predicted class and they have much larger reconstruction errors when the reconstruction is produced from the top-level capsule for that class. We show that setting a threshold on the  distance between the input image and its reconstruction from the winning capsule is very effective at detecting adversarial images for three different datasets. The same technique works quite well for CNNs that have been trained to reconstruct the image from all or part of the last hidden layer before the softmax. We then explore a stronger, white-box attack that takes the reconstruction error into account. This attack is able to fool our detection technique but in order to make the model change its prediction to another class, the attack must typically make the \"adversarial\" image resemble images of the other class.",
      "citation_histogram": [
        [2019, 12],
        [2020, 13],
        [2021, 14],
        [2022, 5]
      ],
      "detail_extracted": true
    },
    {
      "title": "Multiple relational embedding",
      "link": "https://proceedings.neurips.cc/paper/2004/hash/148260a1ce4fe4907df4cd475c442e28-Abstract.html",
      "year": 2004,
      "cited_by": 45,
      "authors": ["Roland Memisevic", "Geoffrey E Hinton"],
      "description": "We describe a way of using multiple different types of similarity rela-tionship to learn a low-dimensional embedding of a dataset. Our method chooses different, possibly overlapping representations of similarity by individually reweighting the dimensions of a common underlying latent space. When applied to a single similarity relation that is based on Eu-clidean distances between the input data points, the method reduces to simple dimensionality reduction. If additional information is available about the dataset or about subsets of it, we can use this information to clean up or otherwise improve the embedding. We demonstrate the po-tential usefulness of this form of semi-supervised dimensionality reduc-tion on some simple examples.",
      "citation_histogram": [
        [2005, 3],
        [2006, 5],
        [2007, 2],
        [2008, 2],
        [2009, 3],
        [2010, 2],
        [2011, 2],
        [2012, 7],
        [2013, 3],
        [2014, 1],
        [2015, 5],
        [2016, 1],
        [2017, 1],
        [2018, 2],
        [2019, 2],
        [2020, 1],
        [2021, 1],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Discovering viewpoint-invariant relationships that characterize objects",
      "link": "https://proceedings.neurips.cc/paper/1990/hash/6faa8040da20ef399b63a72d0e4ab575-Abstract.html",
      "year": 1990,
      "cited_by": 45,
      "authors": ["Richard Zemel", "Geoffrey E Hinton"],
      "description": "Using an unsupervised learning procedure, a network is trained on an en (cid: 173) semble of images of the same two-dimensional object at different positions, orientations and sizes. Each half of the network\" sees\" one fragment of the object, and tries to produce as output a set of 4 parameters that have high mutual information with the 4 parameters output by the other half of the network. Given the ensemble of training patterns, the 4 parameters on which the two halves of the network can agree are the position, orientation, and size of the whole object, or some recoding of them. After training, the network can reject instances of other shapes by using the fact that the predictions made by its two halves disagree. If two competing networks are trained on an unlabelled mixture of images of two objects, they cluster the training cases on the basis of the objects' shapes, independently of the position, orientation, and size.",
      "citation_histogram": [
        [1991, 5],
        [1992, 1],
        [1993, 7],
        [1994, 2],
        [1995, 3],
        [1996, 8],
        [1997, 2],
        [1998, 2],
        [1999, 2],
        [2000, 1],
        [2001, 1],
        [2002, 2],
        [2003, 1],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 2],
        [2010, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Solving random-dot stereograms using the heat equation",
      "link": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.555.5431&rep=rep1&type=pdf",
      "year": 1985,
      "cited_by": 45,
      "authors": ["Richard Szeliski", "Geoffrey Hinton"],
      "description": "Many parallel algorithms have been proposed for finding the correct matches between feature points in random dot stereograms. Some algorithms have used local support funct ions and have achieved globally good solut ions by using relaxation in a parallel network. Recently, Prazdny 1 has shown that iterat ion is unnecessary if a much larger support function is used, and that this support function can be desiqned to work for stereograms containing transparent surfaces. We describe a simple global support function that can be efficiently implemented by relaxation in a network with only local connectivity. This function, which is the solution to the heat diffusion equation, does not work as well as Prazdny's. By using the difference of two heat equations, we can improve the performance'and get results almost identical to Prazdny's. at a lower computational cost.",
      "citation_histogram": [
        [1986, 2],
        [1987, 2],
        [1988, 1],
        [1989, 1],
        [1990, 1],
        [1991, 2],
        [1992, 2],
        [1993, 1],
        [1994, 1],
        [1995, 1],
        [1996, 5],
        [1997, 2],
        [1998, 2],
        [1999, 5],
        [2000, 2],
        [2001, 2],
        [2002, 1],
        [2003, 2],
        [2004, 1],
        [2005, 1],
        [2006, 2],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 2],
        [2011, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Products of hidden markov models",
      "link": "http://proceedings.mlr.press/r3/brown01a.html",
      "year": 2001,
      "cited_by": 44,
      "authors": ["Andrew D Brown", "Geoffrey E Hinton"],
      "description": "We present products of hidden Markov models (PoHMM\u2019s), a way of combining HMM\u2019s to form a distributed state time series model. Inference in a PoHMM is tractable and efficient. Learning of the parameters, although intractable, can be effectively done using the Product of Experts learning rule. The distributed state helps the model to explain data which has multiple causes, and the fact that each model need only explain part of the data means a PoHMM can capture longer range structure than an HMM is capable of. We show some results on modelling character strings, a simple language task and the symbolic family trees problem, which highlight these advantages.",
      "citation_histogram": [
        [2000, 1],
        [2001, 3],
        [2002, 3],
        [2003, 4],
        [2004, 2],
        [2005, 2],
        [2006, 7],
        [2007, 1],
        [2008, 2],
        [2009, 4],
        [2010, 2],
        [2011, 1],
        [2012, 2],
        [2013, 1],
        [2014, 2],
        [2015, 1],
        [2016, 1],
        [2017, 1],
        [2018, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Williams, RJ.(1986)",
      "link": "https://scholar.google.com/scholar?cluster=15123546043827226524&hl=en&oi=scholarr",
      "year": 85,
      "cited_by": 43,
      "authors": ["David E Rumelhart", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [1987, 1],
        [1988, 1],
        [1989, 2],
        [1990, 1],
        [1991, 3],
        [1992, 1],
        [1993, 1],
        [1994, 1],
        [1995, 2],
        [1996, 2],
        [1997, 3],
        [1998, 1],
        [1999, 3],
        [2000, 3],
        [2001, 2],
        [2002, 1],
        [2003, 1],
        [2004, 2],
        [2005, 3],
        [2006, 2],
        [2007, 2],
        [2008, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "& Sainath, T.(2012). Deep neural networks for acoustic modeling in speech recognition",
      "link": "https://scholar.google.com/scholar?cluster=7633591132815720432&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 42,
      "authors": [
        "G Hinton",
        "L Deng",
        "Dong Yu",
        "G Dahl",
        "AR Mohamed",
        "N Jaitly"
      ],
      "description": null,
      "citation_histogram": [
        [2013, 1],
        [2014, 3],
        [2015, 3],
        [2016, 2],
        [2017, 3],
        [2018, 2],
        [2019, 7],
        [2020, 10],
        [2021, 8],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Dark knowledge",
      "link": "http://www.jumpstartgym.com/wp-content/uploads/2010/viewp/Dark-knowledge-ttic-home.pdf",
      "year": 2014,
      "cited_by": 39,
      "authors": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"],
      "description": "Dark knowledge Page 1 Dark knowledge Geoffrey Hinton, Oriol Vinyals & Jeff Dean Google \nInc. Page 2 The conflicting constraints of learning and using \u2022 The easiest way to extract a lot \nof knowledge from the training data is to learn many different models in parallel. \u2013 We want to \nmake the models as different as possible to minimize the correlations between their errors. \u2013 \nWe can use different initializations or different architectures or different subsets of the training \ndata. \u2013 It is helpful to over-fit the individual models. \u2022 A test time we average the predictions of \nall the models or of a selected subset of good models that make different errors. \u2013 That\u2019s how \nalmost all ML competitions are won (eg Netflix) Page 3 Why ensembles are bad at test time \n\u2022 A big ensemble is highly redundant. It has very very little knowledge per parameter. \u2022 At \ntest time we want to minimize the amount of computation and the memory footprint. \u2013 These \u2026",
      "citation_histogram": [
        [2014, 1],
        [2015, 1],
        [2016, 3],
        [2017, 4],
        [2018, 4],
        [2019, 5],
        [2020, 3],
        [2021, 8],
        [2022, 9]
      ],
      "detail_extracted": true
    },
    {
      "title": "Illustrative language understanding: Large-scale visual grounding with image search",
      "link": "https://aclanthology.org/P18-1085/",
      "year": 2018,
      "cited_by": 38,
      "authors": ["Jamie Kiros", "William Chan", "Geoffrey Hinton"],
      "description": "We introduce Picturebook, a large-scale lookup operation to ground language via \u2018snapshots\u2019 of our physical world accessed through image search. For each word in a vocabulary, we extract the top-k images from Google image search and feed the images through a convolutional network to extract a word embedding. We introduce a multimodal gating function to fuse our Picturebook embeddings with other word representations. We also introduce Inverse Picturebook, a mechanism to map a Picturebook embedding back into words. We experiment and report results across a wide range of tasks: word similarity, natural language inference, semantic relatedness, sentiment/topic classification, image-sentence ranking and machine translation. We also show that gate activations corresponding to Picturebook embeddings are highly correlated to human judgments of concreteness ratings.",
      "citation_histogram": [
        [2018, 3],
        [2019, 10],
        [2020, 9],
        [2021, 9],
        [2022, 7]
      ],
      "detail_extracted": true
    },
    {
      "title": "Discovering multiple constraints that are frequently approximately satisfied",
      "link": "https://arxiv.org/abs/1301.2278",
      "year": 2013,
      "cited_by": 38,
      "authors": ["Geoffrey E Hinton", "Yee Whye Teh"],
      "description": "Some high-dimensional data.sets can be modelled by assuming that there are many different linear constraints, each of which is Frequently Approximately Satisfied (FAS) by the data. The probability of a data vector under the model is then proportional to the product of the probabilities of its constraint violations. We describe three methods of learning products of constraints using a heavy-tailed probability distribution for the violations.",
      "citation_histogram": [
        [2001, 1],
        [2002, 1],
        [2003, 3],
        [2004, 2],
        [2005, 4],
        [2006, 5],
        [2007, 4],
        [2008, 3],
        [2009, 2],
        [2010, 1],
        [2011, 2],
        [2012, 4],
        [2013, 2],
        [2014, 1],
        [2015, 1],
        [2016, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "TRAFFIC: Recognizing objects using hierarchical reference frame transformations",
      "link": "https://proceedings.neurips.cc/paper/1989/hash/f340f1b1f65b6df5b5e3f94d95b11daf-Abstract.html",
      "year": 1989,
      "cited_by": 38,
      "authors": ["Richard Zemel", "Michael C Mozer", "Geoffrey E Hinton"],
      "description": "We describe a model that can recognize two-dimensional shapes in an unsegmented image, independent of their orientation, position, and scale. The model, called TRAFFIC, efficiently represents the structural relation between an object and each of its component features by encoding the fixed viewpoint-invariant transformation from the feature's reference frame to the object's in the weights of a connectionist network. Using a hierarchy of such transformations, with increasing complexity of features at each successive layer, the network can recognize multiple objects in parallel. An implemen (cid: 173) tation of TRAFFIC is described, along with experimental results demonstrating the network's ability to recognize constellations of stars in a viewpoint-invariant manner.",
      "citation_histogram": [
        [1990, 1],
        [1991, 4],
        [1992, 4],
        [1993, 3],
        [1994, 1],
        [1995, 1],
        [1996, 2],
        [1997, 2],
        [1998, 4],
        [1999, 1],
        [2000, 1],
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 2],
        [2006, 4],
        [2007, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Why the islands move",
      "link": "https://journals.sagepub.com/doi/abs/10.1068/p130629",
      "year": 1984,
      "cited_by": 38,
      "authors": ["Edwin Hutchins", "Geoffrey E Hinton"],
      "description": "Micronesian navigators routinely make voyages across large expanses of open ocean. To do this, a navigator must judge both the direction in which he is sailing and the distance he has travelled. The rising and setting points of the stars (and other cues) provide instantaneous information about direction, but distance can only be judged by integrating velocity-related information over time. Micronesian navigators judge distance in a way that seems odd. When they are out of sight of land, they imagine that the canoe is stationary and that the islands move back past them. For each voyage, they \u2018attend\u2019 to an island off to the side of the course which is out of sight over the horizon. As they sail, they imagine the island moving back along the horizon changing in bearing until it is imagined to be under the bearing it is known to have from the destination island. Then they know they are near their destination. There is good\u00a0\u2026",
      "citation_histogram": [
        [1993, 1],
        [1994, 1],
        [1995, 2],
        [1996, 1],
        [1997, 2],
        [1998, 1],
        [1999, 1],
        [2000, 3],
        [2001, 1],
        [2002, 3],
        [2003, 1],
        [2004, 3],
        [2005, 4],
        [2006, 1],
        [2007, 1],
        [2008, 2],
        [2009, 3],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning Generative Texture Models with extended Fields-of-Experts.",
      "link": "https://www.pure.ed.ac.uk/ws/files/7893923/bmvc09_1_.pdf",
      "year": 2009,
      "cited_by": 36,
      "authors": [
        "Nicolas Heess",
        "Christopher KI Williams",
        "Geoffrey E Hinton"
      ],
      "description": "We evaluate the ability of the popular Field-of-Experts (FoE) to model structure in images. As a test case we focus on modeling synthetic and natural textures. We find that even for modeling single textures, the FoE provides insufficient flexibility to learn good generative models\u2013it does not perform any better than the much simpler Gaussian FoE. We propose an extended version of the FoE (allowing for bimodal potentials) and demonstrate that this novel formulation, when trained with a better approximation of the likelihood gradient, gives rise to a more powerful generative model of specific visual structure that produces significantly better results for the texture task.",
      "citation_histogram": [
        [2010, 3],
        [2011, 4],
        [2012, 2],
        [2013, 2],
        [2014, 5],
        [2015, 5],
        [2016, 7],
        [2017, 1],
        [2018, 2],
        [2019, 1],
        [2020, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Hierarchical non-linear factor analysis and topographic maps",
      "link": "https://proceedings.neurips.cc/paper/1997/hash/daa96d9681a21445772454cbddf0cac1-Abstract.html",
      "year": 1997,
      "cited_by": 36,
      "authors": ["Zoubin Ghahramani", "Geoffrey E Hinton"],
      "description": "We first describe a hierarchical, generative model that can be viewed as a non-linear generalisation of factor analysis and can be implemented in a neural network. The model performs per (cid: 173) ceptual inference in a probabilistically consistent manner by using top-down, bottom-up and lateral connections. These connections can be learned using simple rules that require only locally avail (cid: 173) able information. We then show how to incorporate lateral con (cid: 173) nections into the generative model. The model extracts a sparse, distributed, hierarchical representation of depth from simplified random-dot stereograms and the localised disparity detectors in the first hidden layer form a topographic map. When presented with image patches from natural scenes, the model develops topo (cid: 173) graphically organised local feature detectors.",
      "citation_histogram": [
        [1997, 1],
        [1998, 3],
        [1999, 7],
        [2000, 2],
        [2001, 3],
        [2002, 1],
        [2003, 2],
        [2004, 2],
        [2005, 6],
        [2006, 3],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 2],
        [2011, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Some computational solutions to Bernstein's problems",
      "link": "https://ci.nii.ac.jp/naid/10015563041/",
      "year": 1984,
      "cited_by": 36,
      "authors": ["G Hinton"],
      "description": "CiNii \u8ad6\u6587 - Some computational solutions to Bernstein's problems CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\n\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\n\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \n\u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 [2/17\u66f4\u65b0]2022\u5e74\n4\u67081\u65e5\u304b\u3089\u306eCiNii Articles\u306eCiNii Research\u3078\u306e\u7d71\u5408\u306b\u3064\u3044\u3066 Some computational solutions to \nBernstein's problems HINTON G. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 HINTON G. \u53ce\u9332\u520a\u884c\u7269 Human Motor \nActions-Bernstein Reassessed Human Motor Actions-Bernstein Reassessed, 1984 Elsevier \nScience \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Natural resolution of ill-posedness of inverse \nkinematics for redundant robots : a challenge to Bernstein's degrees-of-freedom problem \nARIMOTO SUGURU , SEKIMOTO MASAHIRO , HASHIGUCHI HIROE , OZAWA RYUTA \u2026",
      "citation_histogram": [
        [1985, 3],
        [1986, 2],
        [1987, 1],
        [1988, 6],
        [1989, 1],
        [1990, 3],
        [1991, 3],
        [1992, 1],
        [1993, 1],
        [1994, 1],
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 1],
        [1999, 2],
        [2000, 1],
        [2001, 2],
        [2002, 1],
        [2003, 1],
        [2004, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Wie neuronale Netze aus Erfahrung lernen",
      "link": "https://scholar.google.com/scholar?cluster=15465063127417587601&hl=en&oi=scholarr",
      "year": 1992,
      "cited_by": 35,
      "authors": ["Geoffrey E Hinton"],
      "description": null,
      "citation_histogram": [
        [1993, 1],
        [1994, 2],
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 3],
        [1999, 4],
        [2000, 2],
        [2001, 2],
        [2002, 2],
        [2003, 3],
        [2004, 2],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 4],
        [2009, 1],
        [2010, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning causally linked markov random fields",
      "link": "http://proceedings.mlr.press/r5/hinton05a/hinton05a.pdf",
      "year": 2005,
      "cited_by": 34,
      "authors": ["Geoffrey Hinton", "Simon Osindero", "Kejie Bao"],
      "description": "We describe a learning procedure for a generative model that contains a hidden Markov Random Field (MRF) which has directed connections to the observable variables. The learning procedure uses a variational approximation for the posterior distribution over the hidden variables. Despite the intractable partition function of the MRF, the weights on the directed connections and the variational approximation itself can be learned by maximizing a lower bound on the log probability of the observed data. The parameters of the MRF are learned by using the mean field version of contrastive divergence [1]. We show that this hybrid model simultaneously learns parts of objects and their inter-relationships from intensity images. We discuss the extension to multiple MRF\u2019s linked into in a chain graph by directed connections.",
      "citation_histogram": [
        [2005, 5],
        [2006, 4],
        [2007, 2],
        [2008, 4],
        [2009, 4],
        [2010, 2],
        [2011, 2],
        [2012, 1],
        [2013, 1],
        [2014, 1],
        [2015, 1],
        [2016, 1],
        [2017, 1],
        [2018, 2],
        [2019, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Evaluating the interface of a document processor: a comparison of expert judgement and user observation",
      "link": "https://scholar.google.com/scholar?cluster=14384273022491776402&hl=en&oi=scholarr",
      "year": 1984,
      "cited_by": 34,
      "authors": [
        "N Hammond",
        "G Hinton",
        "P Barnard",
        "A MacLean",
        "J Long",
        "A Whitefield"
      ],
      "description": null,
      "citation_histogram": [
        [1987, 2],
        [1988, 1],
        [1989, 3],
        [1990, 5],
        [1991, 3],
        [1992, 2],
        [1993, 2],
        [1994, 2],
        [1995, 1],
        [1996, 3],
        [1997, 1],
        [1998, 1],
        [1999, 1],
        [2000, 1],
        [2001, 2],
        [2002, 1],
        [2003, 1],
        [2004, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Using free energies to represent Q-values in a multiagent reinforcement learning task",
      "link": "https://proceedings.neurips.cc/paper/2000/hash/2d1b2a5ff364606ff041650887723470-Abstract.html",
      "year": 2000,
      "cited_by": 33,
      "authors": ["Brian Sallans", "Geoffrey E Hinton"],
      "description": "The problem of reinforcement learning in large factored Markov decision processes is explored. The Q-value of a state-action pair is approximated by the free energy of a product of experts network. Network parameters are learned on-line using a modified SARSA algorithm which minimizes the inconsistency of the Q-values of consecutive state-action pairs. Ac (cid: 173) tions are chosen based on the current value estimates by fixing the current state and sampling actions from the network using Gibbs sampling. The algorithm is tested on a co-operative multi-agent task. The product of experts model is found to perform comparably to table-based Q-Iearning for small instances of the task, and continues to perform well when the problem becomes too large for a table-based representation.",
      "citation_histogram": [
        [2001, 1],
        [2002, 3],
        [2003, 3],
        [2004, 2],
        [2005, 3],
        [2006, 1],
        [2007, 1],
        [2008, 2],
        [2009, 2],
        [2010, 1],
        [2011, 2],
        [2012, 1],
        [2013, 2],
        [2014, 2],
        [2015, 1],
        [2016, 1],
        [2017, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1. chap. Learning Internal Representations by Error Propagation",
      "link": "https://scholar.google.com/scholar?cluster=14039034898215633952&hl=en&oi=scholarr",
      "year": 1986,
      "cited_by": 33,
      "authors": ["DE Rumelhart", "GE Hinton", "RJ Williams"],
      "description": null,
      "citation_histogram": [
        [2013, 1],
        [2014, 4],
        [2015, 1],
        [2016, 2],
        [2017, 5],
        [2018, 5],
        [2019, 3],
        [2020, 2],
        [2021, 7],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Canonical capsules: Self-supervised capsules in canonical pose",
      "link": "https://proceedings.neurips.cc/paper/2021/hash/d1ee59e20ad01cedc15f5118a7626099-Abstract.html",
      "year": 2021,
      "cited_by": 32,
      "authors": [
        "Weiwei Sun",
        "Andrea Tagliasacchi",
        "Boyang Deng",
        "Sara Sabour",
        "Soroosh Yazdani",
        "Geoffrey E Hinton",
        "Kwang Moo Yi"
      ],
      "description": "We propose a self-supervised capsule architecture for 3D point clouds. We compute capsule decompositions of objects through permutation-equivariant attention, and self-supervise the process by training with pairs of randomly rotated objects. Our key idea is to aggregate the attention masks into semantic keypoints, and use these to supervise a decomposition that satisfies the capsule invariance/equivariance properties. This not only enables the training of a semantically consistent decomposition, but also allows us to learn a canonicalization operation that enables object-centric reasoning. To train our neural network we require neither classification labels nor manually-aligned training datasets. Yet, by learning an object-centric representation in a self-supervised manner, our method outperforms the state-of-the-art on 3D point cloud reconstruction, canonicalization, and unsupervised classification.",
      "citation_histogram": [
        [2021, 9],
        [2022, 23]
      ],
      "detail_extracted": true
    },
    {
      "title": "Local physical models for interactive character animation",
      "link": "https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-8659.00593",
      "year": 2002,
      "cited_by": 32,
      "authors": ["Sageev Oore", "Demetri Terzopoulos", "Geoffrey Hinton"],
      "description": "  Our goal is to design and build a tool for the creation of expressive character animation. Virtual puppetry, also known as performance animation, is a technique in which the user interactively controls a character's motion. In this paper we introduce local physical models for performance animation and describe how they can augment an existing kinematic method to achieve very effective animation control. These models approximate specific physically\u2010generated aspects of a character's motion. They automate certain behaviours, while still letting the user override such motion via a PD\u2010controller if he so desires. Furthermore, they can be tuned to ignore certain undesirable effects, such as the risk of having a character fall over, by ignoring corresponding components of the force. Although local physical models are a quite simple approximation to real physical behaviour, we show that they are extremely useful for\u00a0\u2026",
      "citation_histogram": [
        [2002, 1],
        [2003, 4],
        [2004, 2],
        [2005, 3],
        [2006, 3],
        [2007, 4],
        [2008, 1],
        [2009, 5],
        [2010, 1],
        [2011, 4],
        [2012, 1],
        [2013, 1],
        [2014, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "International conference on machine learning",
      "link": "https://scholar.google.com/scholar?cluster=6552180491133406301&hl=en&oi=scholarr",
      "year": 2013,
      "cited_by": 31,
      "authors": [
        "Ilya Sutskever",
        "James Martens",
        "George Dahl",
        "Geoffrey Hinton"
      ],
      "description": null,
      "citation_histogram": [
        [2016, 1],
        [2017, 2],
        [2018, 1],
        [2019, 1],
        [2020, 8],
        [2021, 7],
        [2022, 11]
      ],
      "detail_extracted": true
    },
    {
      "title": "An unsupervised learning procedure that discovers surfaces in random-dot stereograms",
      "link": "http://www.cs.utoronto.ca/~hinton/absps/beckerIJCNN.pdf",
      "year": 1990,
      "cited_by": 31,
      "authors": ["Geoffrey E Hinton", "Suzanna Becker"],
      "description": "A major goal of research on unsupervised learning procedures is to discover an objective function that defines the quality of an internal representation without any externally supplied information about Hi \u00ab? desired outputs of the system, lfsuch a function could be found, it should allow a hierarchy of representations to be organized boUom-up in a time roughly linear in the depth of the network; This would allow much faster learning than supervised procedures which are generally very slow in networks with many layers of hidden units. Following (Gibson, 1950), we propose that a good objective for perceptual learning is to extract higher-order features that are coherent across time or space. This can be done by maximizing the explicit mutuai information between parameters extracted from spatially or temporally adjacent parts of the input.",
      "citation_histogram": [
        [1990, 3],
        [1991, 9],
        [1992, 1],
        [1993, 2],
        [1994, 2],
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 2],
        [1999, 3],
        [2000, 1],
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "GEMINI: gradient estimation through matrix inversion after noise injection",
      "link": "https://proceedings.neurips.cc/paper/1988/hash/a0a080f42e6f13b3a2df133f073095dd-Abstract.html",
      "year": 1988,
      "cited_by": 31,
      "authors": ["Yann Le Cun", "Conrad Galland", "Geoffrey E Hinton"],
      "description": "Learning procedures that measure how random perturbations of unit ac (cid: 173) tivities correlate with changes in reinforcement are inefficient but simple to implement in hardware. Procedures like back-propagation (Rumelhart, Hinton and Williams, 1986) which compute how changes in activities af (cid: 173) fect the output error are much more efficient, but require more complex hardware. GEMINI is a hybrid procedure for multilayer networks, which shares many of the implementation advantages of correlational reinforce (cid: 173) ment procedures but is more efficient. GEMINI injects noise only at the first hidden layer and measures the resultant effect on the output error. A linear network associated with each hidden layer iteratively inverts the matrix which relates the noise to the error change, thereby obtaining the error-derivatives. No back-propagation is involved, thus allowing un (cid: 173) known non-linearities in the system. Two simulations demonstrate the effectiveness of GEMINI.",
      "citation_histogram": [
        [1990, 1],
        [1991, 1],
        [1992, 2],
        [1993, 1],
        [1994, 1],
        [1995, 3],
        [1996, 3],
        [1997, 2],
        [1998, 2],
        [1999, 1],
        [2000, 2],
        [2001, 1],
        [2002, 2],
        [2003, 3],
        [2004, 3],
        [2005, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "On the importance of momentum and initialization in deep learning",
      "link": "https://scholar.google.com/scholar?cluster=8935610677147751568&hl=en&oi=scholarr",
      "year": 2013,
      "cited_by": 30,
      "authors": [
        "Ilya Sutskever",
        "James Martens",
        "George Dahl",
        "Geoffery Hinton"
      ],
      "description": null,
      "citation_histogram": [
        [2014, 3],
        [2015, 7],
        [2016, 4],
        [2017, 6],
        [2018, 3],
        [2019, 2],
        [2020, 3],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Divide the gradient by a running average of its recent magnitude. COURSERA Neural Netw",
      "link": "https://scholar.google.com/scholar?cluster=399157043475128649&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 30,
      "authors": ["Tijmen Tieleman", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2016, 1],
        [2017, 3],
        [2018, 6],
        [2019, 9],
        [2020, 10]
      ],
      "detail_extracted": true
    },
    {
      "title": "A mode-hopping MCMC sampler",
      "link": "http://www.cs.toronto.edu/pub/reports/csrg/478/TR_CSRG_478.pdf",
      "year": 2003,
      "cited_by": 30,
      "authors": ["Cristian Sminchisescu", "Max Welling", "Geoffrey Hinton"],
      "description": "One of the main shortcomings of Markov chain Monte Carlo samplers is their inability to mix between modes of the target distribution. In this paper we show that advance knowledge of the location of these modes can be incorporated into the MCMC sampler by introducing mode-hopping moves that satisfy detailed balance. The proposed sampling algorithm explores local mode structure through local MCMC moves (eg. diffusion or Hybrid Monte Carlo) but in addition also represents the relative strengths of the different modes correctly using a set of global moves. This \u201cmode-hopping\u201d MCMC sampler can be viewed as a generalization of the darting method [1].",
      "citation_histogram": [
        [2004, 3],
        [2005, 1],
        [2006, 2],
        [2007, 2],
        [2008, 1],
        [2009, 1],
        [2010, 2],
        [2011, 2],
        [2012, 1],
        [2013, 1],
        [2014, 3],
        [2015, 1],
        [2016, 5],
        [2017, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. 2012",
      "link": "https://scholar.google.com/scholar?cluster=4860687452005618140&hl=en&oi=scholarr",
      "year": 2021,
      "cited_by": 29,
      "authors": ["Tijmen Tieleman", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2016, 2],
        [2017, 2],
        [2018, 8],
        [2019, 4],
        [2020, 2],
        [2021, 7],
        [2022, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parameter estimation for linear dynamical systems (Technical Report CRG-TR-96\u20132)",
      "link": "https://scholar.google.com/scholar?cluster=3427959372090429593&hl=en&oi=scholarr",
      "year": 1996,
      "cited_by": 28,
      "authors": ["Z Ghahramani", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [2004, 1],
        [2005, 1],
        [2006, 2],
        [2007, 1],
        [2008, 6],
        [2009, 1],
        [2010, 2],
        [2011, 1],
        [2012, 2],
        [2013, 3],
        [2014, 4],
        [2015, 2],
        [2016, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Improving a statistical language model through non-linear prediction",
      "link": "https://www.sciencedirect.com/science/article/pii/S0925231209000083",
      "year": 2009,
      "cited_by": 27,
      "authors": ["Andriy Mnih", "Zhang Yuecheng", "Geoffrey Hinton"],
      "description": "We show how to improve a state-of-the-art neural network language model that converts the previous \u201ccontext\u201d words into feature vectors and combines these feature vectors linearly to predict the feature vector of the next word. Significant improvements in predictive accuracy are achieved by using a non-linear subnetwork to modulate the effects of the context words or to produce a non-linear correction term when predicting the feature vector. A log-bilinear language model that incorporates both of these improvements achieves a 26% reduction in perplexity over the best n-gram model on a fairly large dataset.",
      "citation_histogram": [
        [2010, 1],
        [2011, 2],
        [2012, 1],
        [2013, 2],
        [2014, 1],
        [2015, 3],
        [2016, 2],
        [2017, 5],
        [2018, 1],
        [2019, 4],
        [2020, 1],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning nonlinear constraints with contrastive backpropagation",
      "link": "https://ieeexplore.ieee.org/abstract/document/1556042/",
      "year": 2005,
      "cited_by": 27,
      "authors": ["Andriy Mnih", "Geoffrey Hinton"],
      "description": "Certain datasets can be efficiently modelled in terms of constraints that are usually satisfied but sometimes are strongly violated. We propose using energy-based density models (EBMs) implementing products of frequently approximately satisfied nonlinear constraints for modelling such datasets. We demonstrate the feasibility of this approach by training an EBM using contrastive backpropagation on a dataset of idealized trajectories of two balls bouncing in a box and showing that the model learns an accurate and efficient representation of the dataset, taking advantage of the approximate independence between subsets of variables.",
      "citation_histogram": [
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 2],
        [2014, 1],
        [2015, 3],
        [2016, 9],
        [2017, 4],
        [2018, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "The ups and downs of Hebb synapses.",
      "link": "https://psycnet.apa.org/doiLanding?doi=10.1037/h0085812",
      "year": 2003,
      "cited_by": 27,
      "authors": ["Geoffrey Hinton"],
      "description": "Les mod\u00e9lisateurs ont trouv\u00e9 de nombreuses r\u00e8gles d'apprentissage diff\u00e9rentes pour les r\u00e9seaux neuraux. Lorsqu'un professeur sp\u00e9cifie le r\u00e9sultat correct, les r\u00e8gles ax\u00e9es sur l'erreur fonctionnent mieux que les r\u00e8gles de Hebb pures o\u00f9 les changements dans la force de la synapse d\u00e9pendent de la corr\u00e9lation entre les activit\u00e9s pr\u00e9 et post synaptiques. Cependant, pour l'apprentissage sans supervision, les r\u00e8gles de Hebb peuvent \u00eatre tr\u00e8s efficaces si elles sont combin\u00e9es \u00e0 une normalisation convenable ou \u00e0 des termes de \u00abd\u00e9sapprentissage\u00bb pour emp\u00eacher que les synapses grandissent sans limite. Les r\u00e8gles de Hebb qui empruntent les taux de changement d'activit\u00e9 plut\u00f4t que l'activit\u00e9 en soi sont utiles pour d\u00e9couvrir les invariants perceptifs et peuvent aussi fournir une fa\u00e7on de mettre en \u0153uvre l'apprentissage ax\u00e9 sur l'erreur.",
      "citation_histogram": [
        [2003, 1],
        [2004, 1],
        [2005, 2],
        [2006, 2],
        [2007, 2],
        [2008, 1],
        [2009, 5],
        [2010, 2],
        [2011, 4],
        [2012, 4],
        [2013, 1],
        [2014, 1],
        [2015, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Efficient stochastic source coding and an application to a Bayesian network source model",
      "link": "https://academic.oup.com/comjnl/article-abstract/40/2_and_3/157/360818",
      "year": 1997,
      "cited_by": 27,
      "authors": ["Brendan J.  Frey", "Geoffrey E.  Hinton"],
      "description": " In this paper, we introduce a new algorithm called \u2018bits-back coding\u2019 that makes stochastic source codes efficient. For a given one-to-many source code, we show that this algorithm can actually be more efficient than the algorithm that always picks the shortest codeword. Optimal efficiency is achieved when codewords are chosen according to the Boltzmann distribution based on the codeword lengths. It turns out that a commonly used technique for determining parameters\u2013maximum-likelihood estimation\u2013actually minimizes the bits-back coding cost when codewords are chosen according to the Boltzmann distribution. A tractable approximation to maximum-likelihood estimation\u2013the generalized expectation-maximization algorithm\u2013minimizes the bits-back coding cost. After presenting a binary Bayesian network model that assigns exponentially many codewords to each symbol, we show how a tractable\u00a0\u2026",
      "citation_histogram": [
        [2002, 1],
        [2003, 3],
        [2004, 3],
        [2005, 2],
        [2006, 1],
        [2007, 1],
        [2008, 2],
        [2009, 2],
        [2010, 1],
        [2011, 2],
        [2012, 6],
        [2013, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Using an autoencoder with deformable templates to discover features for automated speech recognition.",
      "link": "https://www.isca-speech.org/archive_v0/archive_papers/interspeech_2013/i13_1737.pdf",
      "year": 2013,
      "cited_by": 26,
      "authors": ["Navdeep Jaitly", "Geoffrey E Hinton"],
      "description": "In this paper we show how we can discover non-linear features of frames of spectrograms using a novel autoencoder. The autoencoder uses a neural network encoder that predicts how a set of prototypes called templates need to be transformed to reconstruct the data, and a decoder that is a function that performs this operation of transforming prototypes and reconstructing the input. We demonstrate this method on spectrograms from the TIMIT database. The features are used in a Deep Neural Network-Hidden Markov Model (DNN-HMM) hybrid system for automatic speech recognition. On the TIMIT monophone recognition task we were able to achieve gains of 0.5% over Mel log spectra, by augmenting traditional the spectra with the predicted transformation parameters. Further, using the recently discovered \u2018dropout\u2019training, we were able to achieve a phone error rate (PER) of 17.9% on the dev set and 19.5% on\u00a0\u2026",
      "citation_histogram": [
        [2014, 4],
        [2015, 3],
        [2016, 6],
        [2017, 4],
        [2018, 4],
        [2019, 1],
        [2020, 1],
        [2021, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Coaching variables for regression and classification",
      "link": "https://link.springer.com/article/10.1023/A:1008815025242",
      "year": 1998,
      "cited_by": 26,
      "authors": ["Robert Tibshirani", "Geoffrey Hinton"],
      "description": " In a regression or classification setting where we wish to predict Y from x1,x2,..., xp, we suppose that an additional set of \u2018coaching\u2019 variables z1,z2,..., zm are available in our training sample. These might be variables that are difficult to measure, and they will not be available when we predict Y from x1,x2,..., xp in the future. We consider two methods of making use of the coaching variables in order to improve the prediction of Y from x1,x2,..., xp. The relative merits of these approaches are discussed and compared in a number of examples.",
      "citation_histogram": [
        [1998, 2],
        [1999, 1],
        [2000, 4],
        [2001, 1],
        [2002, 2],
        [2003, 3],
        [2004, 2],
        [2005, 3],
        [2006, 2],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep learning for speech recognition and related applications",
      "link": "https://scholar.google.com/scholar?cluster=1160846204246323630&hl=en&oi=scholarr",
      "year": 2009,
      "cited_by": 25,
      "authors": ["Li Deng", "Dong Yu", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2013, 3],
        [2014, 4],
        [2015, 4],
        [2016, 7],
        [2017, 1],
        [2018, 1],
        [2019, 1],
        [2020, 1],
        [2021, 1],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Using matrices to model symbolic relationship",
      "link": "https://proceedings.neurips.cc/paper/2008/hash/cfa0860e83a4c3a763a7e62d825349f7-Abstract.html",
      "year": 2008,
      "cited_by": 25,
      "authors": ["Ilya Sutskever", "Geoffrey E Hinton"],
      "description": "We describe a way of learning matrix representations of objects and relationships. The goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships, which is the main novelty of the method. We demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships. We show that the same system can learn first-order propositions such as $(2, 5)\\member+\\! 3$ or $(Christopher, Penelope)\\member has\\_wife $, and higher-order propositions such as $(3,+\\! 3)\\member plus $ and $(+\\! 3,-\\! 3)\\member inverse $ or $(has\\_husband, has\\_wife)\\in higher\\_oppsex $. We further demonstrate that the system understands how higher-order propositions are related to first-order ones by showing that it can correctly answer questions about first-order propositions involving the relations $+\\! 3$ or $ has\\_wife $ even though it has not been trained on any first-order examples involving these relations.",
      "citation_histogram": [
        [2011, 2],
        [2012, 1],
        [2013, 2],
        [2014, 2],
        [2015, 2],
        [2016, 2],
        [2017, 2],
        [2018, 3],
        [2019, 6],
        [2020, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "A new view of ICA",
      "link": "https://www.stats.ox.ac.uk/~teh/research/unsup/ica2001.pdf",
      "year": 2001,
      "cited_by": 25,
      "authors": [
        "Geoffrey E Hinton",
        "Max Welling",
        "Yee Whye Teh",
        "Simon Osindero"
      ],
      "description": "We present a new way of interpreting ICA as a probability density model and a new way of fitting this model to data. The advantage of our approach is that it suggests simple, novel extensions to overcomplete, undercomplete and multilayer non-linear versions of ICA.",
      "citation_histogram": [
        [2002, 4],
        [2003, 7],
        [2004, 4],
        [2005, 1],
        [2006, 2],
        [2007, 3],
        [2008, 1],
        [2009, 1],
        [2010, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Using relaxation to find a puppet",
      "link": "https://dl.acm.org/doi/abs/10.5555/3015508.3015523",
      "year": 1976,
      "cited_by": 25,
      "authors": ["Geoffrey E.  Hinton"],
      "description": "The problem of finding a puppet in a configuration of overlapping, transparent rectangles is used to show how a relaxation algorithm can extract the globally best figure from a network of conflicting local interpretations.",
      "citation_histogram": [
        [1983, 1],
        [1984, 1],
        [1985, 2],
        [1986, 1],
        [1987, 2],
        [1988, 1],
        [1989, 1],
        [1990, 1],
        [1991, 2],
        [1992, 1],
        [1993, 2],
        [1994, 1],
        [1995, 1],
        [1996, 1],
        [1997, 2],
        [1998, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Unsupervised part representation by flow capsules",
      "link": "https://proceedings.mlr.press/v139/sabour21a.html",
      "year": 2021,
      "cited_by": 24,
      "authors": [
        "Sara Sabour",
        "Andrea Tagliasacchi",
        "Soroosh Yazdani",
        "Geoffrey Hinton",
        "David J Fleet"
      ],
      "description": "Capsule networks aim to parse images into a hierarchy of objects, parts and relations. While promising, they remain limited by an inability to learn effective low level part descriptions. To address this issue we propose a way to learn primary capsule encoders that detect atomic parts from a single image. During training we exploit motion as a powerful perceptual cue for part definition, with an expressive decoder for part generation within a layered image model with occlusion. Experiments demonstrate robust part discovery in the presence of multiple objects, cluttered backgrounds, and occlusion. The learned part decoder is shown to infer the underlying shape masks, effectively filling in occluded regions of the detected shapes. We evaluate FlowCapsules on unsupervised part segmentation and unsupervised image classification.",
      "citation_histogram": [
        [2021, 10],
        [2022, 14]
      ],
      "detail_extracted": true
    },
    {
      "title": "System and method for labelling aerial images",
      "link": "https://patents.google.com/patent/US9704068B2/en",
      "year": 2017,
      "cited_by": 24,
      "authors": ["Volodymyr Mnih", "Geoffrey E Hinton"],
      "description": "A system and method for labelling aerial images. A neural network generates predicted map data. The parameters of the neural network are trained by optimizing an objective function which compensates for noise in the map images. The function compensates both omission noise and registration noise.",
      "citation_histogram": [
        [2016, 1],
        [2017, 3],
        [2018, 5],
        [2019, 4],
        [2020, 4],
        [2021, 4],
        [2022, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Autoregressive product of multi-frame predictions can improve the accuracy of hybrid models",
      "link": "https://research.google/pubs/pub42947/",
      "year": 2014,
      "cited_by": 24,
      "authors": ["Navdeep Jaitly", "Vincent Vanhoucke", "Geoffrey Hinton"],
      "description": "We describe a simple but effective way of using multi-frame targets to improve the accuracy of Artificial Neural Network-Hidden Markov Model (ANN-HMM) hybrid systems. In this approach a Deep Neural Network (DNN) is trained to predict the forced-alignment state of multiple frames using a separate softmax unit for each of the frames. This is in contrast to the usual method of training a DNN to predict only the state of the central frame. By itself this is not sufficient to improve accuracy of the system significantly. However, if we average the predic-tions for each frame-from the different contexts it is associated with-we achieve state of the art results on TIMIT using a fully connected Deep Neural Network without convolutional archi-tectures or dropout training. On a 14 hour subset of Wall Street Journal (WSJ) using a context dependent DNN-HMM system it leads to a relative improvement of 6.4% on the dev set (test-dev93) and 9.3% on test set (test-eval92).",
      "citation_histogram": [
        [2014, 1],
        [2015, 8],
        [2016, 5],
        [2017, 6],
        [2018, 1],
        [2019, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning Multiple Layers of Features from Tiny Images. University of Toronto; Toronto, ON",
      "link": "https://scholar.google.com/scholar?cluster=18071756958395986545&hl=en&oi=scholarr",
      "year": 2009,
      "cited_by": 24,
      "authors": ["A Krizhevsky", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2015, 1],
        [2016, 2],
        [2017, 3],
        [2018, 7],
        [2019, 5],
        [2020, 6]
      ],
      "detail_extracted": true
    },
    {
      "title": "A simple algorithm that discovers efficient perceptual codes",
      "link": "https://books.google.com/books?hl=en&lr=&id=93MEoIbIyLUC&oi=fnd&pg=PA296&dq=info:OFrocC3ajZkJ:scholar.google.com&ots=VjacCcombh&sig=c16jt_O7bGNTcZYVtpBDMu5qa2c",
      "year": 1997,
      "cited_by": 24,
      "authors": ["Brendan J Frey", "Peter Dayan", "Geoffrey E Hinton"],
      "description": "We describe the\" wake-sleep\u201d algorithm that allows a multilayer, unsupervised, neural network to build a hierarchy of representations of sensory input. The network has bottom-up \u201crecognition\" connections that are used to convert sensory input into underlying representations. Unlike most artificial neural networks, it also has top-down \u201cgenerative\u201d connections that can be used to reconstruct the sensory input from the representations. In the \u201cwake\u201d phase of the learning algorithm, the network is driven by the bottomup recognition connections and the top-down generative connections are trained to be better at reconstructing the sensory input from the represen-tation chosen by the recognition process. In the \u201csleep\" phase, the network is driven top-down by the generative connections to produce a fantasized representation and a fantasized sensory input. The recognition connections are then trained to be better at recovering the fantasized representation from the fantasized sensory input. In both phases, the synaptic learning rule is simple and local. The combined effect of the two phases is to create representations of the sensory input that are efficient in the following sense: On average, it takes more bits to describe each sensory input vector directly than to first describe the representation of the sensory input chosen by the recognition process and then describe the difference between the sensory input and its reconstruction from the chosen representation.",
      "citation_histogram": [
        [1999, 3],
        [2000, 1],
        [2001, 3],
        [2002, 1],
        [2003, 3],
        [2004, 1],
        [2005, 2],
        [2006, 2],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 2],
        [2011, 2],
        [2012, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning sequential structure in simple recurrent networks",
      "link": "https://scholar.google.com/scholar?cluster=1922308031387661294&hl=en&oi=scholarr",
      "year": 1986,
      "cited_by": 24,
      "authors": ["DE Rumelhart", "G Hinton", "R Williams"],
      "description": null,
      "citation_histogram": [
        [1991, 1],
        [1992, 1],
        [1993, 1],
        [1994, 3],
        [1995, 2],
        [1996, 1],
        [1997, 3],
        [1998, 1],
        [1999, 1],
        [2000, 2],
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Imagenet classification with deep convnets",
      "link": "https://scholar.google.com/scholar?cluster=8315912059848841304&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 23,
      "authors": ["A Krizhevsky", "I Sutskever", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2016, 3],
        [2017, 6],
        [2018, 2],
        [2019, 2],
        [2020, 5],
        [2021, 3],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Artificial Intelligence and Statistics",
      "link": "https://scholar.google.com/scholar?cluster=1541987260803329128&hl=en&oi=scholarr",
      "year": 2009,
      "cited_by": 23,
      "authors": ["R Salakhutdinov", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2015, 2],
        [2016, 1],
        [2017, 1],
        [2018, 5],
        [2019, 2],
        [2020, 6],
        [2021, 6]
      ],
      "detail_extracted": true
    },
    {
      "title": "Delve data sets",
      "link": "https://scholar.google.com/scholar?cluster=13168443833130392791&hl=en&oi=scholarr",
      "year": 1996,
      "cited_by": 23,
      "authors": [
        "CE Rasmussen",
        "RM Neal",
        "G Hinton",
        "D Camp",
        "M Revow",
        "Z Ghahramani",
        "R Kustra",
        "R Tibshirani"
      ],
      "description": null,
      "citation_histogram": [
        [2011, 1],
        [2012, 2],
        [2013, 2],
        [2014, 2],
        [2015, 3],
        [2016, 4],
        [2017, 5],
        [2018, 2],
        [2019, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Machine learning for neuroscience",
      "link": "https://neuralsystemsandcircuits.biomedcentral.com/articles/10.1186/2042-1001-1-12",
      "year": 2011,
      "cited_by": 22,
      "authors": ["Geoffrey E Hinton"],
      "description": "Machine learning is a type of statistics that places particular emphasis on the use of advanced computational algorithms. As computers become more powerful, and modern experimental methods in areas such as imaging generate vast bodies of data, machine learning is becoming ever more important for extracting reliable and meaningful relationships and for making accurate predictions. Key strands of modern machine learning grew out of attempts to understand how large numbers of interconnected, more or less neuron-like elements could learn to achieve behaviourally meaningful computations and to extract useful features from images or sound waves.By the 1990s, key approaches had converged on an elegant framework called \u2018graphical models\u2019, explained in Koller and Friedman, in which the nodes of a graph represent variables such as edges and corners in an image, or phonemes and words in speech. The probabilistic relationships between nodes are represented by conditional probability tables or simple functions whose parameters are learned from the data. There are three main problems in fitting graphical models to data: inference, parameter learning and structure learning. The inference problem is how to infer the probable values of unobserved variables when the values of a subset of the variables have been observed, and is a problem that perceptual systems need to solve if they are to infer the hidden causes of their sensory input. The parameter-learning problem is how to adjust the parameters governing the way in which one variable influences another, so that the graphical model is a better fit to some observed data. In the\u00a0\u2026",
      "citation_histogram": [
        [2012, 1],
        [2013, 1],
        [2014, 1],
        [2015, 1],
        [2016, 2],
        [2017, 1],
        [2018, 6],
        [2019, 7],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning hierarchical structures with linear relational embedding",
      "link": null,
      "year": 2002,
      "cited_by": 22,
      "authors": ["Alberto Paccanaro Geoffrey E Hinton"],
      "description": null,
      "citation_histogram": [
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 3],
        [2009, 1],
        [2010, 2],
        [2011, 3],
        [2012, 1],
        [2013, 2],
        [2014, 3],
        [2015, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Instantiating deformable models with a neural net",
      "link": "https://www.sciencedirect.com/science/article/pii/S1077314297905403",
      "year": 1997,
      "cited_by": 22,
      "authors": [
        "Christopher KI Williams",
        "Michael Revow",
        "Geoffrey E Hinton"
      ],
      "description": "Deformable models are an attractive approach to recognizing objects which have considerable within-class variability such as handwritten characters. However, there are severe search problems associated with fitting the models to data which could be reduced if a better starting point for the search were available. We show that by training a neural network to predict how a deformable model should be instantiated from an input image, such improved starting points can be obtained. This method has been implemented for a system that recognizes handwritten digits using deformable models, and the results show that the search time can be significantly reduced without compromising recognition performance.",
      "citation_histogram": [
        [1998, 2],
        [1999, 1],
        [2000, 1],
        [2001, 2],
        [2002, 3],
        [2003, 1],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 1],
        [2014, 1],
        [2015, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Using pairs of data-points to define splits for decision trees",
      "link": "https://proceedings.neurips.cc/paper/1995/hash/a113c1ecd3cace2237256f4c712f61b5-Abstract.html",
      "year": 1995,
      "cited_by": 22,
      "authors": ["Geoffrey E Hinton", "Michael Revow"],
      "description": "Conventional binary classification trees such as CART either split the data using axis-aligned hyperplanes or they perform a compu (cid: 173) tationally expensive search in the continuous space of hyperplanes with unrestricted orientations. We show that the limitations of the former can be overcome without resorting to the latter. For every pair of training data-points, there is one hyperplane that is orthog (cid: 173) onal to the line joining the data-points and bisects this line. Such hyperplanes are plausible candidates for splits. In a comparison on a suite of 12 datasets we found that this method of generating candidate splits outperformed the standard methods, particularly when the training sets were small.",
      "citation_histogram": [
        [1998, 3],
        [1999, 2],
        [2000, 1],
        [2001, 2],
        [2002, 3],
        [2003, 4],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 2],
        [2008, 1],
        [2009, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deterministic Boltzmann learning in networks with asymmetric connectivity",
      "link": "https://www.sciencedirect.com/science/article/pii/B9781483214481500068",
      "year": 1991,
      "cited_by": 22,
      "authors": ["Conrad C Galland", "Georey E Hinton"],
      "description": "The simplicity and locality of the \u201ccontrastive Hebb synapse\u201d (CHS) used in Boltzmann machine learning makes it an attractive model for real biological synapses. The slow learning exhibited by the stochastic Boltzmann machine can be greatly improved by using a mean field approximation and it has been shown (Hinton, 1989) that the CHS also performs steepest descent in these deterministic mean field networks. A major weakness of the learning procedure, from a biological perspective, is that the derivation assumes detailed symmetry of the connectivity. Using networks with purely asymmetric connectivity, we show that the CHS still works in practice provided the connectivity is grossly symmetrical so that if unit i sends a connection to unit j, there are numerous indirect feedback paths from j to i. So long as the network settles to a stable state, we show that the CHS approximates steepest descent and that the\u00a0\u2026",
      "citation_histogram": [
        [1990, 1],
        [1991, 2],
        [1992, 2],
        [1993, 6],
        [1994, 2],
        [1995, 2],
        [1996, 1],
        [1997, 1],
        [1998, 1],
        [1999, 1],
        [2000, 1],
        [2001, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parallel Distrilmted Processing",
      "link": "https://scholar.google.com/scholar?cluster=8340514146690482117&hl=en&oi=scholarr",
      "year": 1986,
      "cited_by": 22,
      "authors": ["G Hinton", "T Sejnowski", "DE Rumelhart", "JL McClelland"],
      "description": null,
      "citation_histogram": [[1995, 22]],
      "detail_extracted": true
    },
    {
      "title": "A better way to learn features: technical perspective",
      "link": "https://dl.acm.org/doi/pdf/10.1145/2001269.2001294",
      "year": 2011,
      "cited_by": 21,
      "authors": ["Geoffrey E Hinton"],
      "description": "By Geoffrey E. Hinton global context to select appropriately between local features. The learning algorithm used by the authors is designed to produce a composite generative model called a \u201cdeep belief net,\u201d 3 but they perform top-down inference as if it were a different generative model called a\u201d deep Boltzmann machine.\u201d They achieve quite good results at image completion, and even better results might be obtained if they fine-tuned their generative model as a deep Boltzmann machine using a recent algorithm developed by Salakhutdinov. 7 Machine learning still has some way to go before it can efficiently create the complicated features like SIFT6 used in many leading systems for computer vision. However, this paper should seriously worry those computer vision researchers who still believe that handengineered features have a long-term future. Further improvements from unsupervised learning also seem\u00a0\u2026",
      "citation_histogram": [
        [2012, 1],
        [2013, 2],
        [2014, 4],
        [2015, 3],
        [2016, 3],
        [2017, 3],
        [2018, 1],
        [2019, 2],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Modeling pigeon behavior using a Conditional Restricted Boltzmann Machine.",
      "link": "https://www.cs.utoronto.ca/~hinton/absps/esann_pigeon.pdf",
      "year": 2009,
      "cited_by": 21,
      "authors": [
        "Matthew D Zeiler",
        "Graham W Taylor",
        "Nikolaus F Troje",
        "Geoffrey E Hinton"
      ],
      "description": "In an effort to better understand the complex courtship behaviour of pigeons, we have built a model learned from motion capture data. We employ a Conditional Restricted Boltzmann Machine (CRBM) with binary latent features and real-valued visible units. The units are conditioned on information from previous time steps to capture dynamics. We validate a trained model by quantifying the characteristic \u201cheadbobbing\u201d present in pigeons. We also show how to predict missing data by marginalizing out the hidden variables and minimizing free energy.",
      "citation_histogram": [
        [2010, 1],
        [2011, 2],
        [2012, 1],
        [2013, 1],
        [2014, 2],
        [2015, 5],
        [2016, 1],
        [2017, 1],
        [2018, 3],
        [2019, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "presented in part at the Proceedings of the 30th International Conference on International Conference on Machine Learning",
      "link": "https://scholar.google.com/scholar?cluster=14114009703443622099&hl=en&oi=scholarr",
      "year": 2013,
      "cited_by": 20,
      "authors": ["I Sutskever", "J Martens", "G Dahl", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2018, 3],
        [2019, 3],
        [2020, 4],
        [2021, 4],
        [2022, 6]
      ],
      "detail_extracted": true
    },
    {
      "title": "Wormholes improve contrastive divergence",
      "link": "https://proceedings.neurips.cc/paper/2003/hash/03cf87174debaccd689c90c34577b82f-Abstract.html",
      "year": 2003,
      "cited_by": 20,
      "authors": ["Max Welling", "Andriy Mnih", "Geoffrey E Hinton"],
      "description": "In models that define probabilities via energies, maximum likelihood learning typically involves using Markov Chain Monte Carlo to sample from the model\u2019s distribution. If the Markov chain is started at the data distribution, learning often works well even if the chain is only run for a few time steps [3]. But if the data distribution contains modes separated by regions of very low density, brief MCMC will not ensure that different modes have the correct relative energies because it cannot move particles from one mode to another. We show how to improve brief MCMC by allowing long-range moves that are suggested by the data distribution. If the model is approximately correct, these long-range moves have a reasonable acceptance rate.",
      "citation_histogram": [
        [2003, 1],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 4],
        [2009, 2],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 1],
        [2014, 1],
        [2015, 1],
        [2016, 2],
        [2017, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Imagery without arrays",
      "link": "https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/imagery-without-arrays/C07F8ED496E37E6A1674E4CE670C82DF",
      "year": 1979,
      "cited_by": 20,
      "authors": ["Geoffrey Hinton"],
      "description": "//static.cambridge.org/content/id/urn%3Acambridge.org%3Aid%3Aarticle%3AS0140525X00064360/resource/name/firstPage-S0140525X00064360a.jpg",
      "citation_histogram": [
        [1981, 2],
        [1982, 1],
        [1983, 1],
        [1984, 1],
        [1985, 3],
        [1986, 2],
        [1987, 1],
        [1988, 1],
        [1989, 2],
        [1990, 1],
        [1991, 1],
        [1992, 1],
        [1993, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning multiple layers of features from tiny images",
      "link": "https://scholar.google.com/scholar?cluster=16465799714489188772&hl=en&oi=scholarr",
      "year": 2009,
      "cited_by": 19,
      "authors": ["G Hinton", "A Krizhevsky"],
      "description": null,
      "citation_histogram": [
        [2016, 1],
        [2017, 2],
        [2018, 3],
        [2019, 2],
        [2020, 5],
        [2021, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Free energy coding",
      "link": "https://ieeexplore.ieee.org/abstract/document/488312/",
      "year": 1996,
      "cited_by": 19,
      "authors": ["Brendan J Frey", "Geoffrey E Hinton"],
      "description": "We introduce a new approach to the problem of optimal compression when a source code produces multiple codewords for a given symbol. It may seem that the most sensible codeword to use in this case is the shortest one. However, in the proposed free energy approach, random codeword selection yields an effective codeword length that can be less than the shortest codeword length. If the random choices are Boltzmann distributed, the effective length is optimal for the given source code. The expectation-maximization parameter estimation algorithms minimize this effective codeword length. We illustrate the performance of free energy coding on a simple problem where a compression factor of two is gained by using the new method.",
      "citation_histogram": [
        [1996, 2],
        [1997, 1],
        [1998, 2],
        [1999, 1],
        [2000, 1],
        [2001, 1],
        [2002, 2],
        [2003, 2],
        [2004, 6],
        [2005, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Subclass distillation",
      "link": "https://arxiv.org/abs/2002.03936",
      "year": 2020,
      "cited_by": 18,
      "authors": ["Rafael M\u00fcller", "Simon Kornblith", "Geoffrey Hinton"],
      "description": "After a large \"teacher\" neural network has been trained on labeled data, the probabilities that the teacher assigns to incorrect classes reveal a lot of information about the way in which the teacher generalizes. By training a small \"student\" model to match these probabilities, it is possible to transfer most of the generalization ability of the teacher to the student, often producing a much better small model than directly training the student on the training data. The transfer works best when there are many possible classes because more is then revealed about the function learned by the teacher, but in cases where there are only a few possible classes we show that we can improve the transfer by forcing the teacher to divide each class into many subclasses that it invents during the supervised training. The student is then trained to match the subclass probabilities. For datasets where there are known, natural subclasses we demonstrate that the teacher learns similar subclasses and these improve distillation. For clickthrough datasets where the subclasses are unknown we demonstrate that subclass distillation allows the student to learn faster and better.",
      "citation_histogram": [
        [2020, 4],
        [2021, 7],
        [2022, 7]
      ],
      "detail_extracted": true
    },
    {
      "title": "Guest editorial: Deep learning",
      "link": "https://link.springer.com/article/10.1007/s11263-015-0813-1",
      "year": 2015,
      "cited_by": 18,
      "authors": ["Marc\u2019Aurelio Ranzato", "Geoffrey Hinton", "Yann LeCun"],
      "description": "Deep Learning methods aim at learning feature hierarchies. Applications of deep learning to vision tasks date back to convolutional networks in the early 1990s. These methods have been the subject of a recent surge of interest for two main reasons: when labeled data is scarce, unsupervised learning algorithms can learn useful feature hierarchies. When labeled data is abundant, supervised methods can be used to train very large networks on very large datasets through the use of high-performance computers. Such large networks have been shown to outperform previous state-of-theart methods on several perceptual tasks, including categorylevel object recognition, object detection and semantic segmentation.In \u201cStacked Predictive Sparse Decomposition for Classification of Histology Sections\u201d(doi: 10.1007/s11263-014-0790-9) the authors propose the use of an unsupervised feature learning algorithm for the\u00a0\u2026",
      "citation_histogram": [
        [2016, 6],
        [2017, 2],
        [2018, 2],
        [2019, 1],
        [2020, 1],
        [2021, 5],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "A new way to learn acoustic events",
      "link": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.299.125&rep=rep1&type=pdf",
      "year": 2011,
      "cited_by": 18,
      "authors": ["Navdeep Jaitly", "Geoffrey E Hinton"],
      "description": "Most speech recognition systems still use Mel Frequency Cepstral Coefficients (MFCC\u2019s) or Perceptual Linear Prediction Coefficients because these preserve a lot of the information required for recognition while being much more compact than a high-resolution spectrogram. As computers get faster and methods of modeling high-dimensional data improve, however, high-resolution spectrograms or other very high-dimensional representations of the sound wave become more attractive. They have already surpassed MFCC\u2019s for some tasks [1]. Psychologists have argued that high-quality recognition would be facilitated by finding acoustic events or landmarks that have well-defined onset times, amplitudes and rates in addition to being present or absent. We introduce a new way of learning such acoustic events by using a new type of autoencoder that is given both a spectrogram and a desired global transformation and learns to output the transformed spectrogram. By specifying the global transformation in the appropriate way, we can force the autoencoder to extract accoustic events that, in addition to a probability of being present, have explicit onset times, amplitudes and rates. This makes it much easier to compute relationships between acoustic events.",
      "citation_histogram": [
        [2012, 1],
        [2013, 1],
        [2014, 5],
        [2015, 3],
        [2016, 1],
        [2017, 2],
        [2018, 1],
        [2019, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Improving dimensionality reduction with spectral gradient descent",
      "link": "https://www.sciencedirect.com/science/article/pii/S0893608005001310",
      "year": 2005,
      "cited_by": 18,
      "authors": ["Roland Memisevic", "Geoffrey Hinton"],
      "description": "We introduce spectral gradient descent, a way of improving iterative dimensionality reduction techniques.1 The method uses information contained in the leading eigenvalues of a data affinity matrix to modify the steps taken during a gradient-based optimization procedure. We show that the approach is able to speed up the optimization and to help dimensionality reduction methods find better local minima of their objective functions. We also provide an interpretation of our approach in terms of the power method for finding the leading eigenvalues of a symmetric matrix and verify the usefulness of the approach in some simple experiments.",
      "citation_histogram": [
        [2006, 1],
        [2007, 1],
        [2008, 3],
        [2009, 1],
        [2010, 2],
        [2011, 2],
        [2012, 1],
        [2013, 2],
        [2014, 1],
        [2015, 1],
        [2016, 1],
        [2017, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Proceedings of the 1988 connectionist models summer school",
      "link": "https://scholar.google.com/scholar?cluster=8503041365189089911&hl=en&oi=scholarr",
      "year": 1989,
      "cited_by": 18,
      "authors": ["Geoffrey E Hinton"],
      "description": null,
      "citation_histogram": [
        [1991, 1],
        [1992, 2],
        [1993, 1],
        [1994, 1],
        [1995, 2],
        [1996, 2],
        [1997, 4],
        [1998, 5]
      ],
      "detail_extracted": true
    },
    {
      "title": "Extracting distributed representations of concepts and relations from positive and negative propositions",
      "link": "https://ieeexplore.ieee.org/abstract/document/857906/",
      "year": 2000,
      "cited_by": 17,
      "authors": ["Alberto Paccanaro", "Geoffrey E Hinton"],
      "description": "Linear relational embedding (LRE) was introduced previously by the authors (1999) as a means of extracting a distributed representation of concepts from relational data. The original formulation cannot use negative information and cannot properly handle data in which there are multiple correct answers. In this paper we propose an extended formulation of LRE that solves both these problems. We present results in two simple domains, which show that learning leads to good generalization.",
      "citation_histogram": [
        [2000, 1],
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 1],
        [2006, 2],
        [2007, 1],
        [2008, 1],
        [2009, 2],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 1],
        [2014, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Glove-TalkII: Mapping hand gestures to speech using neural networks",
      "link": "https://proceedings.neurips.cc/paper/1994/hash/c2aee86157b4a40b78132f1e71a9e6f1-Abstract.html",
      "year": 1994,
      "cited_by": 17,
      "authors": ["Sidney Fels", "Geoffrey E Hinton"],
      "description": "Glove-TaikII is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped contin (cid: 173) uously to 10 control parameters of a parallel formant speech syn (cid: 173) thesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently, the best version of Glove-TalkII uses sev (cid: 173) eral input devices (including a CyberGlove, a ContactGlove, a 3-space tracker, and a foot-pedal), a parallel formant speech synthe (cid: 173) sizer and 3 neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed, user-defined relationship between hand-position and vowel sound and does not require any training examples from the user. Volume, fundamental frequency and stop consonants are produced with a fixed mapping from the input devices. One subject has trained to speak intelligibly with Glove-TalkII. He speaks slowly with speech quality similar to a text-to-speech synthesizer but with far more natural-sounding pitch variations.",
      "citation_histogram": [
        [1995, 1],
        [1996, 2],
        [1997, 1],
        [1998, 2],
        [1999, 1],
        [2000, 1],
        [2001, 2],
        [2002, 3],
        [2003, 1],
        [2004, 1],
        [2005, 1],
        [2006, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deflecting adversarial attacks",
      "link": "https://arxiv.org/abs/2002.07405",
      "year": 2020,
      "cited_by": 16,
      "authors": [
        "Yao Qin",
        "Nicholas Frosst",
        "Colin Raffel",
        "Garrison Cottrell",
        "Geoffrey Hinton"
      ],
      "description": "There has been an ongoing cycle where stronger defenses against adversarial attacks are subsequently broken by a more advanced defense-aware attack. We present a new approach towards ending this cycle where we \"deflect'' adversarial attacks by causing the attacker to produce an input that semantically resembles the attack's target class. To this end, we first propose a stronger defense based on Capsule Networks that combines three detection mechanisms to achieve state-of-the-art detection performance on both standard and defense-aware attacks. We then show that undetected attacks against our defense often perceptually resemble the adversarial target class by performing a human study where participants are asked to label images produced by the attack. These attack images can no longer be called \"adversarial'' because our network classifies them the same way as humans do.",
      "citation_histogram": [
        [2019, 1],
        [2020, 6],
        [2021, 8],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "System and method for parallelizing convolutional neural networks",
      "link": "https://patents.google.com/patent/US9563840B2/en",
      "year": 2017,
      "cited_by": 16,
      "authors": [
        "Alexander Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey E Hinton"
      ],
      "description": "A parallel convolutional neural network is provided. The CNN is implemented by a plurality of convolutional neural networks each on a respective processing node. Each CNN has a plurality of layers. A subset of the layers are interconnected between processing nodes such that activations are fed forward across nodes. The remaining subset is not so interconnected.",
      "citation_histogram": [
        [2016, 1],
        [2017, 3],
        [2018, 4],
        [2019, 5],
        [2020, 2],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "System and method for generating training cases for image classification",
      "link": "https://patents.google.com/patent/US9251437B2/en",
      "year": 2016,
      "cited_by": 16,
      "authors": [
        "Alexander Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey E Hinton"
      ],
      "description": "A system and method for generating training images. An existing training image is associated with a classification. The system includes an image processing module that performs color-space deformation on each pixel of the existing training image and then associates the classification to the color-space deformed training image. The technique may be applied to increase the size of a training set for training a neural network.",
      "citation_histogram": [
        [2017, 2],
        [2018, 2],
        [2019, 5],
        [2020, 4],
        [2021, 2],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Acoustics, speech and signal processing (icassp), 2013 ieee international conference on",
      "link": "https://scholar.google.com/scholar?cluster=5187255529575998608&hl=en&oi=scholarr",
      "year": 2013,
      "cited_by": 16,
      "authors": ["A Graves", "A-r Mohamed", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2017, 3],
        [2018, 3],
        [2019, 8],
        [2020, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Computation by neural networks",
      "link": "https://www.nature.com/articles/nn1100_1170",
      "year": 2000,
      "cited_by": 16,
      "authors": ["Geoffrey E Hinton"],
      "description": "Networks of neurons can perform computations that have proved very difficult to emulate in conventional computers. In trying to understand how real nervous systems achieve their remarkable computational abilities, researchers have been confronted with three major theoretical issues. How can we characterize the dynamics of neural networks with recurrent connections? How do the time-varying activities of populations of neurons represent things? How are synapse strengths adjusted to learn these representations? To gain insight into these difficult theoretical issues, it has proved necessary to study grossly idealized models that are as different from real biological neural networks as apples are from planets.The 1980s saw major progress on all three fronts. In a classic 1982 paper 1, Hopfield showed that asynchronous networks with symmetrically connected neurons would settle to locally stable states, known as'\u00a0\u2026",
      "citation_histogram": [
        [2003, 1],
        [2004, 1],
        [2005, 3],
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 1],
        [2012, 3],
        [2013, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Hand-printed digit recognition using",
      "link": "https://books.google.com/books?hl=en&lr=&id=eWBiKaOCNIYC&oi=fnd&pg=PA127&dq=info:ncnsO_hg-VkJ:scholar.google.com&ots=bLiZHUj3nT&sig=SkzfDu0-uaEfybIlRFGCJgm2NZs",
      "year": 1993,
      "cited_by": 16,
      "authors": ["Christopher KI Williams", "Geoffrey E Hinton"],
      "description": "Hand-printed characters can take on a great variety of shapes, especially when they are produced by a diverse population of writers. This variability makes the use of rigid templates impractical for hand-printed character recognition. It has long been realized (eg Ullmann, 1972; Burr, 1981b) that this limitation can be overcome by using elastically deformable models, and recent work (eg Yuille, 1990; Grenander et al., 1990) has provided a general framework for the problem. For images of single digits this framework implies that the best interpretation of an image is the model that minimizes an energy function that includes both the deformation energy of the digit model and the data misfit between the model and the image. An alternative approach to digit recognition is based on statistical pattern recognition techniques; an example is the use of a feedforward neural network for ZIP code digit recognition by le Cun et al.(1990). This method",
      "citation_histogram": [
        [1992, 2],
        [1993, 3],
        [1994, 2],
        [1995, 2],
        [1996, 1],
        [1997, 1],
        [1998, 2],
        [1999, 1],
        [2000, 1],
        [2001, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Teaching with commentaries",
      "link": "https://arxiv.org/abs/2011.03037",
      "year": 2020,
      "cited_by": 15,
      "authors": [
        "Aniruddh Raghu",
        "Maithra Raghu",
        "Simon Kornblith",
        "David Duvenaud",
        "Geoffrey Hinton"
      ],
      "description": "Effective training of deep neural networks can be challenging, and there remain many open questions on how to best learn these models. Recently developed methods to improve neural network training examine teaching: providing learned information during the training process to improve downstream model performance. In this paper, we take steps towards extending the scope of teaching. We propose a flexible teaching framework using commentaries, learned meta-information helpful for training on a particular task. We present gradient-based methods to learn commentaries, leveraging recent work on implicit differentiation for scalability. We explore diverse applications of commentaries, from weighting training examples, to parameterising label-dependent data augmentation policies, to representing attention masks that highlight salient image regions. We find that commentaries can improve training speed and/or performance, and provide insights about the dataset and training process. We also observe that commentaries generalise: they can be reused when training new models to obtain performance benefits, suggesting a use-case where commentaries are stored with a dataset and leveraged in future for improved model training.",
      "citation_histogram": [
        [2021, 8],
        [2022, 7]
      ],
      "detail_extracted": true
    },
    {
      "title": "Gradient-based learning applied to document recognation",
      "link": "https://scholar.google.com/scholar?cluster=17906841379168592381&hl=en&oi=scholarr",
      "year": 2017,
      "cited_by": 15,
      "authors": ["A Krizhevsky", "I Sutskever", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [2019, 1],
        [2020, 2],
        [2021, 5],
        [2022, 7]
      ],
      "detail_extracted": true
    },
    {
      "title": "Imagenet",
      "link": "https://scholar.google.com/scholar?cluster=11644954405066412928&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 15,
      "authors": ["A Krizhevsky", "I Sutskever", "EH Geoffrey"],
      "description": null,
      "citation_histogram": [
        [2015, 1],
        [2016, 2],
        [2017, 3],
        [2018, 7],
        [2019, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Does the brain do inverse graphics",
      "link": "http://www.cs.toronto.edu/~hinton/IPAM5.pdf",
      "year": 2012,
      "cited_by": 15,
      "authors": [
        "Geoffrey Hinton",
        "Alex Krizhevsky",
        "Navdeep Jaitly",
        "Tijmen Tieleman",
        "Yichuan Tang"
      ],
      "description": "\u2022 The idea is to define a simple decoder that produces an image by adding together contributions from each capsule.\u2022 Each capsule learns a fixed \u201ctemplate\u201d that gets intensity-scaled and translated differently for reconstruction each image.\u2022 The encoder must learn to extract the appropriate intensity and translation for each capsule from the input image.",
      "citation_histogram": [
        [2015, 1],
        [2016, 1],
        [2017, 3],
        [2018, 3],
        [2019, 2],
        [2020, 1],
        [2021, 4]
      ],
      "detail_extracted": true
    },
    {
      "title": "Une nouvelle approche de la cognition: le connexionnisme",
      "link": "https://www.cairn.info/load_pdf.php?ID_ARTICLE=DEBA_047_0045&download=1&from-feuilleteur=1",
      "year": 2007,
      "cited_by": 15,
      "authors": ["JL McClelland", "DE Rumelhart", "GE Hinton"],
      "description": "Sous le terme de connexionnisme se rangent plusieurs courants de recherche pr\u00e9sentant certaines diff\u00e9rences, techniques et th\u00e9oriques. Le groupe le plus important actuellement est bas\u00e9 \u00e0 Pittsburgh (universit\u00e9 Carnegie-Mellon) et \u00e0 San Diego (universit\u00e9 de Californie); les auteurs du pr\u00e9sent article, extrait d\u2019un ouvrage en deux volumes paru en 1986, en sont les principaux animateurs. Pour distinguer leur approche notamment de celle de J. Feldman et de D. Ballard (l\u2019\u00ab\u00e9cole de Rochester\u00bb), ils ont choisi la d\u00e9nomination plus sp\u00e9cifique parallel distributed processing (PDP), c\u2019est-\u00e0-dire \u00abtraitement parall\u00e8le r\u00e9parti\u00bb. Leurs th\u00e8ses, tout en \u00e9tant largement repr\u00e9sentatives du connexionnisme en g\u00e9n\u00e9ral, ont le m\u00e9rite de pr\u00e9senter le contraste le plus net avec celles de l\u2019intelligence artificielle et de la psychologie cognitive orthodoxes.",
      "citation_histogram": [
        [1988, 1],
        [1989, 1],
        [1990, 2],
        [1991, 1],
        [1992, 2],
        [1993, 2],
        [1994, 1],
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Relaxing the hyperplane assumption in the analysis and modification of back-propagation neural networks",
      "link": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.30.3571&rep=rep1&type=pdf",
      "year": 1994,
      "cited_by": 15,
      "authors": ["LY Pratt", "AN Christensen"],
      "description": "Several algorithms that operate on back-propagation networks, including some for topology determination, transfer, and visualization, depend on the hyperplane assumption. This is the assumption that the behavior of a network with sigmoidal logistic activations can be characterized by the positions of hidden unit hyperplanes whose activations have been simpli ed to threshold functions. This paper shows that the hyperplane assumption sometimes does not hold. We show an ecient algorithm that overcomes this problem by deriving a network's true decision boundary from network weights. We also show how to assign credit for points on the boundary to hyperplanes, allowing methods that depend on the hyperplane assumption (such as transfer between neural networks) to be made more robust by relaxing this dependence.",
      "citation_histogram": [
        [1994, 2],
        [1995, 3],
        [1996, 1],
        [1997, 2],
        [1998, 1],
        [1999, 1],
        [2000, 1],
        [2001, 2],
        [2002, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parallel Distributed Processing. Exploration of the Microstructure of Cognition. vol. 1: Foundations, DE Rumelhart and JL McClelland",
      "link": "https://scholar.google.com/scholar?cluster=5512744140660394475&hl=en&oi=scholarr",
      "year": 1986,
      "cited_by": 15,
      "authors": ["DE Rumelhart", "GE Hinton", "RJ Williams"],
      "description": null,
      "citation_histogram": [
        [1989, 1],
        [1990, 1],
        [1991, 2],
        [1992, 1],
        [1993, 1],
        [1994, 2],
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 1],
        [1999, 1],
        [2000, 1],
        [2001, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "The role of spatial working memory in shape perception",
      "link": "https://scholar.google.com/scholar?cluster=5923299397978560400&hl=en&oi=scholarr",
      "year": 1981,
      "cited_by": 15,
      "authors": ["GE Hinton"],
      "description": null,
      "citation_histogram": [
        [1982, 1],
        [1983, 3],
        [1984, 2],
        [1985, 2],
        [1986, 2],
        [1987, 1],
        [1988, 1],
        [1989, 1],
        [1990, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "r Mohamed A, Jaitly N, Senior A, Vanhoucke V, Nguyen P, Sainath TN, Kingsbury B (2012) Deep neural networks for acoustic modeling in speech recognition: The shared views of\u00a0\u2026",
      "link": "https://scholar.google.com/scholar?cluster=1661137290096125822&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 15,
      "authors": ["GE Hinton", "Li Deng", "Dong Yu", "George E Dahl"],
      "description": null,
      "citation_histogram": [
        [2015, 2],
        [2016, 2],
        [2017, 1],
        [2018, 3],
        [2019, 3],
        [2020, 2],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv",
      "link": "https://scholar.google.com/scholar?cluster=13375875139284595650&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 14,
      "authors": [
        "G Hinton",
        "N Srivastava",
        "Alex Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "description": null,
      "citation_histogram": [
        [2014, 3],
        [2015, 3],
        [2016, 1],
        [2017, 1],
        [2018, 1],
        [2019, 1],
        [2020, 1],
        [2021, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "The Helmholtz machine through time",
      "link": "https://www.cs.utoronto.ca/~hinton/absps/hmtt.pdf",
      "year": 1995,
      "cited_by": 14,
      "authors": ["G Hinton", "Peter Dayan", "Ava To", "Radford Neal"],
      "description": "We describe the \u201cwake-sleep\u201d algorithm that allows a multilayer, unsupervised, stochastic neural network to build a hierarchical, top-down generative model of an ensemble of data vectors. Because the generative model uses distributed representations that are a non-linear function of the input, it is intractable to compute the posterior probability distribution over hidden representations given the generative model and the current, data vector. It is therefore intractable to fit the gerrative del to data using standard techniques such as gradicit, descent or EM. Ir\u0131stcad of computing the posterior distribution exactly, a \u201cHelmholtz Machine\" uses a separate set of bottorn-up \u201crecogition\u201d connections to produce a compact approximation to the posterior distribution, The wake-sleep algorithm uses the top-down generative connections to provide training dlala for the bottom-up recognition connections ad vice versa. In this paper\u00a0\u2026",
      "citation_histogram": [
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 3],
        [2010, 2],
        [2011, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "TRAFFIC: A model of object recognition based on transformations of feature instances",
      "link": "https://scholar.google.com/scholar?cluster=7450360865575292680&hl=en&oi=scholarr",
      "year": 1988,
      "cited_by": 14,
      "authors": ["RS Zemel", "MC Mozer", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [1990, 1],
        [1991, 1],
        [1992, 2],
        [1993, 4],
        [1994, 1],
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Rumelhart",
      "link": "https://scholar.google.com/scholar?cluster=1847620853866556307&hl=en&oi=scholarr",
      "year": 1977,
      "cited_by": 14,
      "authors": ["DE Rumelhart", "GE Hinton", "RJ Williams"],
      "description": null,
      "citation_histogram": [
        [1977, 1],
        [1978, 2],
        [1979, 2],
        [1980, 1],
        [1981, 2],
        [1982, 1],
        [1983, 1],
        [1984, 1],
        [1985, 2],
        [1986, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Products of hidden Markov models: It takes N> 1 to tango",
      "link": "https://arxiv.org/abs/1205.2614",
      "year": 2012,
      "cited_by": 13,
      "authors": ["Graham W Taylor", "Geoffrey E Hinton"],
      "description": "Products of Hidden Markov Models(PoHMMs) are an interesting class of generative models which have received little attention since their introduction. This maybe in part due to their more computationally expensive gradient-based learning algorithm,and the intractability of computing the log likelihood of sequences under the model. In this paper, we demonstrate how the partition function can be estimated reliably via Annealed Importance Sampling. We perform experiments using contrastive divergence learning on rainfall data and data captured from pairs of people dancing. Our results suggest that advances in learning and evaluation for undirected graphical models and recent increases in available computing power make PoHMMs worth considering for complex time-series modeling tasks.",
      "citation_histogram": [
        [2011, 3],
        [2012, 1],
        [2013, 1],
        [2014, 4],
        [2015, 1],
        [2016, 1],
        [2017, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "An efficient learning procedure for deep Boltzmann machines",
      "link": "https://scholar.google.com/scholar?cluster=360329000693370896&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 13,
      "authors": ["G Hinton", "R Salakhutdinov"],
      "description": null,
      "citation_histogram": [
        [2015, 1],
        [2016, 2],
        [2017, 3],
        [2018, 5],
        [2019, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Probabilistic sequential independent components analysis",
      "link": "https://ieeexplore.ieee.org/abstract/document/1310357/",
      "year": 2004,
      "cited_by": 13,
      "authors": ["Max Welling", "Richard S Zemel", "Geoffrey E Hinton"],
      "description": "Under-complete models, which derive lower dimensional representations of input data, are valuable in domains in which the number of input dimensions is very large, such as data consisting of a temporal sequence of images. This paper presents the under-complete product of experts (UPoE), where each expert models a one-dimensional projection of the data. Maximum-likelihood learning rules for this model constitute a tractable and exact algorithm for learning under-complete independent components. The learning rules for this model coincide with approximate learning rules proposed earlier for under-complete independent component analysis (UICA) models. This paper also derives an efficient sequential learning algorithm from this model and discusses its relationship to sequential independent component analysis (ICA), projection pursuit density estimation, and feature induction algorithms for additive\u00a0\u2026",
      "citation_histogram": [
        [2005, 1],
        [2006, 4],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Minimizing description length in an unsupervised neural network",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.111.1574&rep=rep1&type=pdf",
      "year": 1997,
      "cited_by": 13,
      "authors": ["Geoffrey E Hinton", "Richard S Zemel"],
      "description": "An autoencoder network uses a set of recognition weights to convert an input vector into a representation vector. It then uses a set of generative weights to convert the representation vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the representation vector and the reconstruction error. This information is minimized by choosing representation vectors stochastically according to a Boltzmann distribution. Unfortunately, if the representation vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible representation vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution. This approximation corresponds to using a suboptimal encoding scheme and therefore gives an upper bound on the minimal description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn distributed representations in which many different hidden causes combine to produce each observed data vector. Such representations can be exponentially more efficient in their use of hardware than standard vector quantization or mixture models.",
      "citation_histogram": [
        [1998, 1],
        [1999, 1],
        [2000, 1],
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 2],
        [2005, 2],
        [2006, 2],
        [2007, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Phoneme recognition using time-delay neural networks (Technical Report TR-I-0006)",
      "link": "https://scholar.google.com/scholar?cluster=5072648046235760133&hl=en&oi=scholarr",
      "year": 1987,
      "cited_by": 13,
      "authors": ["A Waibel", "T Hanazawa", "G Hinton", "K Shikano", "K Lang"],
      "description": null,
      "citation_histogram": [
        [1990, 1],
        [1991, 2],
        [1992, 2],
        [1993, 1],
        [1994, 2],
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Pattern matching and variable binding in a stochastic neural network",
      "link": "https://scholar.google.com/scholar?cluster=3802775394973122489&hl=en&oi=scholarr",
      "year": 1987,
      "cited_by": 13,
      "authors": ["David S Touretzky", "Geoffrey E Hinton"],
      "description": "DCPS, a Distributed Connectionist Production System, was developed as a demonstration that connectionist models could exhibit general symbol processing behavior (Touret-zky and Hinton 1985, Touretzky and Hinton 1986). Connectionist models, also known as PDP (Parallel Distributed Processing) models, are networks composed of many simple homogeneous computing elements that are highly interconnected (Feldman 1982, Rumel-hart and McClelland 1986). In designing these networks, connectionists take inspiration from what is known about the structure of the brain, but their models are not required to be anatomically or physiologically correct. The central idea is that from the collec-tive activity of many individual units, each computing a simple function in parallel--as neurons do-intelligent behavior may emerge.DCPS is a forward chaining production system architecture. It is similar in favor to OPS5 (Brownston et al 1985), and not to systems like EMYCIN which employ goaldirected backward chaining (Davis and King 1977). DCPS uses highly restricted rule and working memory formats; it is not as powerful as OPS5 or EMYCIN, but in some respects it is quite powerful, particularly since it performs rule matching entirely in parallel. The DCPS model is implemented as a Boltzmann machine: a stochastic Hopfield net (Hopfield 1982) that employs simulated annealing to search for global energy minima (Fahlman, Hinton, and Sejnowski 1983, Ackley, Hinton, and Sejnowski 1985). In this case, a global energy minimum corresponds to the selection of a production rule and a pair of working memory elements that match the rule's left hand side\u00a0\u2026",
      "citation_histogram": [
        [1989, 1],
        [1990, 1],
        [1991, 3],
        [1992, 2],
        [1993, 1],
        [1994, 2],
        [1995, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "PDP: Computational models of cognition and perception, I",
      "link": "https://scholar.google.com/scholar?cluster=5510264766256089435&hl=en&oi=scholarr",
      "year": 1986,
      "cited_by": 13,
      "authors": ["DE Rumelhart", "GE Hinton", "RJ Williams"],
      "description": null,
      "citation_histogram": [
        [2003, 1],
        [2004, 1],
        [2005, 4],
        [2006, 2],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Convolutional Deep Belief Networks on CIFAR-10. 2010",
      "link": "https://scholar.google.com/scholar?cluster=17186825837604786961&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 13,
      "authors": ["A Krizhevsky", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2011, 1],
        [2012, 1],
        [2013, 1],
        [2014, 2],
        [2015, 3],
        [2016, 2],
        [2017, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "The EM Algorithm for Mixtures of Factor Analyzers. CRG; 1996",
      "link": "https://scholar.google.com/scholar?cluster=16716157597243821950&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 13,
      "authors": ["Z Ghahramani", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [2000, 1],
        [2001, 1],
        [2002, 2],
        [2003, 1],
        [2004, 2],
        [2005, 2],
        [2006, 2],
        [2007, 1],
        [2008, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Proceedings of the Sixth Annual Conference on Computational Learning Theory. COLT\u201993",
      "link": "https://scholar.google.com/scholar?cluster=15462153299249075939&hl=en&oi=scholarr",
      "year": 1993,
      "cited_by": 12,
      "authors": ["GE Hinton", "D Van Camp"],
      "description": null,
      "citation_histogram": [
        [1996, 1],
        [1997, 1],
        [1998, 2],
        [1999, 3],
        [2000, 3],
        [2001, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Discovering high order features with mean field modules",
      "link": "https://proceedings.neurips.cc/paper/1989/hash/a4f23670e1833f3fdb077ca70bbd5d66-Abstract.html",
      "year": 1990,
      "cited_by": 12,
      "authors": ["Conrad C Galland", "Geoffrey E Hinton"],
      "description": "A new form of the deterministic Boltzmann machine (DBM) learn (cid: 173) ing procedure is presented which can efficiently train network mod (cid: 173) ules to discriminate between input vectors according to some cri (cid: 173) terion. The new technique directly utilizes the free energy of these\" mean field modules\" to represent the probability that the criterion is met, the free energy being readily manipulated by the learning procedure. Although conventional deterministic Boltzmann learn (cid: 173) ing fails to extract the higher order feature of shift at a network bottleneck, combining the new mean field modules with the mu (cid: 173) tual information objective function rapidly produces modules that perfectly extract this important higher order feature without direct external supervision.",
      "citation_histogram": [
        [1993, 1],
        [1994, 2],
        [1995, 2],
        [1996, 1],
        [1997, 1],
        [1998, 1],
        [1999, 2],
        [2000, 1],
        [2001, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Connectionist models summer school",
      "link": "http://papers.cnl.salk.edu/PDFs/Proceedings%20of%20the%201988%20Connectionist%20Models%20Summer%20School%201989-3370.pdf",
      "year": 1989,
      "cited_by": 12,
      "authors": ["David Touretzky", "Geoffrey Hinton"],
      "description": "\u00a9INIINIIEOTDOINIDST Page 1 Proceedings of the 1988 \u00a9INIINIIEOTDOINIDST Carnegie \nMellon University Editors: David Touretzky Geoffrey Hinton Terrence Sejnowski Page 2 \nProceedings of the 1988 Connectionist Models Summer School David Touretzky (Carnegie \nMellon University) Geoffrey Hinton (University of Toronto) Terrence Sejnowski (University of \nCalifornia, San Diego and the Salk Institute) Morgan Kaufmann Publishers 2929 Campus Drive \nSuite 260 San Mateo, CA 94403 Page 3 Editor and President Michael \u0412. Morgan Coordinating \nEditor Beverly Kenn\u00f3n-Kelley Production Manager Shirley Jozvell Production Assistant \nElizabeth Myhr Cover Designer Andrea Hendrick Compositor Kennon-Kelley Graphic Design \nLibrary of Congress Cataloging-in-Publication Data is Available 88-031992 Proceedings of \nthe 1988 Connectionist Models Summer School ISBN 0-55860-015-9 MORGAN KAUFMANN \u2026",
      "citation_histogram": [
        [1990, 2],
        [1991, 1],
        [1992, 1],
        [1993, 1],
        [1994, 2],
        [1995, 2],
        [1996, 1],
        [1997, 1],
        [1998, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep learning",
      "link": "https://scholar.google.com/scholar?cluster=5244965670272626378&hl=en&oi=scholarr",
      "year": 2015,
      "cited_by": 11,
      "authors": ["H Geoffrey", "L Yann", "B Yoshua"],
      "description": null,
      "citation_histogram": [
        [2018, 2],
        [2019, 4],
        [2020, 3],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Fast neural network emulation of dynamical systems for computer animation",
      "link": "https://proceedings.neurips.cc/paper/1998/hash/92af93f73faf3cefc129b6bc55a748a9-Abstract.html",
      "year": 1998,
      "cited_by": 11,
      "authors": [
        "Radek Grzeszczuk",
        "Demetri Terzopoulos",
        "Geoffrey E Hinton"
      ],
      "description": "Computer animation through the numerical simulation of physics-based graphics models offers unsurpassed realism, but it can be computation (cid: 173) ally demanding. This paper demonstrates the possibility of replacing the numerical simulation of nontrivial dynamic models with a dramatically more efficient\" NeuroAnimator\" that exploits neural networks. Neu (cid: 173) roAnimators are automatically trained off-line to emulate physical dy (cid: 173) namics through the observation of physics-based models in action. De (cid: 173) pending on the model, its neural network emulator can yield physically realistic animation one or two orders of magnitude faster than conven (cid: 173) tional numerical simulation. We demonstrate NeuroAnimators for a va (cid: 173) riety of physics-based models.",
      "citation_histogram": [
        [2006, 1],
        [2007, 2],
        [2008, 2],
        [2009, 3],
        [2010, 1],
        [2011, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parameter estimation for linear dynamical systems University of Toronto technical report",
      "link": "https://scholar.google.com/scholar?cluster=4839915513911307407&hl=en&oi=scholarr",
      "year": 1996,
      "cited_by": 11,
      "authors": ["Zoubin Ghahramani", "Geoffrey E Hinton"],
      "description": null,
      "citation_histogram": [
        [1998, 1],
        [1999, 1],
        [2000, 1],
        [2001, 2],
        [2002, 1],
        [2003, 1],
        [2004, 2],
        [2005, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Using a neural net to instantiate a deformable model",
      "link": "https://proceedings.neurips.cc/paper/1994/hash/fba9d88164f3e2d9109ee770223212a0-Abstract.html",
      "year": 1994,
      "cited_by": 11,
      "authors": ["Christopher Williams", "Michael Revow", "Geoffrey E Hinton"],
      "description": "Deformable models are an attractive approach to recognizing non (cid: 173) rigid objects which have considerable within class variability. How (cid: 173) ever, there are severe search problems associated with fitting the models to data. We show that by using neural networks to provide better starting points, the search time can be significantly reduced. The method is demonstrated on a character recognition task.",
      "citation_histogram": [
        [1996, 2],
        [1997, 2],
        [1998, 2],
        [1999, 1],
        [2000, 1],
        [2001, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning to make coherent predictions in domains with discontinuities",
      "link": "https://proceedings.neurips.cc/paper/1991/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html",
      "year": 1991,
      "cited_by": 11,
      "authors": ["Suzanna Becker", "Geoffrey E Hinton"],
      "description": "We have previously described an unsupervised learning procedure that discovers spatially coherent propertit> _<; of the world by maximizing the in (cid: 173) formation that parameters extracted from different parts of the sensory input convey about some common underlying cause. When given random dot stereograms of curved surfaces, this procedure learns to extract sur (cid: 173) face depth because that is the property that is coherent across space. It also learns how to interpolate the depth at one location from the depths at nearby locations (Becker and Hint. oll. 1992). 1n this paper, we pro (cid: 173) pose two new models which handle surfaces with discontinuities. The first model attempts to detect cases of discontinuities and reject them. The second model develops a mixture of expert interpolators. It learns to de (cid: 173) tect the locations of discontinuities and to invoke specialized, asymmetric interpolators that do not cross the discontinuities.",
      "citation_histogram": [
        [1992, 1],
        [1993, 1],
        [1994, 1],
        [1995, 2],
        [1996, 1],
        [1997, 1],
        [1998, 1],
        [1999, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Neural network architectures for artificial intelligence",
      "link": "https://dl.acm.org/doi/abs/10.5555/63641",
      "year": 1988,
      "cited_by": 11,
      "authors": ["Geoffrey E Hinton"],
      "description": "Neural network architectures for artificial intelligence | Guide books ACM Digital Library \nhome ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register \nAdvanced Search Journals Magazines Proceedings Books SIGs Conferences People More \nSearch ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library \nCollections More HomeBrowse by TitleBooksNeural network architectures for artificial \nintelligence ABSTRACT No abstract available. Index Terms 1.Neural network architectures \nfor artificial intelligence 1.Computing methodologies 1.Artificial intelligence 1.Philosophical/theoretical \nfoundations of artificial intelligence 1.Cognitive science 2.Modeling and simulation 2.Human-centered \ncomputing 1.Human computer interaction (HCI) Comments Login options Check if you have \naccess through your login credentials or your institution to get full access on this article. \u2026",
      "citation_histogram": [
        [1989, 2],
        [1990, 1],
        [1991, 1],
        [1992, 1],
        [1993, 2],
        [1994, 2],
        [1995, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Neurocomputing: foundations of research, ch. Learning representations by back-propagating errors",
      "link": "https://scholar.google.com/scholar?cluster=1437536640333602380&hl=en&oi=scholarr",
      "year": 1988,
      "cited_by": 11,
      "authors": [
        "David E Rumelhart",
        "Geoffrey E Hinton",
        "Ronald J Williams"
      ],
      "description": null,
      "citation_histogram": [
        [2013, 2],
        [2014, 1],
        [2015, 2],
        [2016, 5],
        [2017, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Delve data for evaluating learning in valid experiments, 1995\u20131996",
      "link": "https://scholar.google.com/scholar?cluster=14015125972323222483&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 11,
      "authors": [
        "CE Rasmussen",
        "RM Neal",
        "G Hinton",
        "D Van Camp",
        "M Revow",
        "Z Ghahramani",
        "R Kustra",
        "R Tibshirani"
      ],
      "description": null,
      "citation_histogram": [
        [1998, 1],
        [1999, 2],
        [2000, 1],
        [2001, 1],
        [2002, 1],
        [2003, 2],
        [2004, 1],
        [2005, 1],
        [2006, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep learning.\" nature 521.7553 (2015): 436",
      "link": "https://scholar.google.com/scholar?cluster=14052226288502349187&hl=en&oi=scholarr",
      "year": 2015,
      "cited_by": 10,
      "authors": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton"],
      "description": null,
      "citation_histogram": [
        [2019, 5],
        [2020, 3],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "COURSERA: Neural Networks Mach",
      "link": "https://scholar.google.com/scholar?cluster=4893354966222893479&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 10,
      "authors": ["T Tieleman", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2017, 2],
        [2018, 2],
        [2019, 3],
        [2020, 1],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Data for evaluating learning in valid experiments (delve)",
      "link": "https://scholar.google.com/scholar?cluster=11293518581089889083&hl=en&oi=scholarr",
      "year": 2003,
      "cited_by": 10,
      "authors": [
        "CE Rasmussen",
        "R Neal",
        "G Hinton",
        "D Camp",
        "M Revow",
        "Z Ghahramani",
        "R Kustra",
        "R Tibshirani"
      ],
      "description": null,
      "citation_histogram": [
        [2008, 1],
        [2009, 1],
        [2010, 2],
        [2011, 1],
        [2012, 3],
        [2013, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Training many small hidden markov models",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.1171",
      "year": 2001,
      "cited_by": 10,
      "authors": ["GE Hinton", "AD Brown"],
      "description": "This paper describes research in progress on two quite different ways of training systems that are composed of many small Hidden Markov Models (HMM's). The first is a purely discriminative method in which all of the parameters of all the HMM's are adjusted to optimize classification performance. The second is an unsupervised method in which many little HMM's are used to model the probability density of a single set of sequences. HMM's have been very successful in automatic speech recognition, mainly because there is an efficient way of fitting an HMM to data: the forward-backward algorithm and the Baum-Welch reestimation formulas. Despite this success, HMM's have several major limitations as models of sequential data. They represent the recent history of the sequence using a single, discrete K-state multinomial. The efficiency of the Baum-Welch restimation algorithm depends on this fact, but it severely limits the representational power of the model. The hidden state of a single HMM can only convey log 2 K",
      "citation_histogram": [
        [2001, 1],
        [2002, 3],
        [2003, 3],
        [2004, 1],
        [2005, 1],
        [2006, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Technical report",
      "link": "https://scholar.google.com/scholar?cluster=13433173948814138169&hl=en&oi=scholarr",
      "year": 1996,
      "cited_by": 10,
      "authors": ["Z Ghahramani", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 2],
        [2014, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Using mixtures of deformable models to capture variations in hand printed digits",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.4747",
      "year": 1993,
      "cited_by": 10,
      "authors": [
        "Michael Revow",
        "Christopher KI Williams",
        "Geoffrey E Hinton"
      ],
      "description": "Deformable models are an attractive way for characterizing handwritten digits since they have relatively few parameters, are able to capture many topological variations, and incorporate much prior knowledge. We have described a system [8] that uses learned digit models consisting of splines whose shape is governed by a small number of control points. Images can be classified by separately fitting each digit model to the image, and using a simple neural network to decide which model fits best. We use an elastic matching algorithm to minimize an energy function that includes both the deformation energy of the digit model and the log probability that the model would generate the inked pixels in the image. The use of multiple models for each digit can characterize the population of handwritten digits better. We show how multiple models may be used without increasing the time required for elastic matching.",
      "citation_histogram": [
        [1993, 1],
        [1994, 1],
        [1995, 2],
        [1996, 1],
        [1997, 3],
        [1998, 1],
        [1999, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Connectionist learning procedure",
      "link": "https://ci.nii.ac.jp/naid/10004158721/",
      "year": 1990,
      "cited_by": 10,
      "authors": ["GE Hinton"],
      "description": "CiNii \u8ad6\u6587 - Connectionist Learning Procedure CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3\n] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \n\u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\n\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 [3/11\u66f4\u65b0]2022\u5e744\u67081\u65e5\u304b\u3089\u306e\nCiNii Articles\u306eCiNii Research\u3078\u306e\u7d71\u5408\u306b\u3064\u3044\u3066 Connectionist Learning Procedure HINTON \nGE \u88ab\u5f15\u7528\u6587\u732e: 2\u4ef6 \u8457\u8005 HINTON GE \u53ce\u9332\u520a\u884c\u7269 Machine Learning Machine Learning, 1990 \nThe MIT Press \u88ab\u5f15\u7528\u6587\u732e: 2\u4ef6\u4e2d 1-2\u4ef6\u3092 \u8868\u793a 1 \u5b89\u5b9a\u6027\u3068\u901f\u5fdc\u6027\u306b\u95a2\u3059\u308b\u30ed\u30d0\u30b9\u30c8\u6027\u80fd\u6307\u6a19\u306b\n\u57fa\u3065\u304f\u30de\u30eb\u30c1\u30d7\u30eb\u30b3\u30f3\u30c8\u30ed\u30fc\u30eb\u30b7\u30b9\u30c6\u30e0 \u7a32\u8449 \u96c5\u7ae0 , \u90ed \u6d77\u86df , \u963f\u90e8 \u5065\u4e00 , \u4e2d\u5c3e \u548c\u592b \u96fb\u6c17\u5b66\u4f1a\u8ad6\u6587\u8a8c\n. C, \u96fb\u5b50\u30fb\u60c5\u5831\u30fb\u30b7\u30b9\u30c6\u30e0\u90e8\u9580\u8a8c = The transactions of the Institute of Electrical Engineers of \nJapan. C, A publication of Electronics, Information and System Society 117(12), 1818-1826, \u2026",
      "citation_histogram": [
        [1997, 1],
        [1998, 1],
        [1999, 2],
        [2000, 1],
        [2001, 1],
        [2002, 3],
        [2003, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "In DE Rumelhart & JL McClelland",
      "link": "https://scholar.google.com/scholar?cluster=18023731316744579439&hl=en&oi=scholarr",
      "year": 1986,
      "cited_by": 10,
      "authors": ["David E Rumelhart", "GE Hinton", "RJ Williams"],
      "description": null,
      "citation_histogram": [
        [1987, 1],
        [1988, 1],
        [1989, 1],
        [1990, 1],
        [1991, 1],
        [1992, 1],
        [1993, 1],
        [1994, 1],
        [1995, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Inferring the meaning of direct perception",
      "link": "https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/inferring-the-meaning-of-direct-perception/4EEBA814921E498F6DB64213134EFD40",
      "year": 1980,
      "cited_by": 10,
      "authors": ["Geoffrey E Hinton"],
      "description": "//static.cambridge.org/content/id/urn%3Acambridge.org%3Aid%3Aarticle%3AS0140525X00005549/resource/name/firstPage-S0140525X00005549a.jpg",
      "citation_histogram": [
        [1985, 1],
        [1986, 3],
        [1987, 1],
        [1988, 1],
        [1989, 1],
        [1990, 1],
        [1991, 1],
        [1992, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Sejnows ki, TJ (1 9 85)",
      "link": "https://scholar.google.com/scholar?cluster=6160353127254649756&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 10,
      "authors": ["DH Ackley", "GA Hinton"],
      "description": null,
      "citation_histogram": [
        [1986, 1],
        [1987, 1],
        [1988, 1],
        [1989, 1],
        [1990, 1],
        [1991, 1],
        [1992, 1],
        [1993, 1],
        [1994, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "RMSProp adaptive learning",
      "link": "https://scholar.google.com/scholar?cluster=4931489485546531296&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 9,
      "authors": ["T Tieleman", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2017, 1],
        [2018, 4],
        [2019, 2],
        [2020, 1],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning multiple layers of features from",
      "link": "https://scholar.google.com/scholar?cluster=568923701030484784&hl=en&oi=scholarr",
      "year": 2009,
      "cited_by": 9,
      "authors": ["Alex Krizhevsky"],
      "description": null,
      "citation_histogram": [
        [2011, 1],
        [2012, 1],
        [2013, 1],
        [2014, 2],
        [2015, 2],
        [2016, 1],
        [2017, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "On contrastive divergence learning. 10th Int",
      "link": "https://scholar.google.com/scholar?cluster=8238193326642514935&hl=en&oi=scholarr",
      "year": 2005,
      "cited_by": 9,
      "authors": ["M Carreira-Perpinan", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2007, 2],
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 1],
        [2012, 2],
        [2013, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parameter estimation for linear dynamical systems.(crg-tr-96-2)",
      "link": "https://scholar.google.com/scholar?cluster=8740958007922050961&hl=en&oi=scholarr",
      "year": 1996,
      "cited_by": 9,
      "authors": ["Zoubin Ghahramani", "Goeffrey E Hinton"],
      "description": null,
      "citation_histogram": [
        [2000, 2],
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning in massively parallel nets",
      "link": "https://www.aaai.org/Library/AAAI/1986/aaai86-190.php",
      "year": 1986,
      "cited_by": 9,
      "authors": ["Geoffrey E Hinton"],
      "description": "The human brain is very different from a conventional digital computer. It relies on massive parallelism rather than raw speed and it stores long-term knowledge by modifying the way its processing elements interact rather than by setting bits in a passive, general purpose memory. It is robust against minor physical damage and it learns from experience instead of being explicitly programmed. We do not yet know how the brain uses the activities of neurons to represent complex, articulated structures, or how the perceptual system turns the raw input into useful internal representations so rapidly. Nor do we know how the brain learns new representational schemes. But over the past few years there have been a lot of new and interesting theories about these issues. Much of the theorizing has been motivated by the belief that the brain is using computational principles which could also be applied to massively parallel artificial systems, if only we knew what the principles were.",
      "citation_histogram": [
        [1987, 1],
        [1988, 1],
        [1989, 1],
        [1990, 3],
        [1991, 2],
        [1992, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Novice use of interactive graph-plotting system",
      "link": "https://scholar.google.com/scholar?cluster=214511760574711326&hl=en&oi=scholarr",
      "year": 1983,
      "cited_by": 9,
      "authors": [
        "N Hammond",
        "A MacLean",
        "G Hinton",
        "J Long",
        "P Barnard",
        "IA Clark"
      ],
      "description": null,
      "citation_histogram": [
        [1986, 1],
        [1987, 2],
        [1988, 1],
        [1989, 2],
        [1990, 1],
        [1991, 1],
        [1992, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep belief networks",
      "link": "https://pdfs.semanticscholar.org/1c2b/b1fbc618dc2103889c620181785dfa838807.pdf",
      "year": 2009,
      "cited_by": 8,
      "authors": ["Ruslan Salakhutdinov", "GE Hinton"],
      "description": "Recently, Hinton et al.[6] derived a way to perform fast, greedy learning of deep belief networks (DBN) one layer at a time, with the top two layers forming an undirected bipartite graph (associate memory).",
      "citation_histogram": [
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 2],
        [2014, 1],
        [2015, 1],
        [2016, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning Distributed Representations by Mapping Concepts and Relations into a Linear Space.",
      "link": "https://www.cs.rhbnc.ac.uk/~alberto/PAPERS.PDF/icml2000.pdf",
      "year": 2000,
      "cited_by": 8,
      "authors": ["Alberto Paccanaro", "Geoffrey E Hinton"],
      "description": "Linear Relational Embedding is a method of learning a distributed representation of concepts from data consisting of binary relations between concepts. concepts are represented as vectors, binary relations as matrices, and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept. A representation for concepts and relations is learned by maximizing an appropriate discriminative goodness function using gradient ascent. On a task involving family relationships, learning is fast and leads to good generalization.",
      "citation_histogram": [
        [2001, 1],
        [2002, 1],
        [2003, 2],
        [2004, 1],
        [2005, 1],
        [2006, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Modeling high-dimensional data by combining simple experts",
      "link": "https://www.aaai.org/Papers/AAAI/2000/AAAI00-211.pdf",
      "year": 2000,
      "cited_by": 8,
      "authors": ["Geoffrey E Hinton"],
      "description": "It is possible to combine multiple non-linear probabilistic models of the same data by multiplying the probability distributions together and then renormalizing. A \u201cproduct of experts\u201d is a very efficient way to model data that simultaneously satisfies many different constraints. It is difficult to fit a product of experts to data using maximum likelihood because the gradient of the log likelihood is intractable, but there is an efficient way of optimizing a different objective function and this produces good models of high-dimensional data.",
      "citation_histogram": [
        [2005, 1],
        [2006, 3],
        [2007, 2],
        [2008, 1],
        [2009, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "The EM algorithm for factor analyzers Technical Report CRG-TR-96-1",
      "link": "https://scholar.google.com/scholar?cluster=12101531116085255631&hl=en&oi=scholarr",
      "year": 1997,
      "cited_by": 8,
      "authors": ["Z Ghahramani", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [2009, 1],
        [2010, 1],
        [2011, 2],
        [2012, 1],
        [2013, 1],
        [2014, 1],
        [2015, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning internal representations by error propagation. Parallel Distributed Processing (PDP): Exploration in the Microstructure of Cognition\", Vol. 1, Chapter 8",
      "link": "https://scholar.google.com/scholar?cluster=8902015293531977872&hl=en&oi=scholarr",
      "year": 1986,
      "cited_by": 8,
      "authors": ["DE Rumelhart", "GE Hinton", "RJ Williams"],
      "description": null,
      "citation_histogram": [
        [1989, 1],
        [1990, 1],
        [1991, 1],
        [1992, 2],
        [1993, 1],
        [1994, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Rmsprop: Divide the Gradient by a Running Average of Its Recent Magnitude, 2012",
      "link": "https://scholar.google.com/scholar?cluster=14426273534795529131&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 8,
      "authors": ["Tijmen Tieleman", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2018, 5],
        [2019, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Visualizing data using t-sne laurens van der maaten micc-ikat",
      "link": "https://scholar.google.com/scholar?cluster=14677949190473238559&hl=en&oi=scholarr",
      "year": 2017,
      "cited_by": 7,
      "authors": ["G Hinton"],
      "description": null,
      "citation_histogram": [
        [2019, 1],
        [2020, 3],
        [2021, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Dropout: A simple way to prevent neural networks from overfitting, 2014",
      "link": "https://scholar.google.com/scholar?cluster=10728953027903007504&hl=en&oi=scholarr",
      "year": 2016,
      "cited_by": 7,
      "authors": [
        "Nitish Srivastava",
        "Geoffrey Hinton",
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Ruslan Salakhutdinov"
      ],
      "description": null,
      "citation_histogram": [
        [2017, 1],
        [2018, 2],
        [2019, 1],
        [2020, 1],
        [2021, 1],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Lecture 4a, Learning to predict the next word",
      "link": "https://scholar.google.com/scholar?cluster=11725064553915625337&hl=en&oi=scholarr",
      "year": 2015,
      "cited_by": 7,
      "authors": ["Geoffrey Hinton"],
      "description": null,
      "citation_histogram": [
        [2017, 1],
        [2018, 1],
        [2019, 1],
        [2020, 1],
        [2021, 1],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Taking inverse graphics seriously",
      "link": "http://www.csri.utoronto.ca/~hinton/csc2535/notes/lec6b.pdf",
      "year": 2013,
      "cited_by": 7,
      "authors": ["Geoffrey Hinton"],
      "description": "\u2022 The idea is to define a simple decoder that produces an image by adding together contributions from each capsule.\u2022 Each capsule learns a fixed \u201ctemplate\u201d that gets intensity-scaled and translated differently for reconstructing each image.\u2022 The encoder must learn to extract the appropriate intensity and translation for each capsule from the input image.",
      "citation_histogram": [
        [2018, 2],
        [2019, 1],
        [2020, 1],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Nov 2008",
      "link": "https://scholar.google.com/scholar?cluster=17045742765545606514&hl=en&oi=scholarr",
      "year": 2008,
      "cited_by": 7,
      "authors": ["LJP van der Maaten", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [2018, 2],
        [2019, 2],
        [2020, 2],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "User\u2019s Guide for t-SNE Software",
      "link": "https://sccn.ucsd.edu/svn/software/tags/EEGLAB7_0_2_9beta/external/fieldtrip-20090727/classification/toolboxes/maaten/tsne/tsne_user_guide2.pdf",
      "year": 2008,
      "cited_by": 7,
      "authors": ["Laurens van der Maaten", "Geoffrey Hinton"],
      "description": "In this document, we describe the use of the t-SNE software that is publicly available online from http://ticc. uvt. nl/~ lvdrmaaten/tsne. Please note that this document does not describe the t-SNE technique itself; for more information on how t-SNE works, we refer to (van der Maaten and Hinton, 2008). The user\u2019s guide assumes that the Matlab environment is used, however, the discussion in section 3 is also of relevance to the use of the t-SNE software in programs that are written in, eg, R, Java, or C++.",
      "citation_histogram": [
        [2018, 2],
        [2019, 1],
        [2020, 2],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "andR. R. Salakhutdinov,\u201cReducingtheDimensionalityofDatawithNeuralNetG works\u201d",
      "link": "https://scholar.google.com/scholar?cluster=7827355741861197635&hl=en&oi=scholarr",
      "year": 2006,
      "cited_by": 7,
      "authors": ["Geoffrey E Hinton"],
      "description": null,
      "citation_histogram": [
        [2015, 1],
        [2016, 1],
        [2017, 4],
        [2018, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Proceedings of the 10th International Workshop on Artificial Intelligence and Statistics",
      "link": "https://scholar.google.com/scholar?cluster=125981226092065374&hl=en&oi=scholarr",
      "year": 2005,
      "cited_by": 7,
      "authors": ["MA Carreira-Perpinan", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [2012, 1],
        [2013, 1],
        [2014, 1],
        [2015, 1],
        [2016, 1],
        [2017, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Relative density nets: a new way to combine backpropagation with hmm's",
      "link": "https://proceedings.neurips.cc/paper/2001/hash/9bb6dee73b8b0ca97466ccb24fff3139-Abstract.html",
      "year": 2001,
      "cited_by": 7,
      "authors": ["Andrew Brown", "Geoffrey E Hinton"],
      "description": "Logistic units in the first hidden layer of a feedforward neural net (cid: 173) work compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's.",
      "citation_histogram": [
        [2005, 2],
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Unsupervised Learning: Foundations of Neural Computation (Computational Neuroscience)",
      "link": "https://scholar.google.com/scholar?cluster=5089313448880872303&hl=en&oi=scholarr",
      "year": 1999,
      "cited_by": 7,
      "authors": ["Geoffrey E Hinton", "Terrence J Sejnowski"],
      "description": null,
      "citation_histogram": [
        [2011, 1],
        [2012, 1],
        [2013, 1],
        [2014, 1],
        [2015, 2],
        [2016, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Combining two methods of recognizing hand-printed digits",
      "link": "http://www.cs.toronto.edu/~hinton/absps/combiningtwo.pdf",
      "year": 1992,
      "cited_by": 7,
      "authors": ["G Hinton", "C Williams", "M Revow"],
      "description": "Hand-printed digits can be recognized quite well by a feedforward neural network that uses equality constraints between weights to achieve limited translation^ invariance. However, the net has no explicit model of what a digit looks like and this can lead it to make confident errors. An alternative approach, which incorporates much more prior knowledge, is to use explicit deformable models of the digits and to recognize a digit by finding which model fits best. We describe a system that uses learned digit models which consist of splines whose shape is governed by 8 control points. The elastic models are good at capturing shape knowledge, and the elastic matching process is good at rejecting parts of the image that are best explained as noise. However, the elastic matching is slow and can get trapped in local optima if the initial configuration of th\u00e8 elastic model is far from the actual data. So we are developing a\u00a0\u2026",
      "citation_histogram": [
        [1993, 1],
        [1994, 2],
        [1995, 1],
        [1996, 1],
        [1997, 1],
        [1998, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Mental simulation",
      "link": "https://link.springer.com/content/pdf/10.1038/347627a0.pdf",
      "year": 1990,
      "cited_by": 7,
      "authors": ["Geoffrey Hinton"],
      "description": "An Introduction to Neural Computing. ByIgor Aleksander and Helen Morton. Chapman and Hal1:1990. Pp. 240. Pbk \u00a315.95*.",
      "citation_histogram": [
        [2008, 1],
        [2009, 1],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "CONNECTIONIST LEARNING PROCEDURES11This chapter appeared in Volume 40 of Artificial Intelligence in 1989, reprinted with permission of North-Holland Publishing. It is a revised\u00a0\u2026",
      "link": "https://scholar.google.com/scholar?cluster=7168611871007736936&hl=en&oi=scholarr",
      "year": 1990,
      "cited_by": 7,
      "authors": ["Geoffrey E Hinton"],
      "description": null,
      "citation_histogram": [
        [1998, 1],
        [1999, 1],
        [2000, 1],
        [2001, 1],
        [2002, 2],
        [2003, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Considerations in dynamic time warping algorithms for discrete word recognition",
      "link": "https://scholar.google.com/scholar?cluster=335763798720782282&hl=en&oi=scholarr",
      "year": 1989,
      "cited_by": 7,
      "authors": ["A Waibel", "T Hanazawa", "G Hinton", "K Shikano", "KJ Lang"],
      "description": null,
      "citation_histogram": [
        [1991, 1],
        [1992, 2],
        [1993, 1],
        [1994, 1],
        [1995, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Sejnowski",
      "link": "https://scholar.google.com/scholar?cluster=14540938059920870640&hl=en&oi=scholarr",
      "year": 1985,
      "cited_by": 7,
      "authors": ["D Ackley", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [1985, 1],
        [1986, 1],
        [1987, 2],
        [1988, 1],
        [1989, 1],
        [1990, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Representation and control in vision",
      "link": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.435.3841&rep=rep1&type=pdf",
      "year": 1978,
      "cited_by": 7,
      "authors": [
        "Aaron Sloman",
        "David Owen",
        "Geoffrey Hinton",
        "Frank Birch",
        "Frank O'Gorman"
      ],
      "description": "Visior-work in A1 has made progress dth relatively small probierri, We are not aware of any system in hick^ many different kinds of knowledge co-operate, Often there is essentid_ly one kind of structure, e, g, a netmrk of lines or regions, and the problem is simply to segment it, and/or to label parts of it, Sometimes models of known objects are used to gulde the analysls and interpretation of an image, as in the work of Roberts (190j), but usually there are few such mdels, and there isn't a very deep hierarzhy of objects coqosed of objects composed of objects... By contrast, recent speech~ understairding systems, like SAY (Lesser 1977, Hayes-Roth 19779, deal with mre complex kinds of interactions between different sorts of knowledge, They are still not very impressive compared with people, but there are some solid achievements, Is the Lack of similar success in vlsion due to inherently mre difficult problems?Some vislon work has explored interactions between different kinds of ho~ dedge, Including iht Essex coding-sheet project (~ rady, Bornat 1976) based on the assumption thak provision for mitiple co-eaisting processes would make'the tasks mch easier, However, more concrete a~ d specific ideas are required for sensible control of a complex system, and a great deal. of domain-specific descriptive know-how has to be exp1. icitly provided for many different sLb-domains,",
      "citation_histogram": [
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Robust and Efficient Medical Imaging with Self-Supervision",
      "link": "https://arxiv.org/abs/2205.09723",
      "year": 2022,
      "cited_by": 6,
      "authors": [
        "Shekoofeh Azizi",
        "Laura Culp",
        "Jan Freyberg",
        "Basil Mustafa",
        "Sebastien Baur",
        "Simon Kornblith",
        "Ting Chen",
        "Patricia MacWilliams",
        "S Sara Mahdavi",
        "Ellery Wulczyn",
        "Boris Babenko",
        "Megan Wilson",
        "Aaron Loh",
        "Po-Hsuan Cameron Chen",
        "Yuan Liu",
        "Pinal Bavishi",
        "Scott Mayer McKinney",
        "Jim Winkens",
        "Abhijit Guha Roy",
        "Zach Beaver",
        "Fiona Ryan",
        "Justin Krogue",
        "Mozziyar Etemadi",
        "Umesh Telang",
        "Yun Liu",
        "Lily Peng",
        "Greg S Corrado",
        "Dale R Webster",
        "David Fleet",
        "Geoffrey Hinton",
        "Neil Houlsby",
        "Alan Karthikesalingam",
        "Mohammad Norouzi",
        "Vivek Natarajan"
      ],
      "description": "Recent progress in Medical Artificial Intelligence (AI) has delivered systems that can reach clinical expert level performance. However, such systems tend to demonstrate sub-optimal \"out-of-distribution\" performance when evaluated in clinical settings different from the training environment. A common mitigation strategy is to develop separate systems for each clinical setting using site-specific data [1]. However, this quickly becomes impractical as medical data is time-consuming to acquire and expensive to annotate [2]. Thus, the problem of \"data-efficient generalization\" presents an ongoing difficulty for Medical AI development. Although progress in representation learning shows promise, their benefits have not been rigorously studied, specifically for out-of-distribution settings. To meet these challenges, we present REMEDIS, a unified representation learning strategy to improve robustness and data-efficiency of medical imaging AI. REMEDIS uses a generic combination of large-scale supervised transfer learning with self-supervised learning and requires little task-specific customization. We study a diverse range of medical imaging tasks and simulate three realistic application scenarios using retrospective data. REMEDIS exhibits significantly improved in-distribution performance with up to 11.5% relative improvement in diagnostic accuracy over a strong supervised baseline. More importantly, our strategy leads to strong data-efficient generalization of medical imaging AI, matching strong supervised baselines using between 1% to 33% of retraining data across tasks. These results suggest that REMEDIS can significantly accelerate the life-cycle of\u00a0\u2026",
      "citation_histogram": [[2022, 6]],
      "detail_extracted": true
    },
    {
      "title": "The next generation of neural networks",
      "link": "https://dl.acm.org/doi/abs/10.1145/3397271.3402425",
      "year": 2020,
      "cited_by": 6,
      "authors": ["Geoffrey Hinton"],
      "description": "The most important unsolved problem with artificial neural networks is how to do unsupervised learning as effectively as the brain. There are currently two main approaches to unsupervised learning. In the first approach, exemplified by BERT and Variational Autoencoders, a deep neural network is used to reconstruct its input. This is problematic for images because the deepest layers of the network need to encode the fine details of the image. An alternative approach, introduced by Becker and Hinton in 1992, is to train two copies of a deep neural network to produce output vectors that have high mutual information when given two different crops of the same image as their inputs. This approach was designed to allow the representations to be untethered from irrelevant details of the input.",
      "citation_histogram": [
        [2011, 1],
        [2012, 1],
        [2013, 1],
        [2014, 1],
        [2015, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Grammar as a foreign language",
      "link": "https://scholar.google.com/scholar?cluster=3346294583078518221&hl=en&oi=scholarr",
      "year": 2015,
      "cited_by": 6,
      "authors": [
        "Petrov Koo",
        "S Petrov",
        "I Sutskever",
        "GE Hinton",
        "O Vinyals",
        "L Kaiser"
      ],
      "description": null,
      "citation_histogram": [
        [2016, 1],
        [2017, 1],
        [2018, 1],
        [2019, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "RMSprop Gradient Optimization",
      "link": "https://scholar.google.com/scholar?cluster=18366485362446569411&hl=en&oi=scholarr",
      "year": 2014,
      "cited_by": 6,
      "authors": ["G Hinton", "T Tieleman"],
      "description": null,
      "citation_histogram": [
        [2016, 1],
        [2017, 2],
        [2018, 2],
        [2019, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Non-linear dimensionality reduction",
      "link": "http://www.cs.utoronto.ca/~hinton/csc2535/notes/lec11new.pdf",
      "year": 2013,
      "cited_by": 6,
      "authors": ["Geoffrey Hinton"],
      "description": "lecture on isomap and sne Page 1 CSC 2535: 2011 Lecture 9 Non-linear dimensionality \nreduction Geoffrey Hinton Page 2 Dimensionality reduction: Some Assumptions \u2022 High-dimensional \ndata often lies on or near a much lower dimensional, curved manifold. \u2022 A good way to represent \ndata points is by their low-dimensional coordinates. \u2022 The low-dimensional representation of \nthe data should capture information about highdimensional pairwise distances. Page 3 The \nbasic idea of non-parameteric dimensionality reduction \u2022 Represent each data-point by a point \nin a lower dimensional space. \u2022 Choose the low-dimensional points so that they optimally \nrepresent some property of the data-points (eg the pairwise distances). \u2013 Many different \nproperties have been tried. \u2022 Do not insist on learning a parametric \u201cencoding\u201d function that \nmaps each individual data-point to its lowdimensional representative. \u2022 Do not insist on \u2026",
      "citation_histogram": [
        [2015, 1],
        [2016, 3],
        [2017, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Training products of experts by minimizing contrastive divergence",
      "link": "https://scholar.google.com/scholar?cluster=117942100792729556&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 6,
      "authors": ["F Wood", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [2012, 1],
        [2013, 3],
        [2014, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "CSC321: Introduction to Neural Networks and Machine Learning",
      "link": "https://www.cs.toronto.edu/~hinton/csc321/notes/lec24.pdf",
      "year": 2010,
      "cited_by": 6,
      "authors": ["Geoffrey Hinton", "Nitish Srivastava"],
      "description": "lecture 12 Page 1 CSC321: Introduction to Neural Networks and Machine Learning Lecture 24: \nNon-linear Support Vector Machines Geoffrey Hinton Page 2 The story so far \u2022 If we use a large \nset of non-adaptive features, we can often make the two classes linearly separable. \u2013 But if we \njust fit any old separating plane, it will not generalize well to new cases. \u2022 If we fit the separating \nplane that maximizes the margin (the minimum distance to any of the data points), we will get \nmuch better generalization. \u2013 Intuitively, we are squeezing out all the surplus capacity that \ncame from using a high-dimensional feature space. \u2022 This can be justified by a whole lot of \nclever mathematics which shows that \u2013 large margin separators have lower VC dimension. \n\u2013 models with lower VC dimension have a smaller gap between the training and test error \nrates. Page 3 Why do large margin separators have lower VC dimension? \u2022 Consider a \u2026",
      "citation_histogram": [
        [2017, 2],
        [2018, 2],
        [2019, 1],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "link": "https://scholar.google.com/scholar?cluster=4910275424540576286&hl=en&oi=scholarr",
      "year": 2010,
      "cited_by": 6,
      "authors": ["GW Taylor", "L Sigal", "L Fleet", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [2017, 1],
        [2018, 1],
        [2019, 3],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Deep belief networks for phone recognition,[in:] nips workshop on deep learning for speech recognition and related applications",
      "link": "https://scholar.google.com/scholar?cluster=4389744043720658307&hl=en&oi=scholarr",
      "year": 2009,
      "cited_by": 6,
      "authors": ["A Mohamed", "G Dahl", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2015, 2],
        [2016, 2],
        [2017, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Visualizing Data using t-SNE Laurens van der Maaten",
      "link": "https://scholar.google.com/scholar?cluster=1325168937120553033&hl=en&oi=scholarr",
      "year": 2008,
      "cited_by": 6,
      "authors": ["G Hinton"],
      "description": null,
      "citation_histogram": [
        [2020, 2],
        [2021, 2],
        [2022, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Improving a statistical language model by modulating the effects of context words.",
      "link": "http://www.cs.utoronto.ca/~hinton/absps/yuecheng.pdf",
      "year": 2008,
      "cited_by": 6,
      "authors": ["Zhang Yuecheng", "Andriy Mnih", "Geoffrey E Hinton"],
      "description": "We show how to improve a state-of-the-art neural network language model that converts the previous \u201ccontext\u201d words into feature vectors and combines these feature vectors to predict the feature vector of the next word. Significant improvements in predictive accuracy are achieved by using higher-level features to modulate the effects of the context words. This is more effective than using the higher-level features to directly predict the feature vector of the next word, but it is also possible to combine both methods.",
      "citation_histogram": [
        [2009, 1],
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning and relearning in boltzmann machines",
      "link": "https://books.google.com/books?hl=en&lr=&id=dUTC55eR4jQC&oi=fnd&pg=PA45&dq=info:QTYZ6nqD4fkJ:scholar.google.com&ots=DB1RrpEqQZ&sig=E7U1lM20ATsnRM4YK0Ld037SabE",
      "year": 2001,
      "cited_by": 6,
      "authors": ["T Sejnowski", "Geoffrey E Hinton"],
      "description": "Microstructure of Cognition, Volume 1: Foundations make use of the ability of a parallel network to perform cooperative searches for good solutions to problems. The basic idea is simple: The weights on the connections between processing units encode knowledge about how things normally fit together in some domain and the initial states or external inputs to a subset of the units encode some fragments of a structure within the domain. These fragments constitute a problem: What is the whole struc-ture from which they probably came? The network computes a\" good solution\u201d to the problem by repeatedly updating the states of units that represent possible other parts of the structure until the network eventually settles into a stable state of activity that represents the solution.",
      "citation_histogram": [
        [2015, 2],
        [2016, 2],
        [2017, 1],
        [2018, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Nonlinear dimensionality reduction using neural networks",
      "link": "http://www.cs.utoronto.ca/~rsalakhu/talks/abstract.ps.gz",
      "year": 2000,
      "cited_by": 6,
      "authors": ["Ruslan Salakhutdinov"],
      "description": "Discovering low-dimensional structure from high-dimensional observations has always been an important task in machine learning. The compact representation can be used for exploratory data analysis, preprocessing, and data visualization. There exist a variety of dimensionality reduction techniques, which can be broadly classified into: Linear methods (such as Principal Component Analysis (PCA)), Non-linear mappings (such as autoencoders), and Proximity based methods (such as Local Linear Embedding (LLE)[6]).Most of the existing algorithms suffer from various drawbacks. If the data lie on an embedded lowdimensional nonlinear manifold, then linear methods, even though computationally efficient, cannot recover this structure. Proximity based methods are more powerful, but their computational cost scales quadratically with the number of observations, so they generally cannot be applied to very large high-dimensional data sets. Nonlinear mapping algorithms, such as autoencoders, are generally painfully slow to train, and are prone to getting stuck in local minima.",
      "citation_histogram": [
        [2017, 1],
        [2018, 2],
        [2019, 3]
      ],
      "detail_extracted": true
    },
    {
      "title": "Scaling in a hierarchical unsupervised network",
      "link": "https://digital-library.theiet.org/content/conferences/10.1049/cp_19991077",
      "year": 1999,
      "cited_by": 6,
      "authors": [
        "Zoubin Ghahramani",
        "Alexander T Korenberg",
        "Geoffrey E Hinton"
      ],
      "description": "A persistent worry with computational models of unsupervised learning is that learning will become more difficult as the problem is scaled. We examine this issue in the context of a novel hierarchical, generative model that can be viewed as a nonlinear generalisation of factor analysis and can be implemented in a neural network. The model performs perceptual inference in a probabilistically consistent manner by using top-down, bottom-up and lateral connections. These connections can be learned using simple rules that require only locally available information. We first demonstrate that the model can extract a sparse, distributed, hierarchical representation of global disparity from simplified random-dot stereograms. We then investigate some of the scaling properties of the algorithm on this problem and find that: 1) increasing the image size leads to faster and more reliable learning; 2) increasing the depth of the\u00a0\u2026",
      "citation_histogram": [
        [2003, 1],
        [2004, 1],
        [2005, 2],
        [2006, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Neural networks for computer-human interfaces: Glove-TalkII",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.52.8310&rep=rep1&type=pdf",
      "year": 1996,
      "cited_by": 6,
      "authors": ["S Sidney Fels", "G Hinton"],
      "description": "Glove-TalkII is system which has an adaptive interface built with neural networks. Glove-TalkII maps hand gestures continuously to 10 control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an arti cial vocal tract that produces speech in real time giving an unlimited vocabulary and unlimited control of fundamental frequency and volume. The best version of Glove-TalkII uses several input devices (including a CyberGlove, a ContactGlove, a 3-space tracker, and a foot-pedal), a parallel formant speech synthesizer and 3 neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a xed, user-de ned relationship between hand-position and vowel sound and does not require any training examples from the user. Volume, fundamental frequency and stop consonants are produced with a xed mapping from the input devices. One subject has trained to speak intelligibly with Glove-TalkII. He speaks slowly with speech quality similar to a text-tospeech synthesizer but with far more natural-sounding pitch variations. Characteristics of the neural networks both enhance and detract from control intimacy.",
      "citation_histogram": [
        [1998, 3],
        [1999, 1],
        [2000, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "El atractivo del procesamiento distribuido en paralelo",
      "link": "https://scholar.google.com/scholar?cluster=16784222623154159050&hl=en&oi=scholarr",
      "year": 1992,
      "cited_by": 6,
      "authors": ["JL McClelland", "DE Rumelhart", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [1999, 1],
        [2000, 1],
        [2001, 2],
        [2002, 1],
        [2003, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "MASSIVELY PARALLEL ARCHITECTURES FOR Al: METL, THISTLE, AND BOLTZMANN MACHINES",
      "link": "https://www.aaai.org/Papers/AAAI/1983/AAAI83-087.pdf",
      "year": 1983,
      "cited_by": 6,
      "authors": [
        "Scott E Fahlman",
        "Geoffrey E Hinton",
        "Terrence J Sejnowski"
      ],
      "description": "It is becoming increasingly apparent that some aspects of intelligent behavior rcquirc enormous computational power and that some sort of massively parallel computing architecture is the most plausible way to deliver such power. Parallelism, rather than raw speed of the computing elements. seems to be the way that the brain gets such jobs done. But even if the need for massive parallelism is admitted, there is still the question of what kind of parallel architecture best fits the needs of various AI tasks.In this paper we will attempt to isolate a number of basic computational tasks that an intelligent system must perform. We will describe several families of massively parallel computing architectures, and we will see which of these computational tasks can be handled by each of these families. In particular, we will describe a new architecture, which we call the Boltzmann machine, whose abilities appear to include a number of tasks that are inefficient or impossible on the other architectures.",
      "citation_histogram": [
        [1987, 2],
        [1988, 1],
        [1989, 1],
        [1990, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Efficient Parametric Projection Pursuit Density Estimation",
      "link": null,
      "year": null,
      "cited_by": 6,
      "authors": ["Max Welling Richard S Zemel Geoffrey", "E Hinton"],
      "description": null,
      "citation_histogram": [
        [2003, 1],
        [2004, 1],
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Rmsprop gradient optimization (2014)",
      "link": "https://scholar.google.com/scholar?cluster=2841734105268223896&hl=en&oi=scholarr",
      "year": 2020,
      "cited_by": 5,
      "authors": ["T Tieleman", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2019, 2],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Cerberus: A multi-headed derenderer",
      "link": "https://arxiv.org/abs/1905.11940",
      "year": 2019,
      "cited_by": 5,
      "authors": ["Boyang Deng", "Simon Kornblith", "Geoffrey Hinton"],
      "description": "To generalize to novel visual scenes with new viewpoints and new object poses, a visual system needs representations of the shapes of the parts of an object that are invariant to changes in viewpoint or pose. 3D graphics representations disentangle visual factors such as viewpoints and lighting from object structure in a natural way. It is possible to learn to invert the process that converts 3D graphics representations into 2D images, provided the 3D graphics representations are available as labels. When only the unlabeled images are available, however, learning to derender is much harder. We consider a simple model which is just a set of free floating parts. Each part has its own relation to the camera and its own triangular mesh which can be deformed to model the shape of the part. At test time, a neural network looks at a single image and extracts the shapes of the parts and their relations to the camera. Each part can be viewed as one head of a multi-headed derenderer. During training, the extracted parts are used as input to a differentiable 3D renderer and the reconstruction error is backpropagated to train the neural net. We make the learning task easier by encouraging the deformations of the part meshes to be invariant to changes in viewpoint and invariant to the changes in the relative positions of the parts that occur when the pose of an articulated body changes. Cerberus, our multi-headed derenderer, outperforms previous methods for extracting 3D parts from single images without part annotations, and it does quite well at extracting natural parts of human figures.",
      "citation_histogram": [
        [2019, 1],
        [2020, 2],
        [2021, 1],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Speech recognition with deep recurrent neural networks. IEEE Int. Conf. on Acoustics, Speech and Signal Processing",
      "link": "https://scholar.google.com/scholar?cluster=8072991689003471533&hl=en&oi=scholarr",
      "year": 2013,
      "cited_by": 5,
      "authors": ["A Graves", "AR Mohamed", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2017, 4],
        [2018, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Speech Recognition with Deep Recurrent Neural Networks. ICASSP 2013",
      "link": "https://scholar.google.com/scholar?cluster=5720467127100246950&hl=en&oi=scholarr",
      "year": 2013,
      "cited_by": 5,
      "authors": ["A Graves", "A-r Mohamed", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2017, 1],
        [2018, 3],
        [2019, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "An efficient learning procedure for deep machines",
      "link": "https://scholar.google.com/scholar?cluster=5301807855891644618&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 5,
      "authors": ["R Salakhutdinov", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2015, 1],
        [2016, 1],
        [2017, 1],
        [2018, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "International Conference on Artificial Intelligence and Statistics",
      "link": "https://scholar.google.com/scholar?cluster=9353330998867079693&hl=en&oi=scholarr",
      "year": 2009,
      "cited_by": 5,
      "authors": ["R Salakhutdinov", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2013, 1],
        [2014, 2],
        [2015, 1],
        [2016, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Recent developments in deep learning",
      "link": "https://scholar.google.com/scholar?cluster=1137371424498935922&hl=en&oi=scholarr",
      "year": 2009,
      "cited_by": 5,
      "authors": ["G Hinton"],
      "description": null,
      "citation_histogram": [
        [2013, 1],
        [2014, 3],
        [2015, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Greedy layer-wise algorithm",
      "link": "https://scholar.google.com/scholar?cluster=8263914351253782783&hl=en&oi=scholarr",
      "year": 2009,
      "cited_by": 5,
      "authors": ["G Hinton"],
      "description": null,
      "citation_histogram": [
        [2014, 3],
        [2015, 1],
        [2016, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Visualizing High-Dimensional Data Using t-629 SNE",
      "link": "https://scholar.google.com/scholar?cluster=2979844096647099108&hl=en&oi=scholarr",
      "year": 2008,
      "cited_by": 5,
      "authors": ["L van der Maaten", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2018, 1],
        [2019, 2],
        [2020, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Cascaded redundancy reduction",
      "link": "https://iopscience.iop.org/article/10.1088/0954-898X/9/1/004/meta",
      "year": 1998,
      "cited_by": 5,
      "authors": ["Virginia R De Sa", "Geoffrey E Hinton"],
      "description": "We describe a method for incrementally constructing a hierarchical generative model of an ensemble of binary data vectors. The model is composed of stochastic, binary, logistic units. Hidden units are added to the model one at a time with the goal of minimizing the information required to describe the data vectors using the model. In addition to the top-down generative weights that define the model, there are bottom-up recognition weights that determine the binary states of the hidden units given a data vector. Even though the stochastic generative model can produce each data vector in many ways, the recognition model is forced to pick just one of these ways. The recognition model therefore underestimates the ability of the generative model to predict the data, but this underestimation greatly simplifies the process of searching for the generative and recognition weights of a new hidden unit.",
      "citation_histogram": [
        [1997, 1],
        [1998, 2],
        [1999, 1],
        [2000, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "The EM algorithm for mixtures of factor analyzers Univ. Toronto, Toronto, ON",
      "link": "https://scholar.google.com/scholar?cluster=10290049285718606583&hl=en&oi=scholarr",
      "year": 1997,
      "cited_by": 5,
      "authors": ["Z Ghahramani", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [2006, 1],
        [2007, 1],
        [2008, 1],
        [2009, 1],
        [2010, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Using spatial coherence as an internal teacher for a neural network",
      "link": "https://scholar.google.com/scholar?cluster=6902037692276296075&hl=en&oi=scholarr",
      "year": 1995,
      "cited_by": 5,
      "authors": ["S Becker", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [1990, 1],
        [1991, 1],
        [1992, 1],
        [1993, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Morgan-Kaufmann",
      "link": "https://scholar.google.com/scholar?cluster=18341773240164091550&hl=en&oi=scholarr",
      "year": 1990,
      "cited_by": 5,
      "authors": ["DS Touretzky", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [1990, 1],
        [1991, 1],
        [1992, 1],
        [1993, 1],
        [1994, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Lang K",
      "link": "https://scholar.google.com/scholar?cluster=9915952204046683358&hl=en&oi=scholarr",
      "year": 1987,
      "cited_by": 5,
      "authors": ["A Waibel", "T Hanazawa", "G Hinton", "K Shikano"],
      "description": null,
      "citation_histogram": [
        [1988, 1],
        [1989, 1],
        [1990, 2],
        [1991, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "The horizontal\u2014vertical delusion",
      "link": "https://journals.sagepub.com/doi/abs/10.1068/p160677",
      "year": 1987,
      "cited_by": 5,
      "authors": ["Geoffrey E Hinton"],
      "description": "Most people can correctly apply the concepts of horizontal and vertical in describing objects, but a simple demonstration shows that they are confused about how these concepts work. The nature of the confusion and its possible causes are briefly discussed.",
      "citation_histogram": [
        [2002, 1],
        [2003, 1],
        [2004, 1],
        [2005, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "rahman Mohamed A, Jaitly N, Senior A, Vanhoucke V, Nguyen P, Sainath T, Kingsbury B (2012) Deep neural networks for acoustic modeling in speech recognition",
      "link": "https://scholar.google.com/scholar?cluster=8168068250310609003&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 5,
      "authors": ["G Hinton", "L Deng", "D Yu", "G Dahl"],
      "description": null,
      "citation_histogram": [
        [2018, 1],
        [2019, 1],
        [2020, 1],
        [2021, 1],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning internal representations by error propagation, ch. 8",
      "link": "https://scholar.google.com/scholar?cluster=11625192235999440336&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 5,
      "authors": ["DE Rumelhart", "GE Hinton", "RJ Williams"],
      "description": null,
      "citation_histogram": [
        [2006, 2],
        [2007, 1],
        [2008, 1],
        [2009, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "R evow, M.(1992). Adaptive elastic models for hand-printed character recognition",
      "link": "https://scholar.google.com/scholar?cluster=12083771296959962725&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 5,
      "authors": ["G Hinton", "C Williams"],
      "description": null,
      "citation_histogram": [
        [1992, 2],
        [1993, 1],
        [1994, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Analog bits: Generating discrete data using diffusion models with self-conditioning",
      "link": "https://arxiv.org/abs/2208.04202",
      "year": 2022,
      "cited_by": 4,
      "authors": ["Ting Chen", "Ruixiang Zhang", "Geoffrey Hinton"],
      "description": "We present Bit Diffusion: a simple and generic approach for generating discrete data with continuous diffusion models. The main idea behind our approach is to first represent the discrete data as binary bits, and then train a continuous diffusion model to model these bits as real numbers which we call analog bits. To generate samples, the model first generates the analog bits, which are then thresholded to obtain the bits that represent the discrete variables. We further propose two simple techniques, namely Self-Conditioning and Asymmetric Time Intervals, which lead to a significant improvement in sample quality. Despite its simplicity, the proposed approach can achieve strong performance in both discrete image generation and image captioning tasks. For discrete image generation, we significantly improve previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens) and ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the best autoregressive model in both sample quality (measured by FID) and efficiency. For image captioning on MS-COCO dataset, our approach achieves competitive results compared to autoregressive models.",
      "citation_histogram": [[2022, 4]],
      "detail_extracted": true
    },
    {
      "title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv: Neural and Evolutionary Computing",
      "link": "https://scholar.google.com/scholar?cluster=1628366915797615511&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 4,
      "authors": [
        "Geoffrey E Hinton",
        "Nitish Srivastava",
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Ruslan Salakhutdinov"
      ],
      "description": null,
      "citation_histogram": [
        [2018, 1],
        [2019, 1],
        [2020, 1],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Caching and replay of place sequences in a Temporal Restricted Boltzmann Machine model of the hippocampus",
      "link": "https://www.science.mcmaster.ca/pnb/department/becker/papers/BeckerHintonSFNAbstract2007.pdf",
      "year": 2007,
      "cited_by": 4,
      "authors": ["Suzanna Becker", "Geoffrey E Hinton"],
      "description": "The hippocampus is thought to be crticial for the rapid formation and cued recall of complex event memories. For example, a set of hippocampal place cells that fires in a particular sequence during exploration may fire in the same sequence during sleep [1]. We propose a novel model of the hippocampal memory system that reconciles a wide range of neurobiological data. Here we address the question of how the hippocampus encodes sequences rapidly, and what is the function of sequence replay.Our model draws upon two recent developments of the Restricted Boltzmann Machine (RBM): 1) a hierarchy of sequentially trained RBM\u2019s [2], and 2) the extension to sequential inputs employed in the Temporal RBM [3]. The top two layers of the model, representing the dentate gyrus (DG) and CA fields, are connected via undirected links to form an autoassociator, allowing the model to generate memories of coherent events, rather than generating top-level unit states independently. The CA region also has directed connections from previous CA states, representing the CA3 recurrent connections. Thus the probability distribution learned over the visible and hidden units is conditioned on earlier states of the autoassociator. The model is trained by contrastive Hebbian learning, with data-driven and generative phases providing the statistics for the positive and negative Hebbian updates respectively. This is broadly consistent with Hasselmo\u2019s proposal that the hippocampus oscillates between encoding and recall modes within each theta cycle [4]. When trained on traversals along a linear track and broadly place-tuned inputs (Fig 1a), the hippocampal\u00a0\u2026",
      "citation_histogram": [
        [2010, 1],
        [2011, 1],
        [2012, 1],
        [2013, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Embedding via clustering: Using spectral information to guide dimensionality reduction",
      "link": "https://ieeexplore.ieee.org/abstract/document/1556439/",
      "year": 2005,
      "cited_by": 4,
      "authors": ["Roland Memisevic", "Geoffrey Hinton"],
      "description": "We describe an approach to improve iterative dimensionality reduction methods by using information contained in the leading eigenvectors of a data affinity matrix. Using an insight from the area of spectral clustering, we suggest modifying the gradient of an iterative method, so that latent space elements belonging to the same cluster are encouraged to move in similar directions during optimization. We also describe way to achieve this without actually having to explicitly perform an eigendecomposition. Preliminary experiments show that our approach makes it possible to speed up iterative methods and helps them to find better local minima of their objective function.",
      "citation_histogram": [
        [2005, 1],
        [2006, 1],
        [2007, 1],
        [2008, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning distributed representations of relational data using linear relational embedding",
      "link": "https://link.springer.com/chapter/10.1007/978-1-4471-0219-9_12",
      "year": 2002,
      "cited_by": 4,
      "authors": ["Alberto Paccanaro", "Geoffrey E Hinton"],
      "description": " Linear Relational Embedding (LRE) is a new method of learning a distributed representation of concepts from data consisting of binary relations between concepts. The final goal of LRE is to be able to generalize, i.e. to infer new relations among the concepts. The version presented here is capable of handling incomplete information and multiple correct answers. We present results on two simple domains, that show an excellent generalization performance.",
      "citation_histogram": [
        [2001, 1],
        [2002, 1],
        [2003, 1],
        [2004, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "INCREMENTAL, SPARSE, AND OTHER VARIANTS",
      "link": "https://books.google.com/books?hl=en&lr=&id=7f61BBKdJ4EC&oi=fnd&pg=PA355&dq=info:S6-0tquXQhgJ:scholar.google.com&ots=PYxCQ0NHno&sig=8l4CiYIMgUxI1wCEwIZpxJM-Plo",
      "year": 1998,
      "cited_by": 4,
      "authors": ["GEOFFREY E HINTON"],
      "description": "The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algo-rithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.",
      "citation_histogram": [
        [2009, 1],
        [2010, 2],
        [2011, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Switching state-space models (Technical Report)",
      "link": "https://scholar.google.com/scholar?cluster=10018232278140663558&hl=en&oi=scholarr",
      "year": 1998,
      "cited_by": 4,
      "authors": ["Z Ghahramani", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [2009, 1],
        [2010, 1],
        [2011, 1],
        [2012, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "A Time-Delay Neural Network Architecture for Isolated Word Recognition",
      "link": "https://ci.nii.ac.jp/naid/10020899467/",
      "year": 1992,
      "cited_by": 4,
      "authors": ["J Lang"],
      "description": "CiNii \u8ad6\u6587 - A Time-Delay Neural Network Architecture for Isolated Word Recognition CiNii \n\u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \n\u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \n\u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\n\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 [2022\u5e741\u6708\u7de0\u5207]CiNii Articles\u3078\u306eCiNii Research\u3078\u306e\u7d71\u5408\u306b\u4f34\u3046\u6a5f\u95a2\u8a8d\u8a3c\u306e\n\u79fb\u884c\u78ba\u8a8d\u306b\u3064\u3044\u3066 [1/6\u66f4\u65b0]2022\u5e744\u67081\u65e5\u304b\u3089\u306eCiNii Articles\u306eCiNii Research\u3078\u306e\u7d71\u5408\u306b\u3064\u3044\u3066 \nA Time-Delay Neural Network Architecture for Isolated Word Recognition LANG J. \u88ab\u5f15\u7528\u6587\u732e\n: 1\u4ef6 \u8457\u8005 LANG J. \u53ce\u9332\u520a\u884c\u7269 Artificial Neural Networks, Paradigms, Applications and \nHardware Artificial Neural Networks, Paradigms, Applications and Hardware, 1992 \u88ab\u5f15\u7528\u6587\u732e\n: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u8996\u899a\u30d1\u30bf\u30fc\u30f3\u8a8d\u77e5\u65b9\u7a0b\u5f0f\u3092\u5fdc\u7528\u3057\u305f\u9023\u7d9a\u97f3\u58f0(\u5358\u8a9e)\u8a8d\u8b58 \u5317\u6dfb \u5fb9\u90ce , \u821f\u68ee \u4fe1 \u2026",
      "citation_histogram": [
        [1994, 1],
        [1995, 1],
        [1996, 1],
        [1997, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Generative back-propagation",
      "link": "https://scholar.google.com/scholar?cluster=4715992374012082077&hl=en&oi=scholarr",
      "year": 1988,
      "cited_by": 4,
      "authors": ["GE Hinton"],
      "description": null,
      "citation_histogram": [
        [2012, 1],
        [2013, 1],
        [2014, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parallel distributed processing. Volume 1: Foundations",
      "link": "https://scholar.google.com/scholar?cluster=5926038363700262643&hl=en&oi=scholarr",
      "year": 1986,
      "cited_by": 4,
      "authors": ["GE Hinton", "JL McClelland", "DE Rumelhart"],
      "description": null,
      "citation_histogram": [
        [2001, 1],
        [2002, 2],
        [2003, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parallel computation and the mass-spring model of motor control",
      "link": "https://scholar.google.com/scholar?cluster=15888384602536714526&hl=en&oi=scholarr",
      "year": 1984,
      "cited_by": 4,
      "authors": ["Geoffrey E Hinton", "Paul Smolensky"],
      "description": null,
      "citation_histogram": [
        [1986, 1],
        [1987, 1],
        [1988, 1],
        [1989, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parallel stochastic search in early vision",
      "link": "https://scholar.google.com/scholar?cluster=7208589719220965902&hl=en&oi=scholarr",
      "year": 1984,
      "cited_by": 4,
      "authors": ["Terrence J Sejnowski", "Geoffrey E Hinton"],
      "description": null,
      "citation_histogram": [
        [1985, 1],
        [1986, 2],
        [1987, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Respectively Reconsidered",
      "link": "https://scholar.google.com/scholar?cluster=16799619679390866905&hl=en&oi=scholarr",
      "year": 1978,
      "cited_by": 4,
      "authors": ["G Hinton"],
      "description": null,
      "citation_histogram": [
        [1982, 1],
        [1983, 1],
        [1984, 1],
        [1985, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "A unified sequence interface for vision tasks",
      "link": "https://arxiv.org/abs/2206.07669",
      "year": 2022,
      "cited_by": 3,
      "authors": [
        "Ting Chen",
        "Saurabh Saxena",
        "Lala Li",
        "Tsung-Yi Lin",
        "David J Fleet",
        "Geoffrey Hinton"
      ],
      "description": "While language tasks are naturally expressed in a single, unified, modeling framework, i.e., generating sequences of tokens, this has not been the case in computer vision. As a result, there is a proliferation of distinct architectures and loss functions for different vision tasks. In this work we show that a diverse set of \"core\" computer vision tasks can also be unified if formulated in terms of a shared pixel-to-sequence interface. We focus on four tasks, namely, object detection, instance segmentation, keypoint detection, and image captioning, all with diverse types of outputs, e.g., bounding boxes or dense masks. Despite that, by formulating the output of each task as a sequence of discrete tokens with a unified interface, we show that one can train a neural network with a single model architecture and loss function on all these tasks, with no task-specific customization. To solve a specific task, we use a short prompt as task description, and the sequence output adapts to the prompt so it can produce task-specific output. We show that such a model can achieve competitive performance compared to well-established task-specific models.",
      "citation_histogram": [[2022, 3]],
      "detail_extracted": true
    },
    {
      "title": "Training distilled machine learning models",
      "link": "https://patents.google.com/patent/US10650328B2/en",
      "year": 2020,
      "cited_by": 3,
      "authors": ["Oriol Vinyals", "Jeffrey Adgate Dean", "Geoffrey E Hinton"],
      "description": "Methods, systems, and apparatus, including computer programs encoded on computer storage media, for training a distilled machine learning model. One of the methods includes training a cumbersome machine learning model, wherein the cumbersome machine learning model is configured to receive an input and generate a respective score for each of a plurality of classes; and training a distilled machine learning model on a plurality of training inputs, wherein the distilled machine learning model is also configured to receive inputs and generate scores for the plurality of classes, comprising: processing each training input using the cumbersome machine learning model to generate a cumbersome target soft output for the training input; and training the distilled machine learning model to, for each of the training inputs, generate a soft output that matches the cumbersome target soft output for the training input.",
      "citation_histogram": [
        [2020, 1],
        [2021, 1],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "System and method for parallelizing convolutional neural networks",
      "link": "https://patents.google.com/patent/US10635966B2/en",
      "year": 2020,
      "cited_by": 3,
      "authors": [
        "Alexander Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey E Hinton"
      ],
      "description": "A parallel convolutional neural network is provided. The CNN is implemented by a plurality of convolutional neural networks each on a respective processing node. Each CNN has a plurality of layers. A subset of the layers are interconnected between processing nodes such that activations are fed forward across nodes. The remaining subset is not so interconnected.",
      "citation_histogram": [
        [2020, 1],
        [2021, 1],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "The deep learning revolution",
      "link": "https://rse.org.uk/wp-content/uploads/2021/08/Hintons-Presentation_20190718.pdf",
      "year": 2019,
      "cited_by": 3,
      "authors": ["Geoffrey Hinton", "Y LeCun"],
      "description": "UCL Tutorial July 17 2009 Page 1 The Deep Learning Revolution Geoffrey Hinton Google \nBrain Team & Vector Institute Page 2 Two paradigms for Artificial Intelligence The logic-inspired \napproach The essence of intelligence is using symbolic rules to manipulate symbolic \nexpressions. We should focus on reasoning. The biologically-inspired approach The essence \nof intelligence is learning the strengths of the connections in a neural network. We should focus \non learning and perception. Page 3 Two views of internal representations \u2022 Internal \nrepresentations are symbolic expressions. \u2013 A programmer can give them to a computer using \nan unambiguous language. \u2013 New representations can be derived by applying rules to existing \nrepresentations. \u2022 Internal representations are nothing like language. \u2013 They are large vectors \nof neural activity. \u2013 They have direct causal effects on other vectors of neural activity. \u2013 These \u2026",
      "citation_histogram": [
        [2020, 1],
        [2021, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Guest editorial: Deep learning",
      "link": "https://nyuscholars.nyu.edu/en/publications/guest-editorial-deep-learning",
      "year": 2015,
      "cited_by": 3,
      "authors": [
        "Ranzato Marc\u2019Aurelio\u2019A",
        "Geoffrey Hinton",
        "Yann LeCun"
      ],
      "description": "Guest Editorial: Deep Learning \u2014 NYU Scholars Skip to main navigation Skip to search \nSkip to main content NYU Scholars Home NYU Scholars Logo Help & FAQ Home Profiles \nResearch Units Research output Search by expertise, name or affiliation Guest Editorial: \nDeep Learning Marc\u2019Aurelio \u2019A Ranzato, Geoffrey Hinton, Yann LeCun Computer Science \nResearch output: Contribution to journal \u203a Editorial \u203a peer-review Overview Fingerprint \nOriginal language English (US) Pages (from-to) 1-2 Number of pages 2 Journal \nInternational Journal of Computer Vision Volume 113 Issue number 1 DOIs https://doi.org/10.1007/s11263-015-0813-1 \nState Published - May 1 2015 ASJC Scopus subject areas Software Computer Vision and \nPattern Recognition Artificial Intelligence Access to Document 10.1007/s11263-015-0813-1 \nOther files and links Link to publication in Scopus Link to citation list in Scopus Cite this APA \u2026",
      "citation_histogram": [
        [2019, 2],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Fast inference and learning for modeling documents with a deep Boltzmann machine",
      "link": "https://openreview.net/forum?id=GtacG-v9TXtUf",
      "year": 2013,
      "cited_by": 3,
      "authors": [
        "Nitish Srivastava",
        "Ruslan Salakhutdinov",
        "Geoffrey Hinton"
      ],
      "description": "We introduce a type of Deep Boltzmann Machine (DBM) that is suitable for extracting distributed semantic representations from a large unstructured collection of documents. We propose an approximate inference method that interacts with learning in a way that makes it possible to train the DBM more efficiently than previously proposed methods. Even though the model has two hidden layers, it can be trained just as efficiently as a standard Restricted Boltzmann Machine. Our experiments show that the model assigns better log probability to unseen data than the Replicated Softmax model. Features extracted from our model outperform LDA, Replicated Softmax, and DocNADE models on document retrieval and document classification tasks.",
      "citation_histogram": [
        [2018, 1],
        [2019, 1],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Coursera: Neural networks for machine learning-Lecture 6.5: RMSprop",
      "link": "https://scholar.google.com/scholar?cluster=14693518667656420375&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 3,
      "authors": ["T Tieleman", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2020, 1],
        [2021, 1],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Rmsprop: Divide the gradient by a running average of its recent magnitude. Coursera",
      "link": "https://scholar.google.com/scholar?cluster=17670751423781933955&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 3,
      "authors": ["T Tieleman", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2019, 1],
        [2020, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "//Proc. Advances in Neural Inform.",
      "link": "https://scholar.google.com/scholar?cluster=5614936233380913052&hl=en&oi=scholarr",
      "year": 2012,
      "cited_by": 3,
      "authors": ["A Krizhevsky", "I Sutskever", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2018, 1],
        [2019, 1],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "15 Learning to Use Spike Timing in a Restricted Boltzmann Machine",
      "link": "ftp://nozdr.ru/biblio/kolxoz/B/Rao,%20Olshausen,%20Lewicki.%20(eds.)%20Probabilistic%20models%20of%20the%20brain..%20Perception%20and%20neural%20function%20(MIT,%202002)(ISBN%200262182246)(335s)_B_.pdf#page=296",
      "year": 2002,
      "cited_by": 3,
      "authors": ["Geoffrey E Hinton", "Andrew D Brown"],
      "description": "A perceived object is represented in the brain by the activities of many neurons, but there is no general consensus on how the activities of individual neurons combine to represent the multiple properties of an object. We start by focussing on the case of a single object that has multiple instantiation parameters such as position, velocity, size and orientation. We assume that each neuron has an ideal stimulus in the space of instantiation parameters and that its activation rate or probability of activation falls off monotonically in all directions as the actual stimulus departs from this ideal. The semantic problem is to define exactly what instantiation parameters are being represented when the activities of many such neurons are specified.Hinton, Rumelhart, and McClelland (1986) consider binary neurons with receptive fields that are convex in instantiation space. They assume that when an object is present it activates all of the neurons in whose receptive fields its instantiation parameters lie. Consequently, if it is known that only one object is present, the parameter values of the object must lie within the feasible region formed by the intersection of the receptive fields of the active neurons. This will be called a conjunctive distributed representation. Assuming that each receptive field occupies only a small fraction of the whole space, an interesting property of this type of \u201ccoarse coding\u201d is that the bigger the receptive fields, the more accurate the representation. However, large receptive fields lead to a loss of resolution when several objects are present simultaneously.",
      "citation_histogram": [
        [2002, 1],
        [2003, 1],
        [2004, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Pattern classification using a mixture of factor analyzers",
      "link": "https://ieeexplore.ieee.org/abstract/document/788172/",
      "year": 1999,
      "cited_by": 3,
      "authors": [
        "Naonori Ueda",
        "Ryohei Nakano",
        "Zoubin Ghahramani",
        "Geoffrey Hinton"
      ],
      "description": "This paper describes a practical application of a mixture of factor analyzers (MFA) to pattern recognition. The MFA extracts locally linear manifolds underlying given high dimensional data. In this respect, the NFA-based approach is similar to the conventional subspace methods that approximate the data space with low dimensional linear subspaces. However, the MFA-based classifier, unlike the conventional subspace methods, can perform classification based on the Bayes decision rule due to its probabilistic formulation. Experimental results show that the MFA-based approach can obtain better classification performance than the conventional subspace methods.",
      "citation_histogram": [
        [2006, 1],
        [2007, 1],
        [2008, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Cellular Architecture of a Neuronal Network",
      "link": "https://scholar.google.com/scholar?cluster=656388708691946753&hl=en&oi=scholarr",
      "year": 1992,
      "cited_by": 3,
      "authors": ["GE Hinton"],
      "description": null,
      "citation_histogram": [
        [1999, 1],
        [2000, 1],
        [2001, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "The unity of consciousness: A connectionist account",
      "link": "https://books.google.com/books?hl=en&lr=&id=f00kBiAU988C&oi=fnd&pg=PA245&dq=info:XdRSEzG8xdEJ:scholar.google.com&ots=460L3zoWTD&sig=UDMd-ufFbTALndXfEKEGsD2WS3g",
      "year": 1991,
      "cited_by": 3,
      "authors": ["Geoffrey E Hinton"],
      "description": "The main thesis of this chapter is that one of the most puzzling aspects of consciousness can be explained by considering an important limitation of con~ nectionist networks. The puzzle concerns the number of things of which we can be simultaneously aware. In some sense, we are only aware of a single Gestalt at a time. In visual perception, if we attend to one object, we cannot simultaneously attend to other objects. In language understanding, when we comprehend one proposition, we do not simultaneously comprehend other propositions. When solving a complex problem, if we focus on one subtask we cannot simultaneously focus on other subtasks. The puzzle arises because our awareness of a Gestalt presumably involves information about the relationships among the constituents of the Gestalt, so despite the phenomenological evidence for the unity and exclusiveness of consciousness, reason tells us that we must be actively representing several of the constituents of our current Gestalt. So why are we not conscious of those constituents? This puzzle is particularly acute for spotlight theories of attention, because, if an object is under the spotlight, then so are its parts.The obvious way out of this dilemma is to mistrust or reinterpret the phenomenological evidence and to abandon the idea that we can only attend to (or be aware of) one thing at a time. I propose a quite different solution that respects the phenomenological evidence: The way in which the constituents are represented is very di\ufb02\u2019erent from the way in which the whole Gestalt is represented. So, if awareness of something corresponds to a particular mode of representation of that\u00a0\u2026",
      "citation_histogram": [
        [1995, 1],
        [1996, 1],
        [1997, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Phoneme recognition: neural networks vs hidden markov models",
      "link": "https://isl.anthropomatik.kit.edu/pdf/Hanazawa1988.pdf",
      "year": 1988,
      "cited_by": 3,
      "authors": ["G Hinton", "IC Shikano"],
      "description": "In recent years, the advent of new learning procedures and the availability of high speed parallel supercomputers have given rise to a renewed interest in connectionist models of intelligence [l]. These models are particularly interesting for cognitive tasks that require massive constraint satisfaction, ie, the parallel evaluation of many clues and r interpretation in the light of numerous interrelated conause of the far-reaching implications to speech recognietworks have recently been compared with other pattern recognition classifiers [2] and explored as alternative to other speech recognition techniques (see [2, 3] for review). Some of these studies report very incouraging performance results [4], but others show neural nets as underperforming existing techniques. One possible explanamixed comparative performance results so far might be given by the inability of many neural network architectures to deal properly with the dynamic nature of speech. Various solutions to this problem, however, are now beginning to emerge [5, 6, 7, 8] and continued work in this area is likely to lead to more powerful speech recognition systems in the future.To capture the dynamic nature of speech a network must be able to 1.) represent temporal relationships between acoustic events, while at the same time 2) provide for rnvanance under translatron in time. The specific movement of a formant in time, for example, is an important cue to determining the identity of a voiced stop, but it is irrelevant whether the same set of events occurs a little sooner or later in the course of time. Without translation invariance a neural net requires precise segmentation, to allgn the input pattern\u00a0\u2026",
      "citation_histogram": [
        [1997, 1],
        [1998, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "the PDP Research Group: Parallel Distributed Processing, 1",
      "link": "https://scholar.google.com/scholar?cluster=6184146275756846121&hl=en&oi=scholarr",
      "year": 1986,
      "cited_by": 3,
      "authors": ["DE Runmelhart", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [1988, 1],
        [1989, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, Vol 4, No. 2, 2012",
      "link": "https://scholar.google.com/scholar?cluster=2432871580390595932&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 3,
      "authors": ["T Tieleman", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2018, 1],
        [2019, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "8: McClel1and, JL (I986). A general framework for parallel distributed processing",
      "link": "https://scholar.google.com/scholar?cluster=11420754417789615480&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 3,
      "authors": ["DE Rumelhart", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [1993, 1],
        [1994, 1],
        [1995, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "DELVE team members (1995). Assessing learning procedures using DELVE",
      "link": "https://scholar.google.com/scholar?cluster=3381868215857759711&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 3,
      "authors": ["G Hinton", "R Neal", "R Tibshirani"],
      "description": null,
      "citation_histogram": [
        [1998, 1],
        [1999, 2]
      ],
      "detail_extracted": true
    },
    {
      "title": "Systems and methods for contrastive learning of visual representations",
      "link": "https://patents.google.com/patent/US11386302B2/en",
      "year": 2022,
      "cited_by": 2,
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Everest Hinton",
        "Kevin Jordan Swersky"
      ],
      "description": "Systems, methods, and computer program products for performing semi-supervised contrastive learning of visual representations are provided. For example, the present disclosure provides systems and methods that leverage particular data augmentation schemes and a learnable nonlinear transformation between the representation and the contrastive loss to provide improved visual representations. Further, the present disclosure also provides improvements for semi-supervised contrastive learning. For example, computer-implemented method may include performing semi-supervised contrastive learning based on a set of one or more unlabeled training data, generating an image classification model based on a portion of a plurality of layers in a projection head neural network used in performing the contrastive learning, performing fine-tuning of the image classification model based on a set of one or more\u00a0\u2026",
      "citation_histogram": [[2022, 2]],
      "detail_extracted": true
    },
    {
      "title": "Aprendizagem Profunda",
      "link": "https://scholar.google.com/scholar?cluster=15102605802282006337&hl=en&oi=scholarr",
      "year": 2015,
      "cited_by": 2,
      "authors": ["Y Lecun", "Y Bengio", "G Hinton"],
      "description": null,
      "citation_histogram": [
        [2021, 1],
        [2022, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "A comparison of classification methods for longitudinal fmri studies",
      "link": "https://www.infona.pl/resource/bwmeta1.element.elsevier-daa989a3-10fc-3fcb-8d1e-6be0d018d57e",
      "year": 2009,
      "cited_by": 2,
      "authors": [
        "T Schmah",
        "G Yourganov",
        "RS Zemel",
        "GE Hinton",
        "SL Small",
        "S Strother"
      ],
      "description": "A Comparison of Classification Methods for Longitudinal fMRI Studies \u00d7 Close The Infona \nportal uses cookies, ie strings of text saved by a browser on the user's device. The portal can \naccess those files and use them to remember the user's data, such as their chosen settings \n(screen view, interface language, etc.), or their login data. By using the Infona portal the user \naccepts automatic saving and using this information for portal operation purposes. More \ninformation on the subject can be found in the Privacy Policy and Terms of Service. By closing \nthis window the user confirms that they have read the information on cookie usage, and they \naccept the privacy policy and the way cookies are used by the portal. You can change the \ncookie settings in your browser. I accept Polski English Login or register account remember me \nPassword recovery INFONA - science communication portal INFONA Search advanced search \u2026",
      "citation_histogram": [
        [2008, 1],
        [2009, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Modelling the statistics of natural images with topographic product of student-t models",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.113.9360&rep=rep1&type=pdf",
      "year": 2006,
      "cited_by": 2,
      "authors": ["Simon Osindero", "Max Welling", "Geoffrey E Hinton"],
      "description": "We present an energy-based model that uses a product of generalised Student-t distributions to capture the statistical structure in datasets. This model is inspired by and particularly applicable to \u201cnatural\u201d datasets such as images. We begin by providing the mathematical framework, where we discuss complete as well as undercomplete 1",
      "citation_histogram": [
        [2006, 1],
        [2007, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Using mixtures of factor analyzers for segmentation and pose estimation",
      "link": "https://www.cs.utoronto.ca/~hinton/absps/faseg.pdf",
      "year": 1998,
      "cited_by": 2,
      "authors": ["Geoffrey E Hinton", "Michael Revow"],
      "description": "To read a hand-written digit string, it is helpful to segment the image into separate digits. Bottom-up segmentation heuristics often fail when neighboring digits overlap substantially. We describe a system that has a stochastic generative model of each digit class and we show that this is the only knowledge required for segmentation. The system uses Gibbs sampling to construct a perceptual interpretation of a digit string and segmentation arises naturally from the\\explaining away\" effects that occur during Bayesian inference. By using conditional mixtures of factor analyzers, it is possible to extract an explicit, compact representation of the instantiation parameters that describe the pose of each digit. These instantiation parameters can then be used as the inputs to a higher level system that models the relationships between digits. The same technique could be used to model individual digits as redundancies between the instantiation parameters of their parts.",
      "citation_histogram": [
        [2003, 1],
        [2004, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Using neural networks to monitor for rare failures",
      "link": "https://www.cs.toronto.edu/~hinton/absps/monitor.pdf",
      "year": 1996,
      "cited_by": 2,
      "authors": ["Geoffrey E Hinton", "Brendan J Frey"],
      "description": "Neural networks are typically trained to perform a non-. linear mapping from a set of measurements to a re-sponse. For monitoring purposes, it would be useful to learn a mapping from sensor values into the probability that the plant was malfunctioning. For rare or unantic\u2014_ ipated failures, however, this is infeasible because there are few or no examples to use for training. This paper;'describes a new type of neural network that can be used-. to extract the underlying regularities from a normally; functioning system and then notice when these regular-;'ities are violated.",
      "citation_histogram": [
        [2006, 1],
        [2007, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Neural networks for pattern recognition. Advanced texts in econometrics",
      "link": "https://scholar.google.com/scholar?cluster=8169001672505006452&hl=en&oi=scholarr",
      "year": 1995,
      "cited_by": 2,
      "authors": ["J Chris Bishop", "C Bishop", "G Hinton", "P Bishop"],
      "description": null,
      "citation_histogram": [
        [2019, 1],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Developing Population Codes For Object Instantiation Parameters",
      "link": "https://www.aaai.org/Papers/Symposia/Fall/1993/FS-93-04/FS93-04-022.pdf",
      "year": 1993,
      "cited_by": 2,
      "authors": ["Richard S Zemel", "Geoffrey E Hinton"],
      "description": "An efficient and useful representation for an object viewed from different positions is in terms of its instantiation parameters. We show how the Minimum Description Length principle (MDL) can be used to train the hidden units of a neural network to develop a population code for the instantiation parameters of an object in an image. Each hidden unit has a location in a low-dimensional implicit space. If the hidden unit activities form a standard shape (a bump) in this space, they can be cheaply encoded by the center of this bump. So the weights from the input units to the hidden units in a self-supervised network are trained to make the activities form a bump. The coordinates of the hidden units in the implicit space are also learned, thus allowing flexibility, as the network develops separate population codes when presented with different objects.",
      "citation_histogram": [
        [2006, 1],
        [2007, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Introduction to the updated edition",
      "link": "https://www.taylorfrancis.com/chapters/edit/10.4324/9781315807997-5/introduction-updated-edition",
      "year": 1989,
      "cited_by": 2,
      "authors": ["Geoffrey E Hinton", "James A Anderson"],
      "description": "A lot has happened since this collection of chapters was first printed in 1981.  The basic ideas presented here are still correct, but they have been extended considerably and are now being applied to practical problems. We mention a few of the major developments here.",
      "citation_histogram": [
        [1991, 1],
        [1992, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Learning representations by backpropagating errors, Cognitive Model",
      "link": "https://scholar.google.com/scholar?cluster=8110946636551541704&hl=en&oi=scholarr",
      "year": 1988,
      "cited_by": 2,
      "authors": ["DE Rumelhart", "GE Hinton", "RJ Williams"],
      "description": null,
      "citation_histogram": [
        [2019, 1],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Models of human inference",
      "link": "https://pascal-francis.inist.fr/vibad/index.php?action=getRecordDetail&idt=7001331",
      "year": 1987,
      "cited_by": 2,
      "authors": ["Geoffrey E Hinton"],
      "description": "Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS",
      "citation_histogram": [
        [1994, 1],
        [1995, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Error Propagation",
      "link": "https://scholar.google.com/scholar?cluster=1475202476442966415&hl=en&oi=scholarr",
      "year": 1986,
      "cited_by": 2,
      "authors": ["DE RUMELHART", "GE Hinton", "RJ Williams"],
      "description": "We now have a rather good understanding of simple two-layer associative networks in which a set of input patterns arriving at an input layer are mapped directly to a set of output patterns at an output layer. Such networks have no hidden units. They involve only inpt/I and otttpul units. ln these cases there is no internal representation. The coding provided by the external world must suffice. These networks have proved useful in a wide variety of applications (cf. Chapters 2, l7, and 18). Perhaps the essential character of such networks is that they map similar input patterns to similar output patterns. This is what allows these networks to make reasonable generalizations and perform reasonably on patterns that have never before been presented. The similarity of pattcrns in a l\u2019Dl\u2019systcm is determined by their overlap. The overlap in such networks is determined outside the learning system itself\u2014by whatever produces the\u00a0\u2026",
      "citation_histogram": [
        [2012, 1],
        [2013, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Parallel Distributed Emergence",
      "link": "https://scholar.google.com/scholar?cluster=9733299570105603192&hl=en&oi=scholarr",
      "year": 1986,
      "cited_by": 2,
      "authors": [
        "David E Rumelhart",
        "Paul Smolensky",
        "James L McClelland",
        "Geoff E Hinton"
      ],
      "description": null,
      "citation_histogram": [
        [1998, 1],
        [1999, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Geoffrey, Visualizing Data using t-SNE",
      "link": "https://scholar.google.com/scholar?cluster=12334563714842076909&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 2,
      "authors": ["LH van der Maaten"],
      "description": null,
      "citation_histogram": [
        [2020, 1],
        [2021, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Neural Networks for Machine Learning Lecture 1a Why do we need machine learning?",
      "link": "http://www.cs.toronto.edu/~bonner/courses/2016s/csc321/lectures/lec1.pdf",
      "year": null,
      "cited_by": 2,
      "authors": ["Geoffrey Hinton", "Nitish Srivastava", "Kevin Swersky"],
      "description": "\u2022 McCulloch-Pitts (1943): influenced Von Neumann.\u2013First compute a weighted sum of the inputs.\u2013Then send out a fixed size spike of activity if the weighted sum exceeds a threshold.\u2013McCulloch and Pitts thought that each spike is like the truth value of a proposition and each neuron combines truth values to compute the truth value of another proposition! output weighted input",
      "citation_histogram": [[2018, 2]],
      "detail_extracted": true
    },
    {
      "title": "Lecture 18: Distributed Representations",
      "link": "http://www.cs.toronto.edu/~hinton/csc321_05/notes/lec18.ps",
      "year": null,
      "cited_by": 2,
      "authors": ["Geoffrey Hinton"],
      "description": "Lecture 18: Distributed Representations Localist representations Examples of componential \nstructure Page 1 1 CSC321: Neural Networks Lecture 18: Distributed Representations Geoffrey \nHinton Localist representations \u2022 The simplest way to represent things with neural networks is \nto dedicate one neuron to each thing. \u2013 Easy to understand. \u2013 Easy to code by hand \u2022 Often \nused to represent inputs to a net \u2013 Easy to learn \u2022 This is what mixture models do. \u2022 Each cluster \ncorresponds to one neuron \u2013 Easy to associate with other representations or responses. \u2022 \nBut localist models are very inefficient whenever the data has componential structure. \nExamples of componential structure \u2022 Big, yellow, Volkswagen \u2013 Do we have a neuron for \nthis combination \u2022 Is the BYV neuron set aside in advance? \u2022 Is it created on the fly? \u2022 How is \nit related to the neurons for big and yellow and Volkswagen? \u2022 Consider a visual scene \u2013 It \u2026",
      "citation_histogram": [
        [2019, 1],
        [2020, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "8L Williams, R.(1986)",
      "link": "https://scholar.google.com/scholar?cluster=13699816541271728036&hl=en&oi=scholarr",
      "year": null,
      "cited_by": 2,
      "authors": ["DE Rumelhart", "GE Hinton"],
      "description": null,
      "citation_histogram": [
        [1995, 1],
        [1996, 1]
      ],
      "detail_extracted": true
    },
    {
      "title": "Systems and methods for contrastive learning of visual representations",
      "link": "https://patents.google.com/patent/US11354778B2/en",
      "year": 2022,
      "cited_by": 1,
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Everest Hinton"
      ],
      "description": "Provided are systems and methods for contrastive learning of visual representations. In particular, the present disclosure provides systems and methods that leverage particular data augmentation schemes and a learnable nonlinear transformation between the representation and the contrastive loss to provide improved visual representations. In contrast to certain existing techniques, the contrastive self-supervised learning algorithms described herein do not require specialized architectures or a memory bank. Some example implementations of the proposed approaches can be referred to as a simple framework for contrastive learning of representations or \u201cSimCLR.\u201d Further example aspects are described below and provide the following benefits and insights.",
      "citation_histogram": [[2022, 1]],
      "detail_extracted": true
    },
    {
      "title": "Convex Representation of Objects Using Neural Network",
      "link": "https://patents.google.com/patent/US20210319209A1/en",
      "year": 2021,
      "cited_by": 1,
      "authors": [
        "Boyang Deng",
        "Kyle Genova",
        "Soroosh Yazdani",
        "Sofien Bouaziz",
        "Geoffrey E Hinton",
        "Andrea Tagliasacchi"
      ],
      "description": "Methods, systems, and apparatus including computer programs encoded on a computer storage medium, for generating convex decomposition of objects using neural network models. One of the methods includes receiving an input that depicts an object. The input is processed using a neural network to generate an output that defines a convex representation of the object. The output includes, for each of a plurality of convex elements, respective parameters that define a position of the convex element in the convex representation of the object.",
      "citation_histogram": [[2022, 1]],
      "detail_extracted": true
    },
    {
      "title": "Processing text using neural networks",
      "link": "https://patents.google.com/patent/US11003856B2/en",
      "year": 2021,
      "cited_by": 1,
      "authors": ["Jamie Ryan Kiros", "William Chan", "Geoffrey E Hinton"],
      "description": "Methods, systems, and apparatus including computer programs encoded on a computer storage medium, for generating a data set that associates each text segment in a vocabulary of text segments with a respective numeric embedding. In one aspect, a method includes providing, to an image search engine, a search query that includes the text segment; obtaining image search results that have been classified as being responsive to the search query by the image search engine, wherein each image search result identifies a respective image; for each image search result, processing the image identified by the image search result using a convolutional neural network, wherein the convolutional neural network has been trained to process the image to generate an image numeric embedding for the image; and generating a numeric embedding for the text segment from the image numeric embeddings for the images\u00a0\u2026",
      "citation_histogram": [[2022, 1]],
      "detail_extracted": true
    },
    {
      "title": "Scaling RBMs to High Dimensional Data with Invertible Neural Networks",
      "link": "https://invertibleworkshop.github.io/INNF_2020/accepted_papers/pdfs/26.pdf",
      "year": 2020,
      "cited_by": 1,
      "authors": [
        "Will Grathwohl",
        "Xuechen Li",
        "Kevin Swersky",
        "Milad Hashemi",
        "J\u00f6rn-Henrick Jacobsen",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "description": "We combine invertible neural networks with RBMs to create a more tractable energy-based model which retains the power of recent scalable EBM variants while allowing for more efficient sampling and likelihood evaluation. We further find that replacing the Gaussian base distributions typically used in normalizing flows with an RBM leads to improved likelihood compared to a flow with a similar architecture possibly providing a pathway to more efficient, but still tractable generative models. We demonstrate the performance of our approach on small image datasets and compare to recent normalizing flows and EBMs.",
      "citation_histogram": [[2022, 1]],
      "detail_extracted": true
    },
    {
      "title": "Technical Perspective A Better Way to Learn Features",
      "link": "https://scholar.google.com/scholar?cluster=4845383847833594802&hl=en&oi=scholarr",
      "year": 2011,
      "cited_by": 1,
      "authors": ["Geoffrey E Hinton"],
      "description": null,
      "citation_histogram": [[2017, 1]],
      "detail_extracted": true
    },
    {
      "title": "Deep Generative Models for Modeling Animate Motion",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.219.2922&rep=rep1&type=pdf",
      "year": 2008,
      "cited_by": 1,
      "authors": ["Graham W Taylor", "Geoffrey E Hinton", "Sam Roweis"],
      "description": "Recent advances in motion capture technology have fueled interest in the synthesis and analysis of complex animate motion for animation and tracking. In this work we focus on model driven analysis and synthesis but avoid the complexities involved in imposing physics-based constraints [1], and the storage requirements involved in concatenating parts of training sequences [2], relying instead on a \u201cpure\u201d learning approach in which all the knowledge in the model comes from the data.Data from modern motion capture systems is high-dimensional and contains complex non-linear relationships between the components of the observation vector, which usually represent joint angles with respect to some skeletal structure. Hidden Markov models cannot model such data efficiently because they rely on a single, discrete K-state multinomial to represent the history of the time series. To model N bits of information about the past history they require 2N hidden states. To avoid this exponential explosion, we need a model with distributed (ie componential) hidden state that has a representational capacity which is linear in the number of components. Linear dynamical systems satisfy this requirement, but they cannot model the complex non-linear dynamics created by the non-linear properties of muscles, contact forces of the foot on the ground and myriad other factors.",
      "citation_histogram": [[2006, 1]],
      "detail_extracted": true
    },
    {
      "title": "E cient stochastic source coding and an application to a Bayesian network source model",
      "link": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.2935&rep=rep1&type=pdf",
      "year": 1997,
      "cited_by": 1,
      "authors": ["Brendan J Frey", "GE Hinton"],
      "description": "In this paper, we introduce a new algorithm called\\bits-back coding\" that makes stochastic source codes e cient. For a given one-to-many source code, we show that this algorithm can actually be more e cient than the algorithm that always picks the shortest codeword. Optimal e ciency is achieved when codewords are chosen according to the Boltzmann distribution based on the codeword lengths. It turns out that a commonly used technique for determining parameters| maximum likelihood estimation| actually minimizes the bits-back coding cost when codewords are chosen according to the Boltzmann distribution. A tractable approximation to maximum likelihood estimation| the generalized expectation maximization algorithm| minimizes the bits-back coding cost. After presenting a binary Bayesian network model that assigns exponentially many codewords to each symbol, we show how a tractable approximation to the Boltzmann distribution can be used for bits-back coding. We illustrate the performance of bits-back coding using using nonsynthetic data with a binary Bayesian network source model that produces 260 possible codewords for each input symbol. The rate for bits-back coding is nearly one half of that obtained by picking the shortest codeword for each symbol.",
      "citation_histogram": [[1998, 1]],
      "detail_extracted": true
    },
    {
      "title": "Learning fast neural network emulators for physics-based models",
      "link": "https://dl.acm.org/doi/abs/10.1145/259081.259254",
      "year": 1997,
      "cited_by": 1,
      "authors": ["Radek Grzezczuk", "Demetri Terzopoulos", "Geoffrey Hinton"],
      "description": "Learning fast neural network emulators for physics-based models | ACM SIGGRAPH 97 \nVisual Proceedings: The art and interdisciplinary programs of SIGGRAPH '97 ACM Digital \nLibrary home ACM home Google, Inc. (search) Advanced Search Browse About Sign in \nRegister Advanced Search Journals Magazines Proceedings Books SIGs Conferences \nPeople More Search ACM Digital Library SearchSearch Advanced Search siggraph \nConference Proceedings Upcoming Events Authors Affiliations Award Winners More HomeConferencesSIGGRAPHProceedingsSIGGRAPH \n'97Learning fast neural network emulators for physics-based models Article Share on \nLearning fast neural network emulators for physics-based models Authors: Radek \nGrzezczuk View Profile , Demetri Terzopoulos View Profile , Geoffrey Hinton View Profile \nAuthors Info & Claims SIGGRAPH '97: ACM SIGGRAPH 97 Visual Proceedings: The art \u2026",
      "citation_histogram": [[2021, 1]],
      "detail_extracted": true
    },
    {
      "title": "Preface to Simplifying Neural Networks by Soft Weight Sharing",
      "link": "https://scholar.google.com/scholar?cluster=14150448258022298191&hl=en&oi=scholarr",
      "year": 1994,
      "cited_by": 1,
      "authors": ["GE Hinton", "S Nowlan"],
      "description": "For a supervised neural network to generalize well, there must be less information in the weights than there is in the output vectors of the training cases. Researchers have considered many possible ways of limiting the information in the weights. One method is to divide the connections into subsets, and force the weights within a subset to be identical. If this\" weight sharing\" is based on an analysis of the natural symmetries of the task, it can be very effective. 2, 3 If we do not know, in advance, which connections should share weight values, we can still encourage solutions that share values by using an appropriate measure for the complexity of the weights. An obvious method is to use a finite number of discrete weight values. The number of bits required to specify each value is then\u2014logp where p is the fraction of the weights having that value. In addition, some bits are required to specify the values used and their probability masses. If we use discrete weight values, it is obvious that the weights are simpler (ie, take less bits to describe) if many of them are identical. Unfortunately, they are no simpler if they are almost identical so we have a nasty search space. The search for simple weights would be much easier if we could find a measure of complexity that decreased smoothly as weight values became more similar to each other. The following chapter describes such a measure and shows that it leads to tractable searches and good generalization on two tasks. Instead of dividing the weights into discrete bins, the method models the distribution of weight values as",
      "citation_histogram": [[2006, 1]],
      "detail_extracted": true
    },
    {
      "title": "Simplifying neural networks by soft weight sharing",
      "link": "http://www.cs.toronto.edu/~hinton/absps/sunspots.pdf",
      "year": 1994,
      "cited_by": 1,
      "authors": ["Geoffrey E Hinton", "S Nowlan"],
      "description": "One way of simplifying neural networks so they generalize better is to add an extra term to the error function that will penalize compleiiity. Simple versions of this approach include penalizing the sum of the squares of the weights or penaiizing the number of nonzero weights. We propose a more complicated penalty term in which the distribution of weight values is modeled as a mixture of multiple gaussians. A set of weights is simple if the weights have high probability density under the mixture model. This can be achieved by clustering the weights into subsets with the weights in each cluster having very similar values. Since we do not know the appropriate means or variances of the clusters in advance, we allow the parameters of the mixture model to adapt at the same time as the network learns. Simulations on two different problems demonstrate that this complexity term is more effective than previous complexity terms.'",
      "citation_histogram": [[2022, 1]],
      "detail_extracted": true
    },
    {
      "title": "Using neural networks to learn intractable generative models",
      "link": "https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_3262004",
      "year": 1994,
      "cited_by": 1,
      "authors": ["Geoffrey E Hinton", "P Dayan", "RM Neal", "RS Zemel"],
      "description": "Using neural networks to learn intractable generative models :: MPG.PuRe Deutsch Hilfe \nDatenschutzhinweis Impressum Volltexte einbeziehen DetailsucheBrowse START ABLAGE \n(0)Werkzeuge Datensatz DATENSATZ AKTIONENEXPORT Zur Ablage hinzuf\u00fcgen Lokale \nTagsFreigabegeschichteDetails\u00dcbersicht Freigegeben Konferenzbeitrag Using neural networks \nto learn intractable generative models MPG-Autoren Es sind keine MPG-Autoren in der \nPublikation vorhanden Externe Ressourcen Es sind keine externen Ressourcen hinterlegt \nVolltexte (frei zug\u00e4nglich) Es sind keine frei zug\u00e4nglichen Volltexte in PuRe verf\u00fcgbar \nErg\u00e4nzendes Material (frei zug\u00e4nglich) Es sind keine frei zug\u00e4nglichen Erg\u00e4nzenden Materialien \nverf\u00fcgbar Zitation Hinton, G., Dayan, P., Neal, R., & Zemel, R. (1994). Using neural networks to \nlearn intractable generative models. In 1994 Proceedings of the Section on Bayesian Statistical \u2026",
      "citation_histogram": [[2012, 1]],
      "detail_extracted": true
    },
    {
      "title": "\u00bb Simulating Brain \u00ab",
      "link": "https://scholar.google.com/scholar?cluster=14943276829020214714&hl=en&oi=scholarr",
      "year": 1993,
      "cited_by": 1,
      "authors": ["Geoffrey E Hinton", "DC Plaut", "T Shallice"],
      "description": "words. If a network of simulated neurons is trained to read and then is damaged, it produces strikingly similar behavior by Geoffrey E. Hinton, David C, Plaut and Tim Shallice bullet wound to the head. He sur-vived the war with a strange disability; although he could read and comprehend some words with ease, many others gave him trouble, He read the word antique as\" vase\" and uncle as\" nephew\" The injury was devastating to the patient, GR, but it provided invaluable information to researchers investigat-ing the mechanisms by which the brain comprehends written language. A properly functioning system for converting letters on a page to spoken sounds reveals little of its inner structure, but when that system ls disrupted, the peculiar pattern of the resulting dysfunction may offer essential clues to the original, undamaged architecture.| 1944 a young soldier suffered a",
      "citation_histogram": [[1994, 1]],
      "detail_extracted": true
    },
    {
      "title": "SCENE-BASED AND VIEWER-CENTERED REPRESENTATIONS FOR COMPARING SHAPE",
      "link": "https://scholar.google.com/scholar?cluster=8767241857205016751&hl=en&oi=scholarr",
      "year": 1988,
      "cited_by": 1,
      "authors": ["LM Parsons", "GE HINTON"],
      "description": null,
      "citation_histogram": [[2001, 1]],
      "detail_extracted": true
    },
    {
      "title": "Learning distributed representations",
      "link": "https://scholar.google.com/scholar?cluster=3069288954800413274&hl=en&oi=scholarr",
      "year": 1986,
      "cited_by": 1,
      "authors": ["G Hinton"],
      "description": "There have been many different proposals for how conceptual information may be represented in neural networks. These range from extreme localist theories in which each concept is represented by a single neural unit (Barlow 1972) to extreme distributed theories in which a concept corresponds to a pattern of activity over a large part of the cortex. These two extremes are the natural implementations of two different theories of semantics. In the structuralist approach, concepts are defined by their relationships to other concepts rather than by some internal essence. The natural expression of this approach in a neural net is to make each concept be a single unit with no internal structure and to use the connections between units to encode the relationships between concepts. In the componential approach each concept is simply a set of features and so a neural net can be made to implement a set of concepts by assigning a unit to each feature and setting the strengths of the connections between units so that each concept corresponds to a stable pattern of activity distributed over the whole network (Hopfield 1982; Kohonen 1977; Willshaw, Buneman, and Longuet-Higgins 1969). The network can then perform concept completion (ie retrieve the whole concept from a sufficient subset of its features). The problem with componential theories is that they have little to say about how concepts are used for structured reasoning. They are primarily concerned with the similarities between concepts or with pairwise associations. They provide no obvious way of representing articulated structures composed of a number of concepts playing different roles within\u00a0\u2026",
      "citation_histogram": [[1995, 1]],
      "detail_extracted": true
    },
    {
      "title": "Technical Report CMU-Cs 84-119",
      "link": "https://www.cs.utoronto.ca/~hinton/absps/bmtr.pdf",
      "year": 1984,
      "cited_by": 1,
      "authors": ["Geoffrey E Hinton", "David H Ackley"],
      "description": "The computational power of massively parallel networks of simple processing elements resides-in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the pre-existing hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general\u00a0\u2026",
      "citation_histogram": [[1987, 1]],
      "detail_extracted": true
    },
    {
      "title": "A Generalist Framework for Panoptic Segmentation of Images and Videos",
      "link": "https://arxiv.org/abs/2210.06366",
      "year": 2022,
      "cited_by": null,
      "authors": [
        "Ting Chen",
        "Lala Li",
        "Saurabh Saxena",
        "Geoffrey Hinton",
        "David J Fleet"
      ],
      "description": "Panoptic segmentation assigns semantic and instance ID labels to every pixel of an image. As permutations of instance IDs are also valid solutions, the task requires learning of high-dimensional one-to-many mapping. As a result, state-of-the-art approaches use customized architectures and task-specific loss functions. We formulate panoptic segmentation as a discrete data generation problem, without relying on inductive bias of the task. A diffusion model based on analog bits is used to model panoptic masks, with a simple, generic architecture and loss function. By simply adding past predictions as a conditioning signal, our method is capable of modeling video (in a streaming setting) and thereby learns to track object instances automatically. With extensive experiments, we demonstrate that our generalist approach can perform competitively to state-of-the-art specialist methods in similar settings.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Scaling Forward Gradient With Local Losses",
      "link": "https://arxiv.org/abs/2210.03310",
      "year": 2022,
      "cited_by": null,
      "authors": [
        "Mengye Ren",
        "Simon Kornblith",
        "Renjie Liao",
        "Geoffrey Hinton"
      ],
      "description": "Forward gradient learning computes a noisy directional gradient and is a biologically plausible alternative to backprop for learning deep neural networks. However, the standard forward gradient algorithm, when applied naively, suffers from high variance when the number of parameters to be learned is large. In this paper, we propose a series of architectural and algorithmic modifications that together make forward gradient learning practical for standard deep learning benchmark tasks. We show that it is possible to substantially reduce the variance of the forward gradient estimator by applying perturbations to activations rather than weights. We further improve the scalability of forward gradient by introducing a large number of local greedy loss functions, each of which involves only a small number of learnable parameters, and a new MLPMixer-inspired architecture, LocalMixer, that is more suitable for local learning. Our approach matches backprop on MNIST and CIFAR-10 and significantly outperforms previously proposed backprop-free algorithms on ImageNet.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Training distilled machine learning models",
      "link": "https://patents.google.com/patent/US11423337B2/en",
      "year": 2022,
      "cited_by": null,
      "authors": ["Oriol Vinyals", "Jeffrey Adgate Dean", "Geoffrey E Hinton"],
      "description": "Methods, systems, and apparatus, including computer programs encoded on computer storage media, for training a distilled machine learning model. One of the methods includes training a cumbersome machine learning model, wherein the cumbersome machine learning model is configured to receive an input and generate a respective score for each of a plurality of classes; and training a distilled machine learning model on a plurality of training inputs, wherein the distilled machine learning model is also configured to receive inputs and generate scores for the plurality of classes, comprising: processing each training input using the cumbersome machine learning model to generate a cumbersome target soft output for the training input; and training the distilled machine learning model to, for each of the training inputs, generate a soft output that matches the cumbersome target soft output for the training input.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Object discovery in images through categorizing object parts",
      "link": "https://patents.google.com/patent/US20220230425A1/en",
      "year": 2022,
      "cited_by": null,
      "authors": [
        "Adam Roman Kosiorek",
        "Geoffrey E Hinton",
        "Sara Sabour Rouh Aghdam",
        "Yee Whye Teh"
      ],
      "description": "Methods, systems, and apparatus, including computer programs encoded on computer storage media, for detecting objects in images. One of the methods includes obtaining an input image; processing the input image to generate predicted part feature data, the predicted part feature data comprising, for each of a plurality of possible object parts: a part presence probability representing a likelihood that the possible object part is depicted in the input image, a predicted pose of the possible object part in the input image given that the possible object part is depicted in the input image, and an object part feature vector characterizing the depiction of the possible object part given that the possible object part is depicted in the input image; and processing the predicted part feature data for the plurality of possible object parts to generate an object detection output that identifies one or more objects depicted in the input image.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Neural network training using the soft nearest neighbor loss",
      "link": "https://patents.google.com/patent/US20220101624A1/en",
      "year": 2022,
      "cited_by": null,
      "authors": [
        "Geoffrey E Hinton",
        "Nicholas Myles Wisener Frosst",
        "Nicolas Guy Robert Papernot"
      ],
      "description": "Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for training a classification neural network. In one aspect, a method comprises: for each of a plurality of network inputs: processing the network input using the classification neural network to generate a classification output that defines a predicted class of the network input; determining a soft nearest neighbor loss, wherein the soft nearest neighbor loss encourages intermediate representations of network inputs of different classes to become more entangled, wherein the entanglement of intermediate representations of network inputs of different classes characterizes how similar pairs of intermediate representations of network inputs of different class are relative to pairs of intermediate representations of network inputs of the same class; and adjusting the current values of the classification neural network\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "System and method for addressing overfitting in a neural network",
      "link": "https://patents.google.com/patent/US20210224659A1/en",
      "year": 2021,
      "cited_by": null,
      "authors": [
        "Geoffrey E Hinton",
        "Alexander Krizhevsky",
        "Ilya Sutskever",
        "Nitish Srivastava"
      ],
      "description": "A system for training a neural network. A switch is linked to feature detectors in at least some of the layers of the neural network. For each training case, the switch randomly selectively disables each of the feature detectors in accordance with a preconfigured probability. The weights from each training case are then normalized for applying the neural network to test data.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "System and method for addressing overfitting in a neural network",
      "link": "https://patents.google.com/patent/US10977557B2/en",
      "year": 2021,
      "cited_by": null,
      "authors": [
        "Geoffrey E Hinton",
        "Alexander Krizhevsky",
        "Ilya Sutskever",
        "Nitish Srivastava"
      ],
      "description": "A system for training a neural network. A switch is linked to feature detectors in at least some of the layers of the neural network. For each training case, the switch randomly selectively disables each of the feature detectors in accordance with a preconfigured probability. The weights from each training case are then normalized for applying the neural network to test data.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Neural Additive Models: Interpretable Machine Learning with Neural Networks",
      "link": "https://research.google/pubs/pub51092/",
      "year": 2021,
      "cited_by": null,
      "authors": [
        "Rishabh Agarwal",
        "Levi Melnick",
        "Nicholas Frosst",
        "Ben Lengerich",
        "Xuezhou Zhang",
        "Rich Caruana",
        "Geoffrey Everest Hinton"
      ],
      "description": "Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationship between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and single decision trees. They perform similarly to\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "System and method for parallelizing convolutional neural networks",
      "link": "https://patents.google.com/patent/US20200327391A1/en",
      "year": 2020,
      "cited_by": null,
      "authors": [
        "Alexander Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey E Hinton"
      ],
      "description": "A parallel convolutional neural network is provided. The CNN is implemented by a plurality of convolutional neural networks each on a respective processing node. Each CNN has a plurality of layers. A subset of the layers are interconnected between processing nodes such that activations are fed forward across nodes. The remaining subset is not so interconnected.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Capsule neural networks",
      "link": "https://patents.google.com/patent/US20200285934A1/en",
      "year": 2020,
      "cited_by": null,
      "authors": [
        "Geoffrey E Hinton",
        "Nicholas Myles Wisener Frosst",
        "Sara Sabour Rouh Aghdam"
      ],
      "description": "Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for a neural network that is configured to receive a network input and to generate a network output for the network input. The neural network comprises a plurality of layers arranged in a sequence, including a plurality of capsule layers. Each particular capsule in a particular capsule layer is configured to receive respective inputs including:(i) outputs generated by capsules of a previous capsule layer that is before the particular capsule layer in the sequence, and (ii) final routing factors between capsules of the previous capsule layer and the particular capsule, wherein the final routing factors are generated by a routing subsystem. Each particular capsule in the particular capsule layer is configured to determine a particular capsule output based on the received inputs, wherein the particular capsule output is of\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Recurrent neural networks with rectified linear units",
      "link": "https://patents.google.com/patent/US10635972B1/en",
      "year": 2020,
      "cited_by": null,
      "authors": ["Quoc V Le", "Geoffrey E Hinton", "Navdeep Jaitly"],
      "description": "Methods and systems for learning long-term dependencies in recurrent neural networks. In one aspect, a neural network system is configured to receive a respective input for each of a plurality of time steps and to generate a respective output for each time step, the neural network system comprising one or more recurrent neural network layers, wherein, for each of the time steps, each of the recurrent neural network layers is configured to receive a layer input for the time step; apply an input weight matrix to the layer input to generate a first output; apply a recurrent weight matrix to a hidden state of the recurrent neural network layer for the time step to generate a second output; combine the first and second outputs to generate a combined output; and apply a rectified linear unit activation function to the combined output to generate a layer output for the time step.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "System and method for addressing overfitting in a neural network",
      "link": "https://patents.google.com/patent/US10366329B2/en",
      "year": 2019,
      "cited_by": null,
      "authors": [
        "Geoffrey E Hinton",
        "Alexander Krizhevsky",
        "Ilya Sutskever",
        "Nitish Srivastava"
      ],
      "description": "A system for training a neural network. A switch is linked to feature detectors in at least some of the layers of the neural network. For each training case, the switch randomly selectively disables each of the feature detectors in accordance with a preconfigured probability. The weights from each training case are then normalized for applying the neural network to test data.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Training distilled machine learning models",
      "link": "https://patents.google.com/patent/US10289962B2/en",
      "year": 2019,
      "cited_by": null,
      "authors": ["Oriol Vinyals", "Jeffrey A Dean", "Geoffrey E Hinton"],
      "description": "Methods, systems, and apparatus, including computer programs encoded on computer storage media, for training a distilled machine learning model. One of the methods includes training a cumbersome machine learning model, wherein the cumbersome machine learning model is configured to receive an input and generate a respective score for each of a plurality of classes; and training a distilled machine learning model on a plurality of training inputs, wherein the distilled machine learning model is also configured to receive inputs and generate scores for the plurality of classes, comprising: processing each training input using the cumbersome machine learning model to generate a cumbersome target soft output for the training input; and training the distilled machine learning model to, for each of the training inputs, generate a soft output that matches the cumbersome target soft output for the training input.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Preface to \u201cSimplifying Neural Networks by Soft Weight Sharing\u201d",
      "link": "https://www.taylorfrancis.com/chapters/edit/10.1201/9780429492525-12/preface-simplifying-neural-networks-soft-weight-sharing-geoffrey-hinton-steven-nowlan",
      "year": 2018,
      "cited_by": null,
      "authors": ["Geoffrey E Hinton", "Steven Nowlan"],
      "description": "Researchers have considered many possible ways of limiting the information in the weights. For a supervised neural network to generalize well, there must be less information in the weights than there is in the output vectors of the training cases. Instead of dividing the weights into discrete bins, the method models the distribution of weight values as a mixture of Gaussians. Each Gaussian acts like a \"soft\" bin. The search for simple weights would be much easier if people could find a measure of complexity that decreased smoothly as weight values became more similar to each other. A weakness is that complexity measure, by focussing only on the probability density of a weight, implicitly assumes that all weights are encoded to the same accuracy. Despite the weaknesses, the method does impressively well at predicting the sunspots time series, and it produces a simple network that is easy to interpret in terms of the\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Illustrative Language Understanding: Large-Scale Visual Grounding with Google Image Search",
      "link": "https://research.google/pubs/pub47099/",
      "year": 2018,
      "cited_by": null,
      "authors": ["Jamie Kiros", "William Chan", "Geoffrey Hinton"],
      "description": "We introduce Picturebook, a large-scale lookup operation to ground language via \u2018snapshots\u2019 of our physical world accessed through image search. For each word in a vocabulary, we extract the top-k images from Google image search and feed the images through a convolutional network to extract a word embedding. We introduce a multimodal gating function to fuse our Picturebook embeddings with other word representations. We also introduce Inverse Picturebook, a mechanism to map a Picturebook embedding back into words. We experiment and report results across a wide range of tasks: word similarity, natural language inference, semantic relatedness, sentiment/topic classification, image-sentence ranking and machine translation. We also show that gate activations corresponding to Picturebook embeddings are highly correlated to human judgments of concreteness ratings.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Deterministic Boltzmann Learning in Networks with Asymmetric Connectivity",
      "link": "https://books.google.com/books?hl=en&lr=&id=_Z6jBQAAQBAJ&oi=fnd&pg=PA3&dq=info:HlELUigbgcwJ:scholar.google.com&ots=xu0Nn7f9Zg&sig=O2ObeAuWMol9lHfkel6HAPvsLHI",
      "year": 2014,
      "cited_by": null,
      "authors": ["Geoffrey E Hinton"],
      "description": "The simplicity and locality of the \u201ccon-trastive Hebb synapse\u201d(CHS) used in Boltz-mann machine learning makes it an attractive model for real biological synapses. The slow learning exhibited by the stochastic Boltzmann machine can be greatly improved by using a mean field approximation and it has been shown (Hinton, 1989) that the CHS also performs steepest descent in these deterministic mean field networks. A major weakness of the learning procedure, from a biological perspective, is that the derivation assumes detailed symmetry of the connectivity. Using networks with purely asymmetric connectivity, we show that the CHS still works in practice provided the connectivity is grossly sym-metrical so that if unit i sends a connection to unit j, there are numerous indirect feedback paths from j to i. So long as the network settles to a stable state, we show that the CHS approximates steepest descent and that the proportional error in the approximation can be expected to decrease as the size of the network increases.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Simulating brain damage (Scientific American, October 1993, 58\u201365)",
      "link": "https://www.taylorfrancis.com/chapters/edit/10.4324/9781315782973-36/simulating-brain-damage-scientific-american-october-1993-58%E2%80%9365-hinton-plaut-shallice",
      "year": 2013,
      "cited_by": null,
      "authors": ["Geoffrey E Hinton", "David C Plaut", "Tim Shallice"],
      "description": "The injury was devastating to the patient, G.R., but it provided invaluable information to researchers investigating the mechanisms by which the brain comprehends written language. A properly functioning system for converting letters on a page to spoken sounds reveals little of its inner structure, but when that system is disrupted, the peculiar pattern of the resulting dysfunction may offer essential clues to the original, undamaged architecture.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Modeling Semantic Similarities in Multiple Maps",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.364.7310&rep=rep1&type=pdf",
      "year": 2009,
      "cited_by": null,
      "authors": ["Laurens Van Der Maaten", "Geoffrey Hinton"],
      "description": "Models that represent words as points in a semantic space are subject to fundamental limitations of metric spaces. These limitations prevent semantic space models from faithfully representing, for example, the pairwise similarities between word meanings as revealed by word association data. In particular, semantic space models cannot faithfully represent intransitive pairwise similarities or the similarities of words that have multiple meanings. In this paper, we present a model that alleviates the limitations of semantic space models by constructing a collection of maps that represent complementary structure in the similarity data. Our model is a variant of a similarity choice model known as Stochastic Neighbor Embedding that constructs multiple maps and allows each object to occur as a point in several different maps. We apply the model to a set of word association data, demonstrating that it can successfully represent intransitive semantic relations as well as words with multiple meanings, and that it outperforms traditional semantic space models in the prediction of word associations. We compare the model to alternative representations of semantic structure, such as topic models and semantic networks.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Workshop summary: Workshop on learning feature hierarchies",
      "link": "https://dl.acm.org/doi/abs/10.1145/1553374.1553543",
      "year": 2009,
      "cited_by": null,
      "authors": [
        "Kay Yu",
        "Ruslan Salakhutdinov",
        "Yann LeCun",
        "Geoff Hinton",
        "Yoshua Bengio"
      ],
      "description": "Workshop summary: Workshop on learning feature hierarchies | Proceedings of the 26th Annual \nInternational Conference on Machine Learning ACM Digital Library home ACM home Google, \nInc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals \nMagazines Proceedings Books SIGs Conferences People More Search ACM Digital Library \nSearchSearch Advanced Search icml Conference Proceedings Upcoming Events Authors \nAffiliations Award Winners More HomeConferencesICMLProceedingsICML '09Workshop \nsummary: Workshop on learning feature hierarchies research-article Share on Workshop \nsummary: Workshop on learning feature hierarchies Authors: Kay Yu NEC Laboratories America \nNEC Laboratories America View Profile , Ruslan Salakhutdinov University of Toronto, Canada \nUniversity of Toronto, Canada View Profile , Yann LeCun New York University New York \u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Artificial Intelligence: Neural Networks",
      "link": "https://onlinelibrary.wiley.com/doi/abs/10.1002/0471743984.vse0673",
      "year": 2005,
      "cited_by": null,
      "authors": ["Geoffrey Hinton"],
      "description": "Artificial neural networks consist of simple processing units that interact via weighted connections. They are sometimes implemented in hardware, but most research involves software simulations. They were originally inspired by ideas about how the brain computes, but some types of neural network that are biologically unrealistic are nevertheless technologically useful. Neural networks learn from examples and so they can solve problems, such as recognizing handwriting, for which it is very hard to write a conventional computer program.The most widely used learning algorithms are supervised: they assume that there is a set of training cases, each consisting of an input vector and a desired output vector. Learning involves sweeping through the training set many times, gradually adjusting the weights on the connections so that the actual output produced by the network gets closer to the desired output. The simplest\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "A Desktop Input Device and Interface for Interactive 3D Character",
      "link": "https://scholar.google.com/scholar?cluster=15463133719986046144&hl=en&oi=scholarr",
      "year": 2002,
      "cited_by": null,
      "authors": ["Geoffrey Hinton"],
      "description": "Abstract need to be controlled for complex human motion does not make (performance animation) a viable so-lution for realistic looking animation [19, pg. 28].We present a novel input device and interface for in-teractively controlling the animation of graphical human character from a desktop environment. The trackers are embedded in a new physical design, which is both simple yet also provides significant benefits, and establishes a tangible interface with coordinate frames inherent to the character. A layered kinematic motion recording strategy accesses subsets of the total degrees of freedom of the character. We present the experiences of three novice users with the system, and that of a long-term user who has prior experience with other complex continuous in-",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "How Neural Networks Learn from Experience Geoffrey E. Hinton",
      "link": "https://books.google.com/books?hl=en&lr=&id=FJblV_iOPjIC&oi=fnd&pg=PA181&dq=info:P8Ci_TFMuDAJ:scholar.google.com&ots=z-Io8iFYXS&sig=INS5txVHWUYUyBzJ0kbf0tpJAws",
      "year": 2002,
      "cited_by": null,
      "authors": ["Geoffrey E Hinton"],
      "description": "The brain is a remarkable computer. It interprets imprecise information from the senses at an incredibly rapid rate. It discerns a whisper in a noisy room, a face in a dimly lit alley and a hidden agenda in a political statement. Most impressive of all, the brain learns-without any explicit instructions\u2014to create the internal representations that make these skills possible. Much is still unknown about how the brain trains itself to process informa-tion, so theories abound. To test these hypotheses, my colleagues and I have attempted to mimic the brain's learning processes by creating networks of artificial neurons. We construct these neural networks by first trying to deduce the essential features of neurons and their interconnections. We then typically program a computer to simulate these features.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Deterministic Boltzmann Learning Performs Steepest",
      "link": "https://books.google.com/books?hl=en&lr=&id=dUTC55eR4jQC&oi=fnd&pg=PA89&dq=info:VoapbAKcsyEJ:scholar.google.com&ots=DB1RrpHnXZ&sig=kyIeYhupMBQukafCbgLPkFmwsrI",
      "year": 2001,
      "cited_by": null,
      "authors": ["Geoffrey E Hinton"],
      "description": "The Boltzmann machine learning procedure has been successfully ap-plied in deterministic networks of analog units that use a mean field ap-proximation to efficiently simulate a truly stochastic system (Peterson and Anderson 1987). This type of\" deterministic Boltzmann machine\"(DBM) learns much faster than the equivalent\" stochastic Boltzmann machine\"(SBM), but since the learning procedure for DBM's is only based on an analogy with SBM's, there is no existing proof that it performs gradient descent in any function, and it has only been justified by simulations. By using the appropriate interpretation for the way in which a DBM represents the probability of an output vector given an input vector, it is shown that the DBM performs steepest descent in the same function as the original SBM, except at rare discontinuities. A very simple way of forcing the weights to become symmetrical is also described, and this makes the DBM more biologically plausible than back-propagation (Werbos 1974; Parker 1985; Rumelhart et al. 1986).",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Products of Hidden Markov Models",
      "link": "https://scholar.google.com/scholar?cluster=2104076887625915474&hl=en&oi=scholarr",
      "year": 2001,
      "cited_by": null,
      "authors": ["Geoffrey E Hinton"],
      "description": "We present products of hidden Markov mod-els (POHMM's), a way of combining HMM's to form a distributed state time series model. Inference in a POHMM is tractable and effi-cient. Learning of the parameters, although intractable, can be effectively done using the Product of Experts learning rule. The distributed state helps the model to explain data which has multiple causes, and the fact that each model need only explain part of the data means a POHMM can capture longer range structure than an HMM is capable of.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "output. Subsequently, using standard techniques, they calculate accelerations and numerically inte",
      "link": "https://scholar.google.com/scholar?cluster=7837427557998457870&hl=en&oi=scholarr",
      "year": 2000,
      "cited_by": null,
      "authors": ["D Rumelhart", "G Hinton", "R Williams"],
      "description": "Commentary on Chapter 34 455 a grate these in order to obtain position and velocity at the next point in time. This general approach is worked out for a restricted class of movements: two degrees of freedom arm positioning movements in two-dimensional space. For this class of problems, it is shown that it is indeed possible to come up with an ANN that does the job satisfactorily. As correctly claimed by the author, this is the first actual demonstration that multijoint movements can be predicted from surface EMG signals and initial state of the extremity.The second part of this chapter starts to consider arm movements in 3D space. For reasons that are not quite clear, the authors take a different approach here. They construct, again using an ANN, the map from EMG to static arm position. When this map is subsequently applied to nonstatic data, the obvious result is obtained that the position predicted by the model does not match the current positionobvious because during movement nonzero accel-eration of the arm is required and thus the torque produced by gravity and net joint torque produced by the muscles cannot be in equilibrium. It might have been more interesting to attempt to extend the approach used in 2D to 3D while retaining the ordering of inputs and outputs of the model. Returning to the first part of the chapter, it must be noted that there have been some earlier attempts to predict the output of the motor system from measured EMG. In particular, Bobbert et al.(1986) were able to predict net joint torque at the ankle during vertical jumping on the basis of EMG data obtained from soleus and gastrocnemius muscles. That and similar studies\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "We introduce a new statistical model for time series that iteratively seg",
      "link": "https://scholar.google.com/scholar?cluster=10464313790826909945&hl=en&oi=scholarr",
      "year": 2000,
      "cited_by": null,
      "authors": ["Geoffrey E Hinton"],
      "description": "Most commonly used probabilistic models of time series are descendants of either hidden Markov models (HMM) or stochastic linear dynamical systems, also known as state-space models (SSM). HMMs represent in-formation about the past of a sequence through a single discrete random variable\u2014the hidden state. The prior probability distribution of this state is derived from the previous hidden state using a stochastic transition matrix. Knowing the state at any time makes the past, present, and future observa-tions statistically independent. This is the Markov independence property that gives the model its name. SSMs represent information about the past through a real-valued hidden state vector. Again, conditioned on this state vector, the past, present, and future observations are statistically independent. The dependency between",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Extracting Distributed Representations of Concepts and Relations from",
      "link": "https://scholar.google.com/scholar?cluster=3422337091294346476&hl=en&oi=scholarr",
      "year": 2000,
      "cited_by": null,
      "authors": ["Alberto Paccanaro", "Geoffrey E Hinton"],
      "description": "Linear Relational Embedding (LRE) was introduced (Paccanaro and Hinton, 1999) as a means of extracting a distributed representation of concepts from relational data. The original formulation cannot use negative information and cannot properly handle data in which there are multiple correct answers. In this paper we propose an extended formulation of LRE that solves both these problems. We present results in two simple domains, which show that learning leads to good generalization.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Learning Population Codes by Minimizing Description",
      "link": "https://books.google.com/books?hl=en&lr=&id=yj04Y0lje4cC&oi=fnd&pg=PA261&dq=info:kcZ4j7-14dAJ:scholar.google.com&ots=b-uzXRd4dx&sig=SkKl4PQw1m-y7rzXeMrCj4RDW_E",
      "year": 1999,
      "cited_by": null,
      "authors": ["Richard S Zemel", "Geoffrey E Hinton"],
      "description": "Most existing unsupervised learning algorithms can be understood using the minimum description length (MDL) principle (Rissanen 1989). Given an ensemble of input vectors, the aim of the learning algorithm is to find a method of coding each input vector that minimizes the total cost, in bits, of communicating the input vectors to a receiver. There are three terms in the total description length:",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "CNL, The Salk Institute, PO Box 85800, San Diego, CA 92186-5800 USA",
      "link": "https://books.google.com/books?hl=en&lr=&id=yj04Y0lje4cC&oi=fnd&pg=PA277&dq=info:iDxvhdNb0XEJ:scholar.google.com&ots=b-uzXRd4dx&sig=OfYTftx5XvcVbqGaQm4NwJwYssw",
      "year": 1999,
      "cited_by": null,
      "authors": ["Peter Dayan", "Geoffrey E Hinton", "Radford M Neal"],
      "description": "Following Helmholtz, we view the human perceptual system as a statistical inference engine whose function is to infer the probable causes of sensory input. We show that a device of this kind can learn how to perform these inferences without requiring a teacher to label each sen-sory input vector with its underlying causes. A recognition model is used to infer a probability distribution over the underlying causes from the sensory input, and a separate generative model, which is also learned, is used to train the recognition model (Zemel 1994; Hinton and Zemel 1994; Zemel and Hinton 1995). As an example of the generative models in which we are interested, consider the shift patterns in Figure 1, which are on four 1 x 8 rows of binary pixels. These were produced by the two-level stochastic hierarchical generative process described in the figure caption. The task of learning is to take a set of examples generated by such a process and induce the model. Note that underlying any pattern there are multiple",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "The DELVE user manual",
      "link": "https://scholar.google.com/scholar?cluster=5495152266088229323&hl=en&oi=scholarr",
      "year": 1996,
      "cited_by": null,
      "authors": [
        "C Rasmussen",
        "RM Neal",
        "GE Hinton",
        "D van Camp",
        "M Revow",
        "Z Ghahramani",
        "R Kustra",
        "R Tibshirani"
      ],
      "description": "This manual describes the preliminary release of the DELVE environment. Some features described here have not yet implemented, as noted. Support for regression tasks is presently somewhat more developed than that for classification tasks. We recommend that you exercise caution when using this version of DELVE for real work, as it is possible that bugs remain in the software. We hope that you will send us reports of any problems you encounter, as well as any other comments you may have on the software or manual, at the e-mail address below. Please mention the version number of the manual and/or the software with any comments you send.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Time-Delay Neural Networks",
      "link": "https://books.google.com/books?hl=en&lr=&id=oWRv7BR4BqMC&oi=fnd&pg=PA35&dq=info:oYYuXtnueEEJ:scholar.google.com&ots=RNDuC2Wwh8&sig=DTNTBuYQF_3hY47U9SCHK5EOmCs",
      "year": 1995,
      "cited_by": null,
      "authors": ["Geoffrey Hinton"],
      "description": "In this paper we present a Time-Delay Neural Network (TDNN) approach to phoneme recognition which is characterized by two important properties. 1) Using a 3 layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces. The TDNN learns these decision surfaces automatically using error backpropagation [1]. 2) The time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independent of position in time and hence not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes \u201cB,\"\u201cD,\u201d and \u201cG\u201d in varying phonetic contexts was chosen. For comparison, several discrete Hidden Markov Models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5 percent correct while the rate ob-",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "The Appeal of Parallel Distributed Processing",
      "link": "https://books.google.com/books?hl=en&lr=&id=pBV526wnJigC&oi=fnd&pg=PA269&dq=info:COx66j1VNd4J:scholar.google.com&ots=nF49JX_Q7m&sig=0gUf4LFfqIcy7GHiA4PvY_OeCrs",
      "year": 1992,
      "cited_by": null,
      "authors": ["Geoffrey E Hinton"],
      "description": "What makes people smarter than machines? They certainly are not quicker or more precise. Yet people are far better at perceiving objects in natural scenes and noting their relations, at understanding language and retrieving contextually appropriate informa-tion from memory, at making plans and carrying out contextually appropriate actions, and at a wide range of other natural cognitive tasks. People are also far better at learning to do these things more accurately and fluently through processing experience. What is the basis for these differences? One answer, perhaps the classic one we might expect from artificial intelligence, is\" software.\" If we only had the right computer program, the argument goes, we might be able to capture the fluidity and adaptability of human information processing.Certainly this answer is partially correct. There have been great breakthroughs in our understanding of cognition as a result of the development of expressive high-level computer languages and powerful algorithms. No doubt there will be more such break-throughs in the future. However, we do not think that software is the whole story. In our view, people are smarter than today's computers because the brain employs a basic computational architecture that is more suited to deal with a central aspect of the natural information processing tasks that people are so good at. We will show through examples that these tasks generally require the simultaneous consideration of many pieces of information or constraints. Each constraint may be imperfectly specified and ambiguous, yet each can play a potentially decisive role in determining the outcome of processing\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "A Learning Algorithm for",
      "link": "https://scholar.google.com/scholar?cluster=6716462492000044339&hl=en&oi=scholarr",
      "year": 1992,
      "cited_by": null,
      "authors": ["GEOFFREY E HINTON"],
      "description": "Evidence about the architecture of the brain and the potential of the new VLSI technology have led to a resurgence of interest in \u201cconnectionist\u201d sys-",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "tributed processing (PDP) idea. These various models, and indeed many in the literature, clearly have many features in common, but they are just as clearly distinct models. How\u00a0\u2026",
      "link": "https://scholar.google.com/scholar?cluster=290351876695495068&hl=en&oi=scholarr",
      "year": 1992,
      "cited_by": null,
      "authors": ["DE RUMELHART", "GE HINTON", "JL MCCLELLAND"],
      "description": "This paper presents a multilayer feedforward network, called the Gaussian potential function network (GPFN), performing association or classification based on a set of potential fields synthesized over the domain of input space by a number of Gaussian potential function units (GPFUs). A GPFU as a basic component of the GPFN is designed to generate a Gaussian form of a potential field. A weighted summation of Gaussian potential fields generated by a suitable number of GPFUs provides an arbitrary shape of a potential field over the domain of input space. A set of such synthesized potential fields obtained from different sets of summation weights defines the associations specified by teaching patterns. GPFUs are implemented by the units of the hidden layer, while the synthesis of potential fields are carried out by the weighted connection between the hidden layer and the output layer. This paper also presents a detailed learning algorithm for the GPFN. Learning consists of the determination of the minimally necessary number of GPFUs and the adjustment of the locations and shapes of the individual Gaussian potential fields defined by GPFUs as well as the summation weights. The learning of the minimally necessary number of GPFUs is based on the control of the effective radius of GPFUs, while the parameter learning is based on the gradient decent procedure. The GPFN designed here presents some distinctive features: 1) the association is based on clustering using Gaussian potential fields, rather than based on space segmentation using linear boundaries as seen in a conventional multilayer feedforward network, 2) the synthesized\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Learning spatially coherent properties of the visual world in connectionist networks",
      "link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/1569/0000/Learning-spatially-coherent-properties-of-the-visual-world-in-connectionist/10.1117/12.48380.short",
      "year": 1991,
      "cited_by": null,
      "authors": ["Suzanna Becker", "Geoffrey E Hinton"],
      "description": "In the unsupervised learning paradigm, a network of neuron-like units is presented with an ensemble of input patterns from a structured environment, such as the visual world, and learns to represent the regularities in that input. The major goal in developing unsupervised learning algorithms is to find objective functions that characterize the quality of the network's representation without explicitly specifying the desired outputs of any of the units. The sort of objective functions considered cause a unit to become tuned to spatially coherent features of visual images (such as texture, depth, shading, and surface orientation), by learning to predict the outputs of other units which have spatially adjacent receptive fields. Simulations show that using an information-theoretic algorithm called IMAX, a network can be trained to represent depth by observing random dot stereograms of surfaces with continuously varying disparities\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Mean field networks that learn to discriminate temporally distorted strings",
      "link": "https://scholar.google.com/scholar?cluster=17188117753476722300&hl=en&oi=scholarr",
      "year": 1991,
      "cited_by": null,
      "authors": ["Geoffrey E Hinton"],
      "description": "Neural networks can be used to discriminate between very similar phonemes and they can handle the variability in time of occurrence by using a time-delay architecture followed by a temporal integration (Lang, Hinton and Waibel, 1990). So far, however, neural networks have been less successful at handling longer duration events that require some-thing equivalent to\" time warping\" in order to match stored knowledge to the data. We present a type of mean field network (MFN) with tied weights that is capable of approximating the recognizer for a hidden markov model (HMM). In the process of settling to a stable state, the MFN finds a blend of likely ways of generating the input string given its internal model of the probabilities of transi-tions between hidden states and the probabil-ities of input symbols given a hidden state.This blend is a heuristic approximation to the full set of path probabilities that is implicitly represented by an HMM recognizer. The learning algorithm for the MFN is less efficient than for an HMM of the same size. However, the MFN is capable of using dis-tributed representations of the hidden state, and this can make it exponentially more efficient than an HMM when modelling strings produced by a generator that itself has com-ponential states. We view this type of MFN as a way of allowing more powerful repre-sentations without abandoning the automatic parameter estimation procedures that have allowed relatively simple models like HMM's to outperform complex AI representations on",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "The Development of the Time-Delayed Neural Network Architecture",
      "link": "https://apps.dtic.mil/sti/citations/ADA221540",
      "year": 1990,
      "cited_by": null,
      "authors": ["Geoffrey Hinton", "James L McClelland"],
      "description": "Currently, one of the most powerful connectionist learning procedures is back-propagation which repeatedly adjusts the weights in a network so as to minimize a measure of the difference between the actual output vector of the network and a desired output vector given the current input vector. The simple weight adjusting rule is derived by propagating partial derivatives of the error backwards through the net using the chain rule. Experiments have shown that back-propagation has most of the properties desired by connectionists. As with any worthwhile learning rule, it can learn non-linear black box functions and make fine distinctions between input patterns in the presence of noise. Moreover, starting from random initial states, back-propagation networks can learn to use their hidden intermediate layer units to efficiently represent the structure that is inherent in their input data, often discovering intuitively pleasing features. The fact that back-propagation can discover features and distinguish between similar patterns in the presence of noise makes it a natural candidate as a speech recognition method. Another reason for expecting back-propagation to be good at speech is the success that hidden Markov models have enjoyed in speech can be useful when there is a rigorous automatic method for tuning its parameters.Descriptors:",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Descent in Weight-Space",
      "link": "https://scholar.google.com/scholar?cluster=8524283973460293735&hl=en&oi=scholarr",
      "year": 1990,
      "cited_by": null,
      "authors": ["Geoffrey E Hinton"],
      "description": "The Boltzmann machine learning procedure has been successfully applied in deterministic networks of analog units that use a mean field approximation to efficiently simulate a truly stochastic system (Peterson and Anderson 1987). This type of\" deterministic Boltzmann machine\"(DBM) learns much faster than the equivalent\" stochastic Boltzmann machine\"(SBM), but since the learning procedure for DBM's is only based on an analogy with SBM's, there is no existing proof that it per-forms gradient descent in any function, and it has only been justified by simulations. By using the appropriate interpretation for the way in which a DBM represents the probability of an output vector given an input vector, it is shown that the DBM performs steepest descent in the same function as the original SBM, except at rare discontinuities. A very simple way of forcing the weights to become symmetrical is also described, and this makes the DBM more biologically plausible than back-propagation (Werbos 1974; Parker 1985; Rumelhart et al. 1986).",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Connectionist Models: Proceedings of the Summer School Held in San Diego, California on 1990",
      "link": "https://apps.dtic.mil/sti/citations/ADA239606",
      "year": 1990,
      "cited_by": null,
      "authors": [
        "David S Touretzky",
        "Jeffrey L Elman",
        "Terrence J SeJnowski",
        "Geoffrey E Hinton"
      ],
      "description": "The simplicity and locality of the contrastive Hebb synapse CHS used in Boltzmann machine learning makes it an attractive model for real biological synapses. The slow learning exhibited by the stochastic Boltzmann machine can be greatly improved by using a mean field approximation and it has been shown Hinton, 1989 that the CHS also performs steepest descent in these deterministic mean field networks. A major weakness of the learning procedure, from a biological perspective, is that the derivation assumes detailed symmetry of the connectivity. Using networks with purely asymmetric connectivity, we show that the CHS still works in practice provided the connectivity is grossly symmetrical so that if unit i sends a connection to unit j, there are numerous indirect feedback paths from j to i.Descriptors:",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "The development of the time-delayed neural network architecture(Final Report)",
      "link": "https://scholar.google.com/scholar?cluster=5425159121730640334&hl=en&oi=scholarr",
      "year": 1990,
      "cited_by": null,
      "authors": ["GEOFFREY HINTON", "JAMESL MCCLELLAND", "KEVINJ LANG"],
      "description": "Currently, one of the most powerful connectionist learning procedures is back-propagation which repeatedly adjusts the weights in a network so as to minimize a measure of the difference between the actual output vector of the network and a desired output vector given the current input vector. The simple weight adjusting rule is derived by propagating partial derivatives of the error backwards through the net using the chain rule. Experiments have shown that back-propagation has most of the properties desired by connectionists. As with any worthwhile learning rule, it can learn non-linear black box functions and make fine distinctions between input patterns in the presence of noise. Moreover, starting from random initial states, back-propagation networks can learn to use their hidden (intermediate layer) units to efficiently represent the structure that is inherent in their input data, often discovering intuitively pleasing\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Proceedings of the Connectionists Models Summer School Held in Pittsburgh, Pennsylvania on June 17-26, 1988",
      "link": "https://apps.dtic.mil/sti/pdfs/ADA207014.pdf",
      "year": 1989,
      "cited_by": null,
      "authors": ["David Touretzky", "Geoffrey Hinton", "Terrence Sejnowski"],
      "description": "Contents Back-Propagation Learning Sequential and Recurrent Networks New Learning Architectures Analysis of Networks Language and Cognition Speech Recognition Vision Part 8 Hardware.Descriptors:",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Connectionist Models: Summer School: Selected Revised Papers",
      "link": "https://scholar.google.com/scholar?cluster=14361384752048824716&hl=en&oi=scholarr",
      "year": 1989,
      "cited_by": null,
      "authors": ["David Touretzky", "Geoffrey Hinton", "Terrence Sejnowski"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Sequential Thought Processes in PDP Models",
      "link": "https://www.cs.toronto.edu/~fritz/absps/pdp14.pdf",
      "year": 1988,
      "cited_by": null,
      "authors": ["GE HINTON"],
      "description": "One of our goals for this book is to offer an alternative framework for viewing cognitive phenomena. We have argued that talk at the level of units and activations of units is the preferable way to describe human thought. There is, however, already an established language for discussing cognitive phenomena. In this chapter we wish to address the relationship between some of the key established concepts and our parallel distributed processing models. There are many important con-cepts from modern cognitive science which must be explicated in our framework. Perhaps the most important, however, is the concept of the schema or related concepts such as scripts, frames, and so on. These large scale data structures have been posited as playing critical roles in the interpretation of input data, the guiding of action, and the storage of knowledge in memory. Indeed, as we have argued elsewhere (cf. Rumelhart, 1980\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Parallel Distributed Processing",
      "link": "https://scholar.google.com/scholar?cluster=4489575474042948195&hl=en&oi=scholarr",
      "year": 1987,
      "cited_by": null,
      "authors": ["P de RumelhartSmolensky", "Jl Mcclelland", "Ge Hinton"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "A New Approach to Cognition\u2014Connectionism",
      "link": "https://www.cairn-int.info/revue-le-debat-1987-5-p-45.htm",
      "year": 1987,
      "cited_by": null,
      "authors": ["JL McClelland", "DE Rumelhart", "GE Hinton"],
      "description": "Le D\u00e9bat\u2019s aim is to bring together, through discussion, the contributions of the humanities and social sciences with a thoroughgoing reflection on current affairs and cultural developments. Read more...",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "List of Systems Comparative Problem Solving Systems 15-880: Advanced Topics in Al: Problem Solving Fall 1985",
      "link": "http://iiif.library.cmu.edu/file/Newell_box00075_fld05342_doc0018/Newell_box00075_fld05342_doc0018.pdf",
      "year": 1985,
      "cited_by": null,
      "authors": ["Geoffrey Hinton", "Tom Mitchell", "Alien Newell"],
      "description": "List of Systems Comparative Problem Solving Systems 15-880: Advanced Topics in Al: Problem \nSolving Fall 1985 Page 1 List of Systems Comparative Problem Solving Systems 15-880: \nAdvanced Topics in Al: Problem Solving Fall 1985 Geoffrey Hinton Tom Mitchell Alien Newell \nVersion #1 23 September 1985 Department of Computer Science Carnegie-Mellon University \nPittsburgh, Pennsylvania 15213 Room 5409 Wean Hall, TuTh 1330:1450, starting Tu 24 Sep \nPage 2 1 List of Problem-Solving Systems This is an annotated list of candidate systems for \nanalysis in the course. The main list is given alphabetical order, but auxiliary groupings arc \nprovided by chronology and by a rough classification into types of systems. In the main, the \nsystems of interest are those that purport to be general intelligent systems to be designed \nto be completed to general intelligent systems. But a number of more limited systems arc \u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Notes for Comparative Problem Solving Systems 15-880: Advanced Topics in Al: Problem Solving FalM 985",
      "link": "http://iiif.library.cmu.edu/file/Newell_box00075_fld05342_doc0028/Newell_box00075_fld05342_doc0028.pdf",
      "year": 1985,
      "cited_by": null,
      "authors": ["Geoffrey Hinton", "Tom Mitchell", "Alien Newell"],
      "description": "There are difficulties in such an enterprise.(1) Such systems are complex and their organi/ntion can be highly idiosyncratic vis a vis other systems. In essence, each system puts forth a total framework, which invites evaluation entirely in its own terms.(2) Such systems necessarily require external specification for each particular task they are to perform (otherwise they could not tackle multiple tasks). Systems differ radically on the form, content and extent of this external input. Indeed, a programming language (eg, Lisp) can be construed to be legitimate exemplar of an AI system, which simply has a massive external specification that is largely procedural.(3) The task domains of systems differ radically. Some may even be primarily learning system while others are performance systems.With no agreed upon basis for evaluating a system as to its power, generality, scope and intelligence, comparative case analysis becomes the method of choice. Thus, this course will have some of the flavor of a course in comparative programming languages. We will devote a small amount of time initially to existing ideas and schemes for evaluating systems, but will treat these as merely grist for the mill. The body of the course will examine a substantial number of systems. Members of the class will undertake the study of individual systems and will lead class discussions about them.(Anyone attending the class will be considered a member of the class for the purposes of assigning systems to be studied.) The analyses will attempt to be comparative. An attempt will be made to evolve a basis of comparison and analysis that will be useful beyond the confines of the class.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Three frames suffice",
      "link": "https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/three-frames-suffice/DE140EBB81215386EA2068FDAF012589",
      "year": 1985,
      "cited_by": null,
      "authors": ["Geoffrey E Hinton"],
      "description": "//static.cambridge.org/content/id/urn%3Acambridge.org%3Aid%3Aarticle%3AS0140525X0002077X/resource/name/firstPage-S0140525X0002077Xa.jpg",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Processing in the Brain",
      "link": "https://scholar.google.com/scholar?cluster=17125557720624339893&hl=en&oi=scholarr",
      "year": 1981,
      "cited_by": null,
      "authors": ["Geoffrey E Hinton"],
      "description": "This chapter introduces some models of how information may be represented and processed in a highly parallel computer like the brain. Despite the staggering amount of information available about the physiology and anatomy of the brain, very little is really known about the nature of the higher-level processing per-formed by the nervous system. There is no established theory about the kinds of neural activity that occur when we hear a sentence, perceive an object, or form a plan, though data on many fascinating and significant bits and pieces is now available.An obvious feature of the brain is its parallelism (see Section 1.5 for a review of the neurophysiological evidence). This parallelism is a major reason for investigating computational models other than the conventional serial digital computer in our attempts to understand how the brain processes information. The concept",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "\u7b97\u6cd5",
      "link": "https://blog.csdn.net/weixin_41697507/article/details/95095299",
      "year": null,
      "cited_by": null,
      "authors": ["Rafael M\u00fcller", "Simon Kornblith", "Geoffrey Hinton"],
      "description": "The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model\u2019s predictions.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Deep Neural Networks for Acoustic Modeling",
      "link": "https://scholar.google.com/scholar?cluster=10273098388487395919&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": [
        "Geoffrey Hinton",
        "Li Deng",
        "Dong Yu",
        "George Dahl",
        "Abdel-rahman Mohamed",
        "Navdeep Jaitly",
        "Andrew Senior",
        "Vincent Vanhoucke",
        "Patrick Nguyen",
        "Tara Sainath",
        "Brian Kingsbury"
      ],
      "description": "Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "When Does Label Smoothing Help? When Does Label Smoothing Help?",
      "link": "https://blog.katastros.com/a?ID=01200-872fbd8f-9c6f-47f3-97f2-dfd083ab5ef9",
      "year": null,
      "cited_by": null,
      "authors": ["Rafael M\u00fcller", "Simon Kornblith", "Geoffrey Hinton"],
      "description": "The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions. but does not hurt generalization or calibration of the model's predictions. but does not hurt generalization or calibration of the model's predictions.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "ImageNet Classification with Deep Convolutional Neural Networks (AlexNet) ImageNet Classification with Deep Convolutional Neural Networks (AlexNet) ImageNet Classification with\u00a0\u2026",
      "link": "https://blog.karatos.in/a?ID=00450-9b1db208-6e87-450e-bc1d-78f9c16d5996",
      "year": null,
      "cited_by": null,
      "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"],
      "description": "We trained a large deep convolutional neural network to divide the 1.2 million high-definition images in the ImageNet LSVRC-2010 competition into 1,000 different categories. For the test data, we got the top-1 error rate of 37.5% and the top-5 error rate of 17.0%. This effect is much better than the previous top. The neural network has 60 million parameters and 650,000 neurons. It consists of five convolutional layers, a max-pooling layer followed by some convolutional layers, and three fully connected layers, as well as the last 1000-Way's softmax layer composition. In order to make training faster, we used unsaturated neurons and a very efficient GPU tool for convolution operations. In order to reduce the overfitting of the fully connected layer, we adopted a newly developed regularization method called\" dropout\", which has been proven to be very effective. In the ILSVRC-2012 competition, we entered a variant of the model and won with a top-5 test error rate of 15.3%. In comparison, the error rate of the second best item is 26.2%.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Attend, Infer, Repeat: Fast Scene Understanding with Deep Generative Models",
      "link": "http://juxi.net/workshop/deep-learning-rss-2016/papers/Eslami%20-%20Scene%20Understanding.pdf",
      "year": null,
      "cited_by": null,
      "authors": [
        "SM Ali Eslami",
        "Nicolas Heess",
        "Theophane Weber",
        "Yuval Tassa",
        "David Szepesvari",
        "Koray Kavukcuoglu",
        "Geoffrey E Hinton"
      ],
      "description": "We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps, corresponding to the number of entities in the scene. We use this scheme to learn to perform inference in partially specified 2D models (variablesized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects\u2013counting, locating and classifying the elements of a scene\u2013without any supervision, eg, decomposing images of 3D scenes with various numbers of objects in a single forward pass of a neural network. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.The human percept of a visual scene is highly structured. Natural scenes decompose into objects that are arranged in space, have visual and physical properties, and are in functional relationships with each other. Artificial systems that interpret images in this way are desirable, as accurate detection of objects and inference of their attributes is thought to be fundamental for many problems of interest. Consider a robot whose task is to clear a table after dinner. To plan a course of action it will need to understand the scene, ie to determine which objects are present, what class each object belongs to, and where each one is located in the scene\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "AlexNet \u8bba\u6587\u7ffb\u8bd1-ImageNet Classification with Deep Convolutional Neural Networks_weixin_34351321 \u7684\u535a\u5ba2-\u7a0b\u5e8f\u5458 ITS401",
      "link": "https://its401.com/article/weixin_34351321/94661512",
      "year": null,
      "cited_by": null,
      "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"],
      "description": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called \u201cdropout\u201d that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\u6458\u8981\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5927\u578b\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6765\u5c06 ImageNet LSVRC-2010 \u7ade\u8d5b\u7684 120 \u4e07\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u5206\u5230 1000 \u4e0d\u540c\u7684\u7c7b\u522b\u4e2d. \u5728\u6d4b\u8bd5\u6570\u636e\u4e0a, \u6211\u4eec\u5f97\u5230\u4e86 top-1 37.5% \u548c top-5 17.0% \u7684\u9519\u8bef\u7387, \u8fd9\u4e2a\u7ed3\u679c\u6bd4\u76ee\u524d\u7684\u6700\u597d\u7ed3\u679c\u597d\u5f88\u591a. \u8fd9\u4e2a\u795e\u7ecf\u7f51\u7edc\u6709 6000 \u4e07\u53c2\u6570\u548c 650000 \u4e2a\u795e\u7ecf\u5143, \u5305\u542b 5 \u4e2a\u5377\u79ef\u5c42 (\u67d0\u4e9b\u5377\u79ef\u5c42\u540e\u9762\u5e26\u6709\u6c60\u5316\u5c42) \u548c 3 \u4e2a\u5168\u8fde\u63a5\u5c42, \u6700\u540e\u662f\u4e00\u4e2a 1000 \u7ef4\u7684 softmax. \u4e3a\u4e86\u8bad\u7ec3\u7684\u66f4\u5feb, \u6211\u4eec\u4f7f\u7528\u4e86\u975e\u9971\u548c\u795e\u7ecf\u5143, \u5e76\u5728\u8fdb\u884c\u5377\u79ef\u64cd\u4f5c\u65f6\u4f7f\u7528\u4e86\u975e\u5e38\u6709\u6548\u7684 GPU. \u4e3a\u4e86\u51cf\u5c11\u5168\u8fde\u63a5\u5c42\u7684\u8fc7\u62df\u5408, \u6211\u4eec\u91c7\u7528\u4e86\u4e00\u4e2a\u6700\u8fd1\u5f00\u53d1\u7684\u540d\u4e3a dropout \u7684\u6b63\u5219\u5316\u65b9\u6cd5, \u7ed3\u679c\u8bc1\u660e\u662f\u975e\u5e38\u6709\u6548\u7684. \u6211\u4eec\u4e5f\u4f7f\u7528\u8fd9\u4e2a\u6a21\u578b\u7684\u4e00\u4e2a\u53d8\u79cd\u53c2\u52a0\u4e86 ILSVRC-2012 \u7ade\u8d5b, \u8d62\u5f97\u4e86\u51a0\u519b\u5e76\u4e14\u4e0e\u7b2c\u4e8c\u540d top-5 26.2\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "ImageNet Classification with Deep Convolutional Neural Networks \u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684 ImageNet \u5206\u7c7b",
      "link": "https://www.cxybb.com/article/Jwenxue/89317880",
      "year": null,
      "cited_by": null,
      "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"],
      "description": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called \u201cdropout\u201d that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\u6458\u8981\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5927\u578b\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6765\u5c06 ImageNet LSVRC-2010 \u7ade\u8d5b\u7684 120 \u4e07\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u5206\u5230 1000 \u4e0d\u540c\u7684\u7c7b\u522b\u4e2d. \u5728\u6d4b\u8bd5\u6570\u636e\u4e0a, \u6211\u4eec\u5f97\u5230\u4e86 top-1 37.5% \u548c top-5 17.0% \u7684\u9519\u8bef\u7387, \u8fd9\u4e2a\u7ed3\u679c\u6bd4\u76ee\u524d\u7684\u6700\u597d\u7ed3\u679c\u597d\u5f88\u591a. \u8fd9\u4e2a\u795e\u7ecf\u7f51\u7edc\u6709 6000 \u4e07\u53c2\u6570\u548c 650000 \u4e2a\u795e\u7ecf\u5143, \u5305\u542b 5 \u4e2a\u5377\u79ef\u5c42 (\u67d0\u4e9b\u5377\u79ef\u5c42\u540e\u9762\u5e26\u6709\u6c60\u5316\u5c42) \u548c 3 \u4e2a\u5168\u8fde\u63a5\u5c42, \u6700\u540e\u662f\u4e00\u4e2a 1000 \u7ef4\u7684 softmax. \u4e3a\u4e86\u8bad\u7ec3\u7684\u66f4\u5feb, \u6211\u4eec\u4f7f\u7528\u4e86\u975e\u9971\u548c\u795e\u7ecf\u5143, \u5e76\u5728\u8fdb\u884c\u5377\u79ef\u64cd\u4f5c\u65f6\u4f7f\u7528\u4e86\u975e\u5e38\u6709\u6548\u7684 GPU. \u4e3a\u4e86\u51cf\u5c11\u5168\u8fde\u63a5\u5c42\u7684\u8fc7\u62df\u5408, \u6211\u4eec\u91c7\u7528\u4e86\u4e00\u4e2a\u6700\u8fd1\u5f00\u53d1\u7684\u540d\u4e3a dropout \u7684\u6b63\u5219\u5316\u65b9\u6cd5, \u7ed3\u679c\u8bc1\u660e\u662f\u975e\u5e38\u6709\u6548\u7684. \u6211\u4eec\u4e5f\u4f7f\u7528\u8fd9\u4e2a\u6a21\u578b\u7684\u4e00\u4e2a\u53d8\u79cd\u53c2\u52a0\u4e86 ILSVRC-2012 \u7ade\u8d5b, \u8d62\u5f97\u4e86\u51a0\u519b\u5e76\u4e14\u4e0e\u7b2c\u4e8c\u540d top-5 26.2\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "AlexNet \u8bba\u6587\u7ffb\u8bd1-ImageNet Classification with Deep Convolutional Neural Networks",
      "link": "https://blog.csdn.net/weixin_34351321/article/details/94661512",
      "year": null,
      "cited_by": null,
      "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"],
      "description": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called \u201cdropout\u201d that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\u6458\u8981\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5927\u578b\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6765\u5c06 ImageNet LSVRC-2010 \u7ade\u8d5b\u7684 120 \u4e07\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u5206\u5230 1000 \u4e0d\u540c\u7684\u7c7b\u522b\u4e2d. \u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\uff0c \u6211\u4eec\u5f97\u5230\u4e86 top-1 37.5% \u548c top-5 17.0% \u7684\u9519\u8bef\u7387\uff0c \u8fd9\u4e2a\u7ed3\u679c\u6bd4\u76ee\u524d\u7684\u6700\u597d\u7ed3\u679c\u597d\u5f88\u591a. \u8fd9\u4e2a\u795e\u7ecf\u7f51\u7edc\u6709 6000 \u4e07\u53c2\u6570\u548c 650000 \u4e2a\u795e\u7ecf\u5143\uff0c \u5305\u542b 5 \u4e2a\u5377\u79ef\u5c42\uff08 \u67d0\u4e9b\u5377\u79ef\u5c42\u540e\u9762\u5e26\u6709\u6c60\u5316\u5c42\uff09 \u548c 3 \u4e2a\u5168\u8fde\u63a5\u5c42\uff0c \u6700\u540e\u662f\u4e00\u4e2a 1000 \u7ef4\u7684 softmax. \u4e3a\u4e86\u8bad\u7ec3\u7684\u66f4\u5feb\uff0c \u6211\u4eec\u4f7f\u7528\u4e86\u975e\u9971\u548c\u795e\u7ecf\u5143\uff0c \u5e76\u5728\u8fdb\u884c\u5377\u79ef\u64cd\u4f5c\u65f6\u4f7f\u7528\u4e86\u975e\u5e38\u6709\u6548\u7684 GPU. \u4e3a\u4e86\u51cf\u5c11\u5168\u8fde\u63a5\u5c42\u7684\u8fc7\u62df\u5408\uff0c \u6211\u4eec\u91c7\u7528\u4e86\u4e00\u4e2a\u6700\u8fd1\u5f00\u53d1\u7684\u540d\u4e3a dropout \u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c \u7ed3\u679c\u8bc1\u660e\u662f\u975e\u5e38\u6709\u6548\u7684. \u6211\u4eec\u4e5f\u4f7f\u7528\u8fd9\u4e2a\u6a21\u578b\u7684\u4e00\u4e2a\u53d8\u79cd\u53c2\u52a0\u4e86 ILSVRC-2012 \u7ade\u8d5b\uff0c \u8d62\u5f97\u4e86\u51a0\u519b\u5e76\u4e14\u4e0e\u7b2c\u4e8c\u540d top\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "WJ CLANCEY",
      "link": "https://scholar.google.com/scholar?cluster=4926066525481894785&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": [
        "AIELLO LC",
        "S AMAREL",
        "Y ANZAI",
        "HG BARROW",
        "H BERLINER",
        "RS BOYER",
        "R BRACHMAN",
        "BG BUCHANAN",
        "R DAVIS",
        "R DECHTER",
        "J DE KLEER",
        "L ERMAN",
        "MR GENESERETH",
        "ML GINSBERG",
        "B GROSZ",
        "PJ HAYES",
        "G HINTON",
        "K KONOLIGE",
        "WG LEHNERT",
        "HJ LEVESQUE",
        "V LIFSCHITZ",
        "AK MACKWORTH",
        "R MOORE",
        "HH NAGEL",
        "NJ NILSSON",
        "J PEARL",
        "CR PERRAULT",
        "M POLLACK",
        "ZW PYLYSHYN",
        "R QUINLAN",
        "R REITER",
        "E SANDEWALL",
        "Y SHOHAM",
        "H TANAKA",
        "W WAHLSTER",
        "A BUNDY",
        "TM MITCHELL",
        "Y WILKS"
      ],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684 ImageNet \u5206\u7c7b",
      "link": "https://blog.csdn.net/jwenxue/article/details/89317880",
      "year": null,
      "cited_by": null,
      "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"],
      "description": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called \u201cdropout\u201d that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\u6458\u8981\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5927\u578b\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6765\u5c06 ImageNet LSVRC-2010 \u7ade\u8d5b\u7684 120 \u4e07\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u5206\u5230 1000 \u4e0d\u540c\u7684\u7c7b\u522b\u4e2d. \u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\uff0c \u6211\u4eec\u5f97\u5230\u4e86 top-1 37.5% \u548c top-5 17.0% \u7684\u9519\u8bef\u7387\uff0c \u8fd9\u4e2a\u7ed3\u679c\u6bd4\u76ee\u524d\u7684\u6700\u597d\u7ed3\u679c\u597d\u5f88\u591a. \u8fd9\u4e2a\u795e\u7ecf\u7f51\u7edc\u6709 6000 \u4e07\u53c2\u6570\u548c 650000 \u4e2a\u795e\u7ecf\u5143\uff0c \u5305\u542b 5 \u4e2a\u5377\u79ef\u5c42\uff08 \u67d0\u4e9b\u5377\u79ef\u5c42\u540e\u9762\u5e26\u6709\u6c60\u5316\u5c42\uff09 \u548c 3 \u4e2a\u5168\u8fde\u63a5\u5c42\uff0c \u6700\u540e\u662f\u4e00\u4e2a 1000 \u7ef4\u7684 softmax. \u4e3a\u4e86\u8bad\u7ec3\u7684\u66f4\u5feb\uff0c \u6211\u4eec\u4f7f\u7528\u4e86\u975e\u9971\u548c\u795e\u7ecf\u5143\uff0c \u5e76\u5728\u8fdb\u884c\u5377\u79ef\u64cd\u4f5c\u65f6\u4f7f\u7528\u4e86\u975e\u5e38\u6709\u6548\u7684 GPU. \u4e3a\u4e86\u51cf\u5c11\u5168\u8fde\u63a5\u5c42\u7684\u8fc7\u62df\u5408\uff0c \u6211\u4eec\u91c7\u7528\u4e86\u4e00\u4e2a\u6700\u8fd1\u5f00\u53d1\u7684\u540d\u4e3a dropout \u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c \u7ed3\u679c\u8bc1\u660e\u662f\u975e\u5e38\u6709\u6548\u7684. \u6211\u4eec\u4e5f\u4f7f\u7528\u8fd9\u4e2a\u6a21\u578b\u7684\u4e00\u4e2a\u53d8\u79cd\u53c2\u52a0\u4e86 ILSVRC-2012 \u7ade\u8d5b\uff0c \u8d62\u5f97\u4e86\u51a0\u519b\u5e76\u4e14\u4e0e\u7b2c\u4e8c\u540d top\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Inverting Generative Black Boxes with Breeder Learning",
      "link": "https://scholar.google.com/scholar?cluster=8975374897880977748&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": ["Vinod Nair", "Joshua M Susskind", "Geoffrey Hinton"],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "Efficient Parametric Projection Pursuit Density Estimation",
      "link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1059.9303&rep=rep1&type=pdf",
      "year": null,
      "cited_by": null,
      "authors": ["Geoffrey E Hinton"],
      "description": "Product models of low dimensional experts are a powerful way to avoid the curse of dimensionality. We present the\" under \u0e0d complete product of experts\"(UPoE), where each expert models a one dimensional pro \u0e0d jection of the data. The UPoE may be inter \u0e0d preted as a parametric probabilistic model for projection pursuit. Its ML learning rules are identical to the approximate learning rules proposed before for under-complete ICA. We also derive an efficient sequential learning al \u0e0d gorithm and discuss its relationship to pro \u0e0d jection pursuit density estimation and fea \u0e0d ture induction algorithms for additive ran \u0e0d dom field models.",
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "GE Hinton Preface to the Special Issue on Connectionist Symbol Processing DS Touretzky BoltzCONS: Dynamic symbol structures in a connectionist network",
      "link": "https://scholar.google.com/scholar?cluster=872800908050247235&hl=en&oi=scholarr",
      "year": null,
      "cited_by": null,
      "authors": [
        "GE Hinton",
        "JB Pollack",
        "M Derthick",
        "P Smolensky",
        "MF St John",
        "JL McClelland",
        "P Thagard",
        "KJ Holyoak",
        "G Nelson",
        "D Gochfeld",
        "JY Halpern",
        "T Hrycej",
        "V Lifschitz"
      ],
      "description": null,
      "citation_histogram": null,
      "detail_extracted": true
    },
    {
      "title": "THROUGH MATRIX INVERSION AFTER NOISE INJECTION",
      "link": "http://yann.lecun.org/exdb/publis/pdf/lecun-89b.pdf",
      "year": null,
      "cited_by": null,
      "authors": ["Yann Le Cun", "Conrad C Galland", "Geoffrey E Hinton"],
      "description": "Learning procedures that measure how random perturbations of unit activities correlate with changes in reinforcement are inefficient but simple to implement in hardware. Procedures like back-propagation (Rumelhart, Hinton and Williams, 1986) which compute how changes in activities affect the output error are much more efficient, but require more complex hardware. GEMINI is a hybrid procedure for multilayer networks, which shares many of the implementation advantages of correlational reinforcement procedures but is more efficient. GEMINI injects noise only at the \ufb01rst hidden layer and measures the resultant effect on the output error. A linear network associated with each hidden layer iteratively inverts the matrix which relates the noise to the error change, thereby obtaining the error-derivatives. No back-propagation is involved, thus allowing un-known non-linearities in the system. Two simulations\u00a0\u2026",
      "citation_histogram": null,
      "detail_extracted": true
    }
  ],
  "all_publications_retrieved": true,
  "all_publications_extracted": true
}
